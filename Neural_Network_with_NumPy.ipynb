{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network with NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Net with NumPy"
      ],
      "metadata": {
        "id": "0xJZm3XuZSSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "GOr-3QEGhrEb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bCRYvql3ZId3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and visualizing the data"
      ],
      "metadata": {
        "id": "yrzffoctj6Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset used: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset"
      ],
      "metadata": {
        "id": "QIzYm_Lfhsj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Bicicletas.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TJi9tuQyhqh9",
        "outputId": "b11427a0-bbde-449b-a5f6-c51882c01489"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   clima  temperatura  bicicletas_alugadas\n",
              "0      2     0.363625                  985\n",
              "1      2     0.353739                  801\n",
              "2      1     0.189405                 1349\n",
              "3      1     0.212122                 1562\n",
              "4      1     0.229270                 1600"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a081dacb-1313-4102-a11f-0e139ce2c70c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clima</th>\n",
              "      <th>temperatura</th>\n",
              "      <th>bicicletas_alugadas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.363625</td>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.353739</td>\n",
              "      <td>801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.189405</td>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.212122</td>\n",
              "      <td>1562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.229270</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a081dacb-1313-4102-a11f-0e139ce2c70c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a081dacb-1313-4102-a11f-0e139ce2c70c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a081dacb-1313-4102-a11f-0e139ce2c70c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy95vi3Tjahh",
        "outputId": "b45368ab-b42f-4afd-fe67-dba4534e1f6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(731, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "relation between features"
      ],
      "metadata": {
        "id": "tUK45VIfj9Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data.temperatura, data.bicicletas_alugadas)\n",
        "plt.ylabel('rented bikes')\n",
        "plt.xlabel('temperature')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Oa2g1-_5jbjc",
        "outputId": "618760b2-a779-4aa3-d46c-638c91cdacd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'temperature')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data.clima, data.bicicletas_alugadas)\n",
        "plt.ylabel('rented bikes')\n",
        "plt.xlabel('weather')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "5l8YoqbWjzrD",
        "outputId": "99eb2e01-a51e-4f66-980a-e5cd55097367"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'weather')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing data"
      ],
      "metadata": {
        "id": "1EGAN1mvkgxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = data.bicicletas_alugadas.values\n",
        "x = data[['clima', 'temperatura']].values"
      ],
      "metadata": {
        "id": "0eXVh2OpkjzZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkP_j0IKk68F",
        "outputId": "284bd783-c832-4000-cc1d-13c853e7b84a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.       0.363625]\n",
            " [2.       0.353739]\n",
            " [1.       0.189405]\n",
            " ...\n",
            " [2.       0.2424  ]\n",
            " [1.       0.2317  ]\n",
            " [2.       0.223487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#divide by the maximum value per row\n",
        "#return values between 0 and 1\n",
        "x = x/np.amax(x,axis=0)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZCiiVPyk21W",
        "outputId": "ca058093-053e-4544-f8dc-d8e0e8e67bbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.66666667 0.43242565]\n",
            " [0.66666667 0.42066914]\n",
            " [0.33333333 0.22524188]\n",
            " ...\n",
            " [0.66666667 0.28826395]\n",
            " [0.33333333 0.27553942]\n",
            " [0.66666667 0.26577246]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ymax = np.amax(y)\n",
        "y = y/ymax\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO8aGP8blU04",
        "outputId": "090ca4ec-f5f9-406a-8a30-06e307d45c88"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(731,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the model"
      ],
      "metadata": {
        "id": "X36L_qq6mN6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(sum):\n",
        "  return 1/(1+np.exp(-sum))\n",
        "\n",
        "def relu(sum):\n",
        "  return np.maximum(0, sum)"
      ],
      "metadata": {
        "id": "KeX738GAmNJf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [\n",
        "    {\"input_dim\": 2, \"output_dim\":64, \"activation\":'relu'},\n",
        "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"}, #OUTPUT LAYER\n",
        "]\n"
      ],
      "metadata": {
        "id": "SRQe1hy_oQIL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_layers(architecture, seed = 42):\n",
        "  np.random.seed(seed)\n",
        "  num_layers = len(architecture)\n",
        "  parameters = {}\n",
        "\n",
        "  for index, layer in enumerate(architecture):\n",
        "    layer_index = index + 1\n",
        "\n",
        "    layer_input_size = layer[\"input_dim\"]\n",
        "    layer_output_size = layer['output_dim']\n",
        "\n",
        "\n",
        "    parameters['W' + str(layer_index)] = np.random.randn(layer_output_size, layer_input_size) *0.1\n",
        "    parameters['b' + str(layer_index)] = np.random.randn(layer_output_size, 1)*0.1\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "0V4bEcq6pVa6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fowardprop_one_layer(last_activated, weights, biases, activation_func = 'relu'):\n",
        "  \n",
        "  output = np.dot(weights, last_activated) + biases\n",
        "\n",
        "  if activation_func == 'relu':\n",
        "    return relu(output), output\n",
        "  elif activation_func == 'sigmoid':\n",
        "    return sigmoid(output), output\n",
        "  else:\n",
        "    print(\"ACTIVATION FUNCTION ERROR\")\n",
        "  "
      ],
      "metadata": {
        "id": "-69uL5gQry-O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_propagation(X, parameters, architecture):\n",
        "  history = {}\n",
        "  current = X\n",
        "\n",
        "  for index, layer in enumerate(architecture):\n",
        "    index_layer = index+1\n",
        "\n",
        "    last_activated = current\n",
        "    \n",
        "    activation = layer[\"activation\"]\n",
        "    \n",
        "    weights = parameters[\"W\" + str(index_layer)]\n",
        "    \n",
        "    bias = parameters['b' + str(index_layer)]\n",
        "    \n",
        "    current, output = fowardprop_one_layer(last_activated, weights, bias, activation)\n",
        "    \n",
        "    history[\"A\" + str(index)] = last_activated\n",
        "    \n",
        "    history['Z' + str(index_layer)] = output\n",
        "\n",
        "  return current, history"
      ],
      "metadata": {
        "id": "Q88FLCPltLsq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(yhat, ytrue):\n",
        "  m = yhat.shape[1]\n",
        "  cost = -1 /m * (np.dot(ytrue,np.log(yhat).T) + np.dot(1-ytrue, np.log(1-yhat).T))\n",
        "  return np.squeeze(cost)"
      ],
      "metadata": {
        "id": "adX8triO0gkv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(parameters, gradient, architecture, learning_rate):\n",
        "\n",
        "  for layer_index, layer in enumerate(architecture,1):\n",
        "    parameters['W' + str(layer_index)] -= learning_rate * gradient['dW' + str(layer_index)] \n",
        "    parameters['b' + str(layer_index)] -= learning_rate * gradient['db' + str(layer_index)]    \n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "2mvDd0Zo1iEJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backprop(dActivated, output):\n",
        "  sig = sigmoid(output)\n",
        "  return dActivated * sig * (1-sig)\n",
        "def relu_backprop(dActivated, output):\n",
        "  d = np.array(dActivated, copy = True)\n",
        "  d[output <= 0] = 0\n",
        "  return d"
      ],
      "metadata": {
        "id": "t553QOlT62k0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(yhat, Y, history, parameters, architecture):\n",
        "  gradients ={}\n",
        "  Y = Y.reshape(yhat.shape)\n",
        "\n",
        "  prev_dActivated = - (np.divide(Y, yhat) - np.divide(1-Y, 1-yhat))\n",
        "\n",
        "  for prev_layer_index, layer in reversed(list(enumerate(architecture))):\n",
        "\n",
        "    current_layer_index = prev_layer_index + 1\n",
        "\n",
        "    current_activation_function = layer['activation']\n",
        "\n",
        "    current_dActivated = prev_dActivated\n",
        "\n",
        "    prev_activated = history['A' + str(prev_layer_index)]\n",
        "    current_output = history['Z' + str(current_layer_index)]\n",
        "\n",
        "    current_weights = parameters['W' + str(current_layer_index)]\n",
        "    current_biases = parameters['b' + str(current_layer_index)]\n",
        "\n",
        "    prev_dActivated, current_dW, current_db = backprop_one_layer(current_dActivated, current_weights, current_biases, current_output, prev_activated, current_activation_function)\n",
        "    gradients[\"dW\" + str(current_layer_index)] = current_dW\n",
        "    gradients[\"db\" + str(current_layer_index)] = current_db\n",
        "  \n",
        "  return gradients"
      ],
      "metadata": {
        "id": "NAfLr3Et3Gv1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_one_layer(current_dActivated, current_weights, current_biases, current_output, prev_dActivated, activation = 'relu'):\n",
        "  m = prev_dActivated.shape[1]\n",
        "  if activation == 'relu':\n",
        "    backprop_func = relu_backprop\n",
        "  elif activation =='sigmoid':\n",
        "    backprop_func = sigmoid_backprop\n",
        "  else:\n",
        "    raise Exception (\"BACKPROPAGATION ACTIVATION FUNCTION ERROR\")\n",
        "  \n",
        "  current_dOutput = backprop_func(current_dActivated, current_output)\n",
        "  current_dW = np.dot(current_dOutput,prev_dActivated.T) / m\n",
        "  current_db = np.sum(current_dOutput, axis = 1, keepdims = True) / m\n",
        "  prev_dActivated = np.dot(current_weights.T, current_dOutput)\n",
        "\n",
        "  return prev_dActivated, current_dW, current_db"
      ],
      "metadata": {
        "id": "ts4wh2ek7cjQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, Y,x_test, y_test, architecture, epochs, learning_rate ):\n",
        "  parameters = initialize_layers(architecture)\n",
        "  loss_history =[]\n",
        "  test_loss_history = []\n",
        "\n",
        "  for i in range(epochs):\n",
        "    yhat, history = foward_propagation(X = X, parameters=parameters, architecture = architecture)\n",
        "    yhat_test, history2 = foward_propagation(X = x_test, parameters=parameters, architecture = architecture)\n",
        "    \n",
        "    loss = loss_func(yhat, Y)\n",
        "    loss_history.append(loss)\n",
        "    test_loss = loss_func(yhat_test, y_test)\n",
        "    test_loss_history.append(test_loss)\n",
        "\n",
        "    gradients = backpropagation(yhat, Y, history, parameters, architecture)\n",
        "    parameters = update(parameters, gradients, architecture, learning_rate)\n",
        "\n",
        "    if (i%50):\n",
        "      print(f\"Iteration: {i} ---- loss: {loss} \")\n",
        "\n",
        "  return parameters, loss_history, test_loss_history"
      ],
      "metadata": {
        "id": "HdY7Apiz98K3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the network"
      ],
      "metadata": {
        "id": "v_gmvuXgvnDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "DhAyhwSBAaDf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, loss_history, test_loss_history = train(np.transpose(x_train), np.transpose(y_train.reshape((y_train.shape[0],1))), \n",
        "                                                    np.transpose(x_test),np.transpose(y_test.reshape((y_test.shape[0],1))),\n",
        "                                                   architecture, 10000, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00doRCWtAvP3",
        "outputId": "c1ca10a4-0e9b-410e-9380-fd761ccbadac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA sada de streaming foi truncada nas ltimas 5000 linhas.\u001b[0m\n",
            "Iteration: 4898 ---- loss: 0.6625242464478225 \n",
            "Iteration: 4899 ---- loss: 0.6625177121364968 \n",
            "Iteration: 4901 ---- loss: 0.6625046482765645 \n",
            "Iteration: 4902 ---- loss: 0.6624981184031666 \n",
            "Iteration: 4903 ---- loss: 0.6624915895899597 \n",
            "Iteration: 4904 ---- loss: 0.6624850618393814 \n",
            "Iteration: 4905 ---- loss: 0.6624785351218418 \n",
            "Iteration: 4906 ---- loss: 0.6624720094997498 \n",
            "Iteration: 4907 ---- loss: 0.6624654849341102 \n",
            "Iteration: 4908 ---- loss: 0.6624589614185561 \n",
            "Iteration: 4909 ---- loss: 0.6624524390643914 \n",
            "Iteration: 4910 ---- loss: 0.6624459179522508 \n",
            "Iteration: 4911 ---- loss: 0.6624393979065349 \n",
            "Iteration: 4912 ---- loss: 0.6624328789280536 \n",
            "Iteration: 4913 ---- loss: 0.6624263614585714 \n",
            "Iteration: 4914 ---- loss: 0.6624198457888277 \n",
            "Iteration: 4915 ---- loss: 0.6624133311897836 \n",
            "Iteration: 4916 ---- loss: 0.6624068176622445 \n",
            "Iteration: 4917 ---- loss: 0.6624003052070158 \n",
            "Iteration: 4918 ---- loss: 0.6623937938249014 \n",
            "Iteration: 4919 ---- loss: 0.662387283516706 \n",
            "Iteration: 4920 ---- loss: 0.662380774365522 \n",
            "Iteration: 4921 ---- loss: 0.6623742663566787 \n",
            "Iteration: 4922 ---- loss: 0.6623677594433683 \n",
            "Iteration: 4923 ---- loss: 0.6623612536072976 \n",
            "Iteration: 4924 ---- loss: 0.6623547488492668 \n",
            "Iteration: 4925 ---- loss: 0.6623482451700764 \n",
            "Iteration: 4926 ---- loss: 0.6623417420538411 \n",
            "Iteration: 4927 ---- loss: 0.662335239376165 \n",
            "Iteration: 4928 ---- loss: 0.6623287377803198 \n",
            "Iteration: 4929 ---- loss: 0.6623222372671029 \n",
            "Iteration: 4930 ---- loss: 0.6623157378373125 \n",
            "Iteration: 4931 ---- loss: 0.6623092394917443 \n",
            "Iteration: 4932 ---- loss: 0.6623027422311947 \n",
            "Iteration: 4933 ---- loss: 0.66229624605646 \n",
            "Iteration: 4934 ---- loss: 0.6622897509683348 \n",
            "Iteration: 4935 ---- loss: 0.6622832569676137 \n",
            "Iteration: 4936 ---- loss: 0.662276764055091 \n",
            "Iteration: 4937 ---- loss: 0.6622702722189205 \n",
            "Iteration: 4938 ---- loss: 0.6622637813995301 \n",
            "Iteration: 4939 ---- loss: 0.6622572917050106 \n",
            "Iteration: 4940 ---- loss: 0.6622508031280528 \n",
            "Iteration: 4941 ---- loss: 0.6622443156432941 \n",
            "Iteration: 4942 ---- loss: 0.6622378292515244 \n",
            "Iteration: 4943 ---- loss: 0.6622313438842911 \n",
            "Iteration: 4944 ---- loss: 0.6622248595182627 \n",
            "Iteration: 4945 ---- loss: 0.6622183762471898 \n",
            "Iteration: 4946 ---- loss: 0.6622118940718598 \n",
            "Iteration: 4947 ---- loss: 0.6622054129930597 \n",
            "Iteration: 4948 ---- loss: 0.6621989330115761 \n",
            "Iteration: 4949 ---- loss: 0.6621924541281947 \n",
            "Iteration: 4951 ---- loss: 0.662179499974253 \n",
            "Iteration: 4952 ---- loss: 0.6621730246781626 \n",
            "Iteration: 4953 ---- loss: 0.6621665507142445 \n",
            "Iteration: 4954 ---- loss: 0.6621600782318211 \n",
            "Iteration: 4955 ---- loss: 0.6621536069596243 \n",
            "Iteration: 4956 ---- loss: 0.6621471367907832 \n",
            "Iteration: 4957 ---- loss: 0.6621406677695526 \n",
            "Iteration: 4958 ---- loss: 0.6621341998475673 \n",
            "Iteration: 4959 ---- loss: 0.6621277330320908 \n",
            "Iteration: 4960 ---- loss: 0.6621212673239012 \n",
            "Iteration: 4961 ---- loss: 0.6621148025960971 \n",
            "Iteration: 4962 ---- loss: 0.6621083385943027 \n",
            "Iteration: 4963 ---- loss: 0.662101875681151 \n",
            "Iteration: 4964 ---- loss: 0.6620954138762195 \n",
            "Iteration: 4965 ---- loss: 0.6620889531802826 \n",
            "Iteration: 4966 ---- loss: 0.6620824935941145 \n",
            "Iteration: 4967 ---- loss: 0.6620760350988303 \n",
            "Iteration: 4968 ---- loss: 0.6620695776760177 \n",
            "Iteration: 4969 ---- loss: 0.6620631213648298 \n",
            "Iteration: 4970 ---- loss: 0.6620566661527815 \n",
            "Iteration: 4971 ---- loss: 0.6620502120005128 \n",
            "Iteration: 4972 ---- loss: 0.6620437589617689 \n",
            "Iteration: 4973 ---- loss: 0.6620373070373187 \n",
            "Iteration: 4974 ---- loss: 0.6620308562017859 \n",
            "Iteration: 4975 ---- loss: 0.6620244064093225 \n",
            "Iteration: 4976 ---- loss: 0.6620179577259961 \n",
            "Iteration: 4977 ---- loss: 0.6620115102687553 \n",
            "Iteration: 4978 ---- loss: 0.6620050639292344 \n",
            "Iteration: 4979 ---- loss: 0.6619986187156602 \n",
            "Iteration: 4980 ---- loss: 0.6619921746574818 \n",
            "Iteration: 4981 ---- loss: 0.6619857317661437 \n",
            "Iteration: 4982 ---- loss: 0.6619792899959641 \n",
            "Iteration: 4983 ---- loss: 0.6619728493237677 \n",
            "Iteration: 4984 ---- loss: 0.6619664097491028 \n",
            "Iteration: 4985 ---- loss: 0.6619599713720343 \n",
            "Iteration: 4986 ---- loss: 0.6619535341192362 \n",
            "Iteration: 4987 ---- loss: 0.6619470979914664 \n",
            "Iteration: 4988 ---- loss: 0.6619406629756456 \n",
            "Iteration: 4989 ---- loss: 0.6619342293373092 \n",
            "Iteration: 4990 ---- loss: 0.6619277971021748 \n",
            "Iteration: 4991 ---- loss: 0.6619213655612227 \n",
            "Iteration: 4992 ---- loss: 0.6619149351474877 \n",
            "Iteration: 4993 ---- loss: 0.6619085058617192 \n",
            "Iteration: 4994 ---- loss: 0.6619020777046672 \n",
            "Iteration: 4995 ---- loss: 0.6618956506770804 \n",
            "Iteration: 4996 ---- loss: 0.6618892247797068 \n",
            "Iteration: 4997 ---- loss: 0.6618828000132939 \n",
            "Iteration: 4998 ---- loss: 0.6618763765726101 \n",
            "Iteration: 4999 ---- loss: 0.6618699540252841 \n",
            "Iteration: 5001 ---- loss: 0.6618571117121832 \n",
            "Iteration: 5002 ---- loss: 0.6618506921294384 \n",
            "Iteration: 5003 ---- loss: 0.6618442738785681 \n",
            "Iteration: 5004 ---- loss: 0.661837856818315 \n",
            "Iteration: 5005 ---- loss: 0.6618314407589929 \n",
            "Iteration: 5006 ---- loss: 0.6618250259887689 \n",
            "Iteration: 5007 ---- loss: 0.6618186122865496 \n",
            "Iteration: 5008 ---- loss: 0.6618121997973974 \n",
            "Iteration: 5009 ---- loss: 0.66180578829915 \n",
            "Iteration: 5010 ---- loss: 0.6617993779986647 \n",
            "Iteration: 5011 ---- loss: 0.6617929678718867 \n",
            "Iteration: 5012 ---- loss: 0.661786558948473 \n",
            "Iteration: 5013 ---- loss: 0.6617801510332264 \n",
            "Iteration: 5014 ---- loss: 0.6617737443977744 \n",
            "Iteration: 5015 ---- loss: 0.6617673388721528 \n",
            "Iteration: 5016 ---- loss: 0.6617609345079656 \n",
            "Iteration: 5017 ---- loss: 0.6617545312035891 \n",
            "Iteration: 5018 ---- loss: 0.661748129207781 \n",
            "Iteration: 5019 ---- loss: 0.6617417281553616 \n",
            "Iteration: 5020 ---- loss: 0.6617353284092213 \n",
            "Iteration: 5021 ---- loss: 0.6617289298042348 \n",
            "Iteration: 5022 ---- loss: 0.661722532242102 \n",
            "Iteration: 5023 ---- loss: 0.6617161360025667 \n",
            "Iteration: 5024 ---- loss: 0.6617097407040694 \n",
            "Iteration: 5025 ---- loss: 0.6617033467257872 \n",
            "Iteration: 5026 ---- loss: 0.6616969538727538 \n",
            "Iteration: 5027 ---- loss: 0.6616905622960911 \n",
            "Iteration: 5028 ---- loss: 0.6616841724792353 \n",
            "Iteration: 5029 ---- loss: 0.6616777836432232 \n",
            "Iteration: 5030 ---- loss: 0.661671396087793 \n",
            "Iteration: 5031 ---- loss: 0.661665009634051 \n",
            "Iteration: 5032 ---- loss: 0.6616586242905292 \n",
            "Iteration: 5033 ---- loss: 0.6616522400540303 \n",
            "Iteration: 5034 ---- loss: 0.6616458570101665 \n",
            "Iteration: 5035 ---- loss: 0.6616394750524334 \n",
            "Iteration: 5036 ---- loss: 0.6616330943603963 \n",
            "Iteration: 5037 ---- loss: 0.66162671419521 \n",
            "Iteration: 5038 ---- loss: 0.6616203346251789 \n",
            "Iteration: 5039 ---- loss: 0.661613956017093 \n",
            "Iteration: 5040 ---- loss: 0.6616075788955248 \n",
            "Iteration: 5041 ---- loss: 0.6616012029000923 \n",
            "Iteration: 5042 ---- loss: 0.6615948280128554 \n",
            "Iteration: 5043 ---- loss: 0.6615884543983189 \n",
            "Iteration: 5044 ---- loss: 0.6615820817102813 \n",
            "Iteration: 5045 ---- loss: 0.6615757104051891 \n",
            "Iteration: 5046 ---- loss: 0.661569340519128 \n",
            "Iteration: 5047 ---- loss: 0.6615629717363326 \n",
            "Iteration: 5048 ---- loss: 0.661556604068053 \n",
            "Iteration: 5049 ---- loss: 0.6615502376252809 \n",
            "Iteration: 5051 ---- loss: 0.661537508127645 \n",
            "Iteration: 5052 ---- loss: 0.6615311451651317 \n",
            "Iteration: 5053 ---- loss: 0.6615247833141971 \n",
            "Iteration: 5054 ---- loss: 0.6615184226136684 \n",
            "Iteration: 5055 ---- loss: 0.6615120630903107 \n",
            "Iteration: 5056 ---- loss: 0.6615057046022146 \n",
            "Iteration: 5057 ---- loss: 0.6614993473446289 \n",
            "Iteration: 5058 ---- loss: 0.6614929914394064 \n",
            "Iteration: 5059 ---- loss: 0.6614866366875587 \n",
            "Iteration: 5060 ---- loss: 0.6614802830678422 \n",
            "Iteration: 5061 ---- loss: 0.6614739307437214 \n",
            "Iteration: 5062 ---- loss: 0.6614675794654479 \n",
            "Iteration: 5063 ---- loss: 0.6614612294298375 \n",
            "Iteration: 5064 ---- loss: 0.6614548805650171 \n",
            "Iteration: 5065 ---- loss: 0.6614485328810864 \n",
            "Iteration: 5066 ---- loss: 0.6614421863027165 \n",
            "Iteration: 5067 ---- loss: 0.6614358410511216 \n",
            "Iteration: 5068 ---- loss: 0.661429496747273 \n",
            "Iteration: 5069 ---- loss: 0.6614231538700048 \n",
            "Iteration: 5070 ---- loss: 0.661416812021448 \n",
            "Iteration: 5071 ---- loss: 0.6614104714101293 \n",
            "Iteration: 5072 ---- loss: 0.6614041319477624 \n",
            "Iteration: 5073 ---- loss: 0.6613977936913542 \n",
            "Iteration: 5074 ---- loss: 0.661391456588179 \n",
            "Iteration: 5075 ---- loss: 0.6613851207415118 \n",
            "Iteration: 5076 ---- loss: 0.6613787859227668 \n",
            "Iteration: 5077 ---- loss: 0.6613724524305339 \n",
            "Iteration: 5078 ---- loss: 0.6613661201063397 \n",
            "Iteration: 5079 ---- loss: 0.6613597889165839 \n",
            "Iteration: 5080 ---- loss: 0.6613534589323677 \n",
            "Iteration: 5081 ---- loss: 0.6613471301733134 \n",
            "Iteration: 5082 ---- loss: 0.6613408024929981 \n",
            "Iteration: 5083 ---- loss: 0.6613344760351174 \n",
            "Iteration: 5084 ---- loss: 0.661328150931007 \n",
            "Iteration: 5085 ---- loss: 0.6613218267699352 \n",
            "Iteration: 5086 ---- loss: 0.661315504001859 \n",
            "Iteration: 5087 ---- loss: 0.6613091823867724 \n",
            "Iteration: 5088 ---- loss: 0.6613028619177936 \n",
            "Iteration: 5089 ---- loss: 0.6612965425951277 \n",
            "Iteration: 5090 ---- loss: 0.6612902246993747 \n",
            "Iteration: 5091 ---- loss: 0.661283907665029 \n",
            "Iteration: 5092 ---- loss: 0.6612775921442765 \n",
            "Iteration: 5093 ---- loss: 0.661271277558826 \n",
            "Iteration: 5094 ---- loss: 0.6612649643278599 \n",
            "Iteration: 5095 ---- loss: 0.6612586522334585 \n",
            "Iteration: 5096 ---- loss: 0.6612523408778097 \n",
            "Iteration: 5097 ---- loss: 0.6612460301035256 \n",
            "Iteration: 5098 ---- loss: 0.6612397223248866 \n",
            "Iteration: 5099 ---- loss: 0.6612334159217509 \n",
            "Iteration: 5101 ---- loss: 0.6612208065242162 \n",
            "Iteration: 5102 ---- loss: 0.6612145037410547 \n",
            "Iteration: 5103 ---- loss: 0.6612082020349769 \n",
            "Iteration: 5104 ---- loss: 0.6612019019598974 \n",
            "Iteration: 5105 ---- loss: 0.6611956030830316 \n",
            "Iteration: 5106 ---- loss: 0.6611893053296097 \n",
            "Iteration: 5107 ---- loss: 0.6611830088669938 \n",
            "Iteration: 5108 ---- loss: 0.6611767136155581 \n",
            "Iteration: 5109 ---- loss: 0.6611704194629606 \n",
            "Iteration: 5110 ---- loss: 0.6611641267618401 \n",
            "Iteration: 5111 ---- loss: 0.6611578349993319 \n",
            "Iteration: 5112 ---- loss: 0.6611515447075206 \n",
            "Iteration: 5113 ---- loss: 0.6611452554710521 \n",
            "Iteration: 5114 ---- loss: 0.6611389674604685 \n",
            "Iteration: 5115 ---- loss: 0.6611326808063966 \n",
            "Iteration: 5116 ---- loss: 0.6611263951244408 \n",
            "Iteration: 5117 ---- loss: 0.6611201107233123 \n",
            "Iteration: 5118 ---- loss: 0.6611138273712074 \n",
            "Iteration: 5119 ---- loss: 0.6611075452825931 \n",
            "Iteration: 5120 ---- loss: 0.6611012644214661 \n",
            "Iteration: 5121 ---- loss: 0.6610949848899168 \n",
            "Iteration: 5122 ---- loss: 0.6610887067598876 \n",
            "Iteration: 5123 ---- loss: 0.6610824304125426 \n",
            "Iteration: 5124 ---- loss: 0.6610761554363337 \n",
            "Iteration: 5125 ---- loss: 0.6610698814472467 \n",
            "Iteration: 5126 ---- loss: 0.6610636089205046 \n",
            "Iteration: 5127 ---- loss: 0.6610573375017617 \n",
            "Iteration: 5128 ---- loss: 0.6610510671988682 \n",
            "Iteration: 5129 ---- loss: 0.6610447983748611 \n",
            "Iteration: 5130 ---- loss: 0.6610385305866784 \n",
            "Iteration: 5131 ---- loss: 0.6610322640708746 \n",
            "Iteration: 5132 ---- loss: 0.6610259988793555 \n",
            "Iteration: 5133 ---- loss: 0.6610197344427423 \n",
            "Iteration: 5134 ---- loss: 0.6610134711737785 \n",
            "Iteration: 5135 ---- loss: 0.6610072092198201 \n",
            "Iteration: 5136 ---- loss: 0.661000948443839 \n",
            "Iteration: 5137 ---- loss: 0.6609946891630446 \n",
            "Iteration: 5138 ---- loss: 0.6609884313134508 \n",
            "Iteration: 5139 ---- loss: 0.6609821746339144 \n",
            "Iteration: 5140 ---- loss: 0.6609759191360696 \n",
            "Iteration: 5141 ---- loss: 0.6609696648152338 \n",
            "Iteration: 5142 ---- loss: 0.6609634117472591 \n",
            "Iteration: 5143 ---- loss: 0.6609571599614114 \n",
            "Iteration: 5144 ---- loss: 0.660950909308336 \n",
            "Iteration: 5145 ---- loss: 0.6609446594102844 \n",
            "Iteration: 5146 ---- loss: 0.6609384111160097 \n",
            "Iteration: 5147 ---- loss: 0.6609321651431349 \n",
            "Iteration: 5148 ---- loss: 0.6609259202920156 \n",
            "Iteration: 5149 ---- loss: 0.6609196766001912 \n",
            "Iteration: 5151 ---- loss: 0.6609071940172471 \n",
            "Iteration: 5152 ---- loss: 0.6609009555831046 \n",
            "Iteration: 5153 ---- loss: 0.6608947186670331 \n",
            "Iteration: 5154 ---- loss: 0.6608884831468699 \n",
            "Iteration: 5155 ---- loss: 0.6608822487297684 \n",
            "Iteration: 5156 ---- loss: 0.6608760149559102 \n",
            "Iteration: 5157 ---- loss: 0.6608697813396295 \n",
            "Iteration: 5158 ---- loss: 0.6608635487752299 \n",
            "Iteration: 5159 ---- loss: 0.6608573174649808 \n",
            "Iteration: 5160 ---- loss: 0.660851087450788 \n",
            "Iteration: 5161 ---- loss: 0.6608448586758944 \n",
            "Iteration: 5162 ---- loss: 0.6608386306015397 \n",
            "Iteration: 5163 ---- loss: 0.6608324038700871 \n",
            "Iteration: 5164 ---- loss: 0.6608261782275631 \n",
            "Iteration: 5165 ---- loss: 0.6608199536789204 \n",
            "Iteration: 5166 ---- loss: 0.6608137303699285 \n",
            "Iteration: 5167 ---- loss: 0.6608075083578369 \n",
            "Iteration: 5168 ---- loss: 0.6608012870580132 \n",
            "Iteration: 5169 ---- loss: 0.6607950661314639 \n",
            "Iteration: 5170 ---- loss: 0.660788846340951 \n",
            "Iteration: 5171 ---- loss: 0.6607826278074813 \n",
            "Iteration: 5172 ---- loss: 0.660776410295855 \n",
            "Iteration: 5173 ---- loss: 0.6607701938545981 \n",
            "Iteration: 5174 ---- loss: 0.6607639787650236 \n",
            "Iteration: 5175 ---- loss: 0.6607577649541689 \n",
            "Iteration: 5176 ---- loss: 0.6607515523089993 \n",
            "Iteration: 5177 ---- loss: 0.6607453409606356 \n",
            "Iteration: 5178 ---- loss: 0.6607391309230449 \n",
            "Iteration: 5179 ---- loss: 0.6607329223097157 \n",
            "Iteration: 5180 ---- loss: 0.6607267146038688 \n",
            "Iteration: 5181 ---- loss: 0.6607205082204005 \n",
            "Iteration: 5182 ---- loss: 0.6607143030665918 \n",
            "Iteration: 5183 ---- loss: 0.6607080991284345 \n",
            "Iteration: 5184 ---- loss: 0.6607018964497948 \n",
            "Iteration: 5185 ---- loss: 0.6606956951544029 \n",
            "Iteration: 5186 ---- loss: 0.6606894950863361 \n",
            "Iteration: 5187 ---- loss: 0.6606832961955604 \n",
            "Iteration: 5188 ---- loss: 0.6606770985926196 \n",
            "Iteration: 5189 ---- loss: 0.6606709023380877 \n",
            "Iteration: 5190 ---- loss: 0.660664707461458 \n",
            "Iteration: 5191 ---- loss: 0.6606585139245391 \n",
            "Iteration: 5192 ---- loss: 0.6606523211032963 \n",
            "Iteration: 5193 ---- loss: 0.6606461296606995 \n",
            "Iteration: 5194 ---- loss: 0.6606399394124529 \n",
            "Iteration: 5195 ---- loss: 0.6606337503957203 \n",
            "Iteration: 5196 ---- loss: 0.6606275626414781 \n",
            "Iteration: 5197 ---- loss: 0.6606213762061427 \n",
            "Iteration: 5198 ---- loss: 0.6606151910281272 \n",
            "Iteration: 5199 ---- loss: 0.6606090069385105 \n",
            "Iteration: 5201 ---- loss: 0.6605966426098238 \n",
            "Iteration: 5202 ---- loss: 0.6605904622902168 \n",
            "Iteration: 5203 ---- loss: 0.6605842834287137 \n",
            "Iteration: 5204 ---- loss: 0.6605781058188004 \n",
            "Iteration: 5205 ---- loss: 0.6605719293730848 \n",
            "Iteration: 5206 ---- loss: 0.6605657541021027 \n",
            "Iteration: 5207 ---- loss: 0.660559580116293 \n",
            "Iteration: 5208 ---- loss: 0.6605534075046189 \n",
            "Iteration: 5209 ---- loss: 0.6605472363011431 \n",
            "Iteration: 5210 ---- loss: 0.6605410664003932 \n",
            "Iteration: 5211 ---- loss: 0.6605348977084734 \n",
            "Iteration: 5212 ---- loss: 0.6605287309643056 \n",
            "Iteration: 5213 ---- loss: 0.6605225689406029 \n",
            "Iteration: 5214 ---- loss: 0.6605164082887596 \n",
            "Iteration: 5215 ---- loss: 0.6605102489460858 \n",
            "Iteration: 5216 ---- loss: 0.6605040908563155 \n",
            "Iteration: 5217 ---- loss: 0.6604979339615318 \n",
            "Iteration: 5218 ---- loss: 0.660491778166519 \n",
            "Iteration: 5219 ---- loss: 0.6604856237138441 \n",
            "Iteration: 5220 ---- loss: 0.6604794706027838 \n",
            "Iteration: 5221 ---- loss: 0.6604733187453308 \n",
            "Iteration: 5222 ---- loss: 0.6604671681869724 \n",
            "Iteration: 5223 ---- loss: 0.6604610189434811 \n",
            "Iteration: 5224 ---- loss: 0.6604548707124211 \n",
            "Iteration: 5225 ---- loss: 0.6604487241128096 \n",
            "Iteration: 5226 ---- loss: 0.6604425793239407 \n",
            "Iteration: 5227 ---- loss: 0.6604364358599364 \n",
            "Iteration: 5228 ---- loss: 0.6604302936891975 \n",
            "Iteration: 5229 ---- loss: 0.6604241528704012 \n",
            "Iteration: 5230 ---- loss: 0.6604180133116206 \n",
            "Iteration: 5231 ---- loss: 0.6604118749086245 \n",
            "Iteration: 5232 ---- loss: 0.6604057377570924 \n",
            "Iteration: 5233 ---- loss: 0.6603996019810598 \n",
            "Iteration: 5234 ---- loss: 0.6603934676130587 \n",
            "Iteration: 5235 ---- loss: 0.6603873344067409 \n",
            "Iteration: 5236 ---- loss: 0.6603812025185343 \n",
            "Iteration: 5237 ---- loss: 0.6603750719914864 \n",
            "Iteration: 5238 ---- loss: 0.660368942845424 \n",
            "Iteration: 5239 ---- loss: 0.6603628149446372 \n",
            "Iteration: 5240 ---- loss: 0.6603566884253802 \n",
            "Iteration: 5241 ---- loss: 0.6603505631924874 \n",
            "Iteration: 5242 ---- loss: 0.6603444392788054 \n",
            "Iteration: 5243 ---- loss: 0.6603383166487308 \n",
            "Iteration: 5244 ---- loss: 0.6603321958368277 \n",
            "Iteration: 5245 ---- loss: 0.660326076520226 \n",
            "Iteration: 5246 ---- loss: 0.6603199584052194 \n",
            "Iteration: 5247 ---- loss: 0.6603138416097483 \n",
            "Iteration: 5248 ---- loss: 0.6603077261525137 \n",
            "Iteration: 5249 ---- loss: 0.6603016120692259 \n",
            "Iteration: 5251 ---- loss: 0.66028938774869 \n",
            "Iteration: 5252 ---- loss: 0.6602832777492682 \n",
            "Iteration: 5253 ---- loss: 0.6602771689564941 \n",
            "Iteration: 5254 ---- loss: 0.6602710614239911 \n",
            "Iteration: 5255 ---- loss: 0.6602649553303649 \n",
            "Iteration: 5256 ---- loss: 0.6602588506250193 \n",
            "Iteration: 5257 ---- loss: 0.6602527471055272 \n",
            "Iteration: 5258 ---- loss: 0.6602466449703237 \n",
            "Iteration: 5259 ---- loss: 0.6602405442791536 \n",
            "Iteration: 5260 ---- loss: 0.6602344447819657 \n",
            "Iteration: 5261 ---- loss: 0.660228346520976 \n",
            "Iteration: 5262 ---- loss: 0.6602222491479829 \n",
            "Iteration: 5263 ---- loss: 0.6602161529573422 \n",
            "Iteration: 5264 ---- loss: 0.6602100581054945 \n",
            "Iteration: 5265 ---- loss: 0.6602039647046631 \n",
            "Iteration: 5266 ---- loss: 0.6601978724050612 \n",
            "Iteration: 5267 ---- loss: 0.6601917812839472 \n",
            "Iteration: 5268 ---- loss: 0.6601856913587142 \n",
            "Iteration: 5269 ---- loss: 0.6601796027580017 \n",
            "Iteration: 5270 ---- loss: 0.6601735156191701 \n",
            "Iteration: 5271 ---- loss: 0.6601674301360151 \n",
            "Iteration: 5272 ---- loss: 0.6601613458938853 \n",
            "Iteration: 5273 ---- loss: 0.6601552630697145 \n",
            "Iteration: 5274 ---- loss: 0.6601491816240469 \n",
            "Iteration: 5275 ---- loss: 0.6601431013981965 \n",
            "Iteration: 5276 ---- loss: 0.6601370226418637 \n",
            "Iteration: 5277 ---- loss: 0.6601309451898484 \n",
            "Iteration: 5278 ---- loss: 0.6601248690064131 \n",
            "Iteration: 5279 ---- loss: 0.6601187943467715 \n",
            "Iteration: 5280 ---- loss: 0.6601127208945302 \n",
            "Iteration: 5281 ---- loss: 0.6601066487758035 \n",
            "Iteration: 5282 ---- loss: 0.6601005784090024 \n",
            "Iteration: 5283 ---- loss: 0.6600945092422218 \n",
            "Iteration: 5284 ---- loss: 0.6600884414969742 \n",
            "Iteration: 5285 ---- loss: 0.6600823751323085 \n",
            "Iteration: 5286 ---- loss: 0.6600763100064982 \n",
            "Iteration: 5287 ---- loss: 0.6600702463963527 \n",
            "Iteration: 5288 ---- loss: 0.6600641846367996 \n",
            "Iteration: 5289 ---- loss: 0.6600581251636367 \n",
            "Iteration: 5290 ---- loss: 0.6600520671615869 \n",
            "Iteration: 5291 ---- loss: 0.6600460103605608 \n",
            "Iteration: 5292 ---- loss: 0.6600399550525343 \n",
            "Iteration: 5293 ---- loss: 0.6600339010215935 \n",
            "Iteration: 5294 ---- loss: 0.6600278483249492 \n",
            "Iteration: 5295 ---- loss: 0.6600217970480458 \n",
            "Iteration: 5296 ---- loss: 0.660015746966375 \n",
            "Iteration: 5297 ---- loss: 0.660009698411773 \n",
            "Iteration: 5298 ---- loss: 0.6600036511192504 \n",
            "Iteration: 5299 ---- loss: 0.6599976053372248 \n",
            "Iteration: 5301 ---- loss: 0.6599855178589574 \n",
            "Iteration: 5302 ---- loss: 0.6599794762949495 \n",
            "Iteration: 5303 ---- loss: 0.6599734359066318 \n",
            "Iteration: 5304 ---- loss: 0.659967396997008 \n",
            "Iteration: 5305 ---- loss: 0.6599613593965744 \n",
            "Iteration: 5306 ---- loss: 0.6599553231577685 \n",
            "Iteration: 5307 ---- loss: 0.6599492885668143 \n",
            "Iteration: 5308 ---- loss: 0.6599432560781711 \n",
            "Iteration: 5309 ---- loss: 0.6599372241979129 \n",
            "Iteration: 5310 ---- loss: 0.6599311946042137 \n",
            "Iteration: 5311 ---- loss: 0.6599251665562461 \n",
            "Iteration: 5312 ---- loss: 0.6599191397727253 \n",
            "Iteration: 5313 ---- loss: 0.6599131144275703 \n",
            "Iteration: 5314 ---- loss: 0.6599070904020601 \n",
            "Iteration: 5315 ---- loss: 0.6599010675834213 \n",
            "Iteration: 5316 ---- loss: 0.6598950461928351 \n",
            "Iteration: 5317 ---- loss: 0.6598890260721914 \n",
            "Iteration: 5318 ---- loss: 0.6598830074901997 \n",
            "Iteration: 5319 ---- loss: 0.6598769900889979 \n",
            "Iteration: 5320 ---- loss: 0.6598709742837878 \n",
            "Iteration: 5321 ---- loss: 0.659864959708524 \n",
            "Iteration: 5322 ---- loss: 0.6598589468747177 \n",
            "Iteration: 5323 ---- loss: 0.6598529352216133 \n",
            "Iteration: 5324 ---- loss: 0.6598469250915096 \n",
            "Iteration: 5325 ---- loss: 0.6598409162265537 \n",
            "Iteration: 5326 ---- loss: 0.6598349088716765 \n",
            "Iteration: 5327 ---- loss: 0.6598289028864887 \n",
            "Iteration: 5328 ---- loss: 0.6598228989542414 \n",
            "Iteration: 5329 ---- loss: 0.6598168961976109 \n",
            "Iteration: 5330 ---- loss: 0.6598108946805477 \n",
            "Iteration: 5331 ---- loss: 0.6598048943365858 \n",
            "Iteration: 5332 ---- loss: 0.659798895462839 \n",
            "Iteration: 5333 ---- loss: 0.6597928979365639 \n",
            "Iteration: 5334 ---- loss: 0.6597869019824723 \n",
            "Iteration: 5335 ---- loss: 0.6597809072387857 \n",
            "Iteration: 5336 ---- loss: 0.6597749142216799 \n",
            "Iteration: 5337 ---- loss: 0.6597689226489476 \n",
            "Iteration: 5338 ---- loss: 0.6597629324993634 \n",
            "Iteration: 5339 ---- loss: 0.6597569436378843 \n",
            "Iteration: 5340 ---- loss: 0.6597509563684993 \n",
            "Iteration: 5341 ---- loss: 0.6597449702396548 \n",
            "Iteration: 5342 ---- loss: 0.6597389857946726 \n",
            "Iteration: 5343 ---- loss: 0.659733002193149 \n",
            "Iteration: 5344 ---- loss: 0.6597270204711979 \n",
            "Iteration: 5345 ---- loss: 0.6597210399876476 \n",
            "Iteration: 5346 ---- loss: 0.6597150610042439 \n",
            "Iteration: 5347 ---- loss: 0.6597090833068391 \n",
            "Iteration: 5348 ---- loss: 0.6597031071418581 \n",
            "Iteration: 5349 ---- loss: 0.6596971323007091 \n",
            "Iteration: 5351 ---- loss: 0.6596851868820383 \n",
            "Iteration: 5352 ---- loss: 0.659679216395108 \n",
            "Iteration: 5353 ---- loss: 0.6596732479431852 \n",
            "Iteration: 5354 ---- loss: 0.6596672808053576 \n",
            "Iteration: 5355 ---- loss: 0.6596613151878623 \n",
            "Iteration: 5356 ---- loss: 0.6596553508035639 \n",
            "Iteration: 5357 ---- loss: 0.6596493880724144 \n",
            "Iteration: 5358 ---- loss: 0.6596434269593491 \n",
            "Iteration: 5359 ---- loss: 0.6596374661969236 \n",
            "Iteration: 5360 ---- loss: 0.6596315079559216 \n",
            "Iteration: 5361 ---- loss: 0.6596255511730853 \n",
            "Iteration: 5362 ---- loss: 0.6596195960079307 \n",
            "Iteration: 5363 ---- loss: 0.6596136421374336 \n",
            "Iteration: 5364 ---- loss: 0.65960768969586 \n",
            "Iteration: 5365 ---- loss: 0.6596017386976644 \n",
            "Iteration: 5366 ---- loss: 0.6595957889850119 \n",
            "Iteration: 5367 ---- loss: 0.6595898408779491 \n",
            "Iteration: 5368 ---- loss: 0.6595838944282496 \n",
            "Iteration: 5369 ---- loss: 0.6595779495578313 \n",
            "Iteration: 5370 ---- loss: 0.6595720057385246 \n",
            "Iteration: 5371 ---- loss: 0.6595660631529286 \n",
            "Iteration: 5372 ---- loss: 0.6595601218414427 \n",
            "Iteration: 5373 ---- loss: 0.65955418126069 \n",
            "Iteration: 5374 ---- loss: 0.6595482423109991 \n",
            "Iteration: 5375 ---- loss: 0.659542304609233 \n",
            "Iteration: 5376 ---- loss: 0.6595363683752704 \n",
            "Iteration: 5377 ---- loss: 0.6595304336596692 \n",
            "Iteration: 5378 ---- loss: 0.659524500222398 \n",
            "Iteration: 5379 ---- loss: 0.6595185683264514 \n",
            "Iteration: 5380 ---- loss: 0.6595126377956397 \n",
            "Iteration: 5381 ---- loss: 0.6595067086549007 \n",
            "Iteration: 5382 ---- loss: 0.6595007810432674 \n",
            "Iteration: 5383 ---- loss: 0.6594948547423397 \n",
            "Iteration: 5384 ---- loss: 0.6594889298985083 \n",
            "Iteration: 5385 ---- loss: 0.6594830065157091 \n",
            "Iteration: 5386 ---- loss: 0.6594770843028317 \n",
            "Iteration: 5387 ---- loss: 0.6594711637152374 \n",
            "Iteration: 5388 ---- loss: 0.6594652445556728 \n",
            "Iteration: 5389 ---- loss: 0.6594593267624984 \n",
            "Iteration: 5390 ---- loss: 0.6594534104840132 \n",
            "Iteration: 5391 ---- loss: 0.6594474954830459 \n",
            "Iteration: 5392 ---- loss: 0.6594415819403214 \n",
            "Iteration: 5393 ---- loss: 0.6594356698669719 \n",
            "Iteration: 5394 ---- loss: 0.6594297591721467 \n",
            "Iteration: 5395 ---- loss: 0.659423849897707 \n",
            "Iteration: 5396 ---- loss: 0.6594179421031682 \n",
            "Iteration: 5397 ---- loss: 0.6594120356962123 \n",
            "Iteration: 5398 ---- loss: 0.6594061307166837 \n",
            "Iteration: 5399 ---- loss: 0.6594002272407171 \n",
            "Iteration: 5401 ---- loss: 0.6593884245614076 \n",
            "Iteration: 5402 ---- loss: 0.6593825249829807 \n",
            "Iteration: 5403 ---- loss: 0.6593766268068637 \n",
            "Iteration: 5404 ---- loss: 0.659370730164087 \n",
            "Iteration: 5405 ---- loss: 0.659364836082446 \n",
            "Iteration: 5406 ---- loss: 0.6593589435190897 \n",
            "Iteration: 5407 ---- loss: 0.6593530523356593 \n",
            "Iteration: 5408 ---- loss: 0.659347162523823 \n",
            "Iteration: 5409 ---- loss: 0.6593412742859459 \n",
            "Iteration: 5410 ---- loss: 0.6593353874237179 \n",
            "Iteration: 5411 ---- loss: 0.659329501945476 \n",
            "Iteration: 5412 ---- loss: 0.659323617931703 \n",
            "Iteration: 5413 ---- loss: 0.6593177354731815 \n",
            "Iteration: 5414 ---- loss: 0.6593118543024196 \n",
            "Iteration: 5415 ---- loss: 0.6593059744983916 \n",
            "Iteration: 5416 ---- loss: 0.6593000963033312 \n",
            "Iteration: 5417 ---- loss: 0.6592942194317808 \n",
            "Iteration: 5418 ---- loss: 0.6592883438049921 \n",
            "Iteration: 5419 ---- loss: 0.6592824688795705 \n",
            "Iteration: 5420 ---- loss: 0.6592765954520965 \n",
            "Iteration: 5421 ---- loss: 0.659270723343781 \n",
            "Iteration: 5422 ---- loss: 0.6592648527302477 \n",
            "Iteration: 5423 ---- loss: 0.659258983335164 \n",
            "Iteration: 5424 ---- loss: 0.6592531150900238 \n",
            "Iteration: 5425 ---- loss: 0.6592472482471246 \n",
            "Iteration: 5426 ---- loss: 0.6592413828465759 \n",
            "Iteration: 5427 ---- loss: 0.6592355188478985 \n",
            "Iteration: 5428 ---- loss: 0.6592296564329126 \n",
            "Iteration: 5429 ---- loss: 0.6592237953369611 \n",
            "Iteration: 5430 ---- loss: 0.6592179356597002 \n",
            "Iteration: 5431 ---- loss: 0.6592120774484463 \n",
            "Iteration: 5432 ---- loss: 0.6592062207650239 \n",
            "Iteration: 5433 ---- loss: 0.6592003655034846 \n",
            "Iteration: 5434 ---- loss: 0.659194510881919 \n",
            "Iteration: 5435 ---- loss: 0.6591886569225539 \n",
            "Iteration: 5436 ---- loss: 0.6591828048624291 \n",
            "Iteration: 5437 ---- loss: 0.6591769544751183 \n",
            "Iteration: 5438 ---- loss: 0.6591711055860846 \n",
            "Iteration: 5439 ---- loss: 0.6591652580249437 \n",
            "Iteration: 5440 ---- loss: 0.6591594115482428 \n",
            "Iteration: 5441 ---- loss: 0.6591535664612215 \n",
            "Iteration: 5442 ---- loss: 0.6591477225882053 \n",
            "Iteration: 5443 ---- loss: 0.659141880242483 \n",
            "Iteration: 5444 ---- loss: 0.6591360393299366 \n",
            "Iteration: 5445 ---- loss: 0.6591301997952803 \n",
            "Iteration: 5446 ---- loss: 0.6591243617819762 \n",
            "Iteration: 5447 ---- loss: 0.659118525209765 \n",
            "Iteration: 5448 ---- loss: 0.659112690957487 \n",
            "Iteration: 5449 ---- loss: 0.6591068591446035 \n",
            "Iteration: 5451 ---- loss: 0.6590951995971234 \n",
            "Iteration: 5452 ---- loss: 0.6590893721220723 \n",
            "Iteration: 5453 ---- loss: 0.6590835461583066 \n",
            "Iteration: 5454 ---- loss: 0.6590777214544936 \n",
            "Iteration: 5455 ---- loss: 0.6590718986871826 \n",
            "Iteration: 5456 ---- loss: 0.6590660775057835 \n",
            "Iteration: 5457 ---- loss: 0.6590602577533607 \n",
            "Iteration: 5458 ---- loss: 0.6590544392301078 \n",
            "Iteration: 5459 ---- loss: 0.659048622455112 \n",
            "Iteration: 5460 ---- loss: 0.6590428070321097 \n",
            "Iteration: 5461 ---- loss: 0.6590369928066331 \n",
            "Iteration: 5462 ---- loss: 0.6590311799922037 \n",
            "Iteration: 5463 ---- loss: 0.6590253687166532 \n",
            "Iteration: 5464 ---- loss: 0.659019558893925 \n",
            "Iteration: 5465 ---- loss: 0.6590137505632984 \n",
            "Iteration: 5466 ---- loss: 0.6590079436262839 \n",
            "Iteration: 5467 ---- loss: 0.6590021382020054 \n",
            "Iteration: 5468 ---- loss: 0.6589963342230478 \n",
            "Iteration: 5469 ---- loss: 0.6589905316863259 \n",
            "Iteration: 5470 ---- loss: 0.658984730683113 \n",
            "Iteration: 5471 ---- loss: 0.6589789309051729 \n",
            "Iteration: 5472 ---- loss: 0.6589731325747531 \n",
            "Iteration: 5473 ---- loss: 0.658967335731655 \n",
            "Iteration: 5474 ---- loss: 0.6589615404484279 \n",
            "Iteration: 5475 ---- loss: 0.6589557465008277 \n",
            "Iteration: 5476 ---- loss: 0.6589499540268672 \n",
            "Iteration: 5477 ---- loss: 0.6589441630497186 \n",
            "Iteration: 5478 ---- loss: 0.6589383740769678 \n",
            "Iteration: 5479 ---- loss: 0.658932586541033 \n",
            "Iteration: 5480 ---- loss: 0.658926800429451 \n",
            "Iteration: 5481 ---- loss: 0.6589210159179653 \n",
            "Iteration: 5482 ---- loss: 0.658915232758755 \n",
            "Iteration: 5483 ---- loss: 0.6589094510673352 \n",
            "Iteration: 5484 ---- loss: 0.6589036713576578 \n",
            "Iteration: 5485 ---- loss: 0.6588978933019028 \n",
            "Iteration: 5486 ---- loss: 0.6588921168124495 \n",
            "Iteration: 5487 ---- loss: 0.6588863417164559 \n",
            "Iteration: 5488 ---- loss: 0.6588805680681785 \n",
            "Iteration: 5489 ---- loss: 0.6588747959561339 \n",
            "Iteration: 5490 ---- loss: 0.6588690253949375 \n",
            "Iteration: 5491 ---- loss: 0.6588632566385348 \n",
            "Iteration: 5492 ---- loss: 0.6588574892790232 \n",
            "Iteration: 5493 ---- loss: 0.658851723563359 \n",
            "Iteration: 5494 ---- loss: 0.6588459591193784 \n",
            "Iteration: 5495 ---- loss: 0.6588401962430946 \n",
            "Iteration: 5496 ---- loss: 0.6588344360841906 \n",
            "Iteration: 5497 ---- loss: 0.6588286788488456 \n",
            "Iteration: 5498 ---- loss: 0.6588229230885853 \n",
            "Iteration: 5499 ---- loss: 0.6588171689093898 \n",
            "Iteration: 5501 ---- loss: 0.6588056651958898 \n",
            "Iteration: 5502 ---- loss: 0.6587999157393734 \n",
            "Iteration: 5503 ---- loss: 0.658794167655927 \n",
            "Iteration: 5504 ---- loss: 0.658788421187338 \n",
            "Iteration: 5505 ---- loss: 0.658782676172951 \n",
            "Iteration: 5506 ---- loss: 0.6587769325848011 \n",
            "Iteration: 5507 ---- loss: 0.658771190330285 \n",
            "Iteration: 5508 ---- loss: 0.6587654495418332 \n",
            "Iteration: 5509 ---- loss: 0.6587597102151204 \n",
            "Iteration: 5510 ---- loss: 0.6587539722882864 \n",
            "Iteration: 5511 ---- loss: 0.6587482360167716 \n",
            "Iteration: 5512 ---- loss: 0.6587425011464387 \n",
            "Iteration: 5513 ---- loss: 0.6587367677494841 \n",
            "Iteration: 5514 ---- loss: 0.6587310359799136 \n",
            "Iteration: 5515 ---- loss: 0.6587253055901524 \n",
            "Iteration: 5516 ---- loss: 0.6587195766975568 \n",
            "Iteration: 5517 ---- loss: 0.6587138493955187 \n",
            "Iteration: 5518 ---- loss: 0.6587081234718958 \n",
            "Iteration: 5519 ---- loss: 0.6587023985815246 \n",
            "Iteration: 5520 ---- loss: 0.658696675189558 \n",
            "Iteration: 5521 ---- loss: 0.6586909532194732 \n",
            "Iteration: 5522 ---- loss: 0.6586852325972214 \n",
            "Iteration: 5523 ---- loss: 0.6586795135402128 \n",
            "Iteration: 5524 ---- loss: 0.6586737956925619 \n",
            "Iteration: 5525 ---- loss: 0.6586680792669078 \n",
            "Iteration: 5526 ---- loss: 0.6586623643745643 \n",
            "Iteration: 5527 ---- loss: 0.6586566510798765 \n",
            "Iteration: 5528 ---- loss: 0.6586509391455266 \n",
            "Iteration: 5529 ---- loss: 0.6586452287167532 \n",
            "Iteration: 5530 ---- loss: 0.6586395195298722 \n",
            "Iteration: 5531 ---- loss: 0.6586338107082392 \n",
            "Iteration: 5532 ---- loss: 0.6586281033116759 \n",
            "Iteration: 5533 ---- loss: 0.6586223974814147 \n",
            "Iteration: 5534 ---- loss: 0.6586166930818459 \n",
            "Iteration: 5535 ---- loss: 0.6586109901954503 \n",
            "Iteration: 5536 ---- loss: 0.6586052888786307 \n",
            "Iteration: 5537 ---- loss: 0.658599588976038 \n",
            "Iteration: 5538 ---- loss: 0.6585938901966614 \n",
            "Iteration: 5539 ---- loss: 0.6585881924948537 \n",
            "Iteration: 5540 ---- loss: 0.6585824962620878 \n",
            "Iteration: 5541 ---- loss: 0.6585768015264136 \n",
            "Iteration: 5542 ---- loss: 0.6585711083155601 \n",
            "Iteration: 5543 ---- loss: 0.6585654165767195 \n",
            "Iteration: 5544 ---- loss: 0.6585597264119067 \n",
            "Iteration: 5545 ---- loss: 0.6585540386209083 \n",
            "Iteration: 5546 ---- loss: 0.6585483523173594 \n",
            "Iteration: 5547 ---- loss: 0.6585426674823146 \n",
            "Iteration: 5548 ---- loss: 0.6585369841708474 \n",
            "Iteration: 5549 ---- loss: 0.6585313024087124 \n",
            "Iteration: 5551 ---- loss: 0.6585199427839848 \n",
            "Iteration: 5552 ---- loss: 0.6585142656243702 \n",
            "Iteration: 5553 ---- loss: 0.6585085899051673 \n",
            "Iteration: 5554 ---- loss: 0.6585029157032253 \n",
            "Iteration: 5555 ---- loss: 0.6584972431316376 \n",
            "Iteration: 5556 ---- loss: 0.6584915719417312 \n",
            "Iteration: 5557 ---- loss: 0.658485902200033 \n",
            "Iteration: 5558 ---- loss: 0.6584802340820537 \n",
            "Iteration: 5559 ---- loss: 0.6584745674358338 \n",
            "Iteration: 5560 ---- loss: 0.6584689020989565 \n",
            "Iteration: 5561 ---- loss: 0.6584632382726121 \n",
            "Iteration: 5562 ---- loss: 0.6584575759980619 \n",
            "Iteration: 5563 ---- loss: 0.6584519152082371 \n",
            "Iteration: 5564 ---- loss: 0.6584462559636473 \n",
            "Iteration: 5565 ---- loss: 0.6584405983053735 \n",
            "Iteration: 5566 ---- loss: 0.6584349425941822 \n",
            "Iteration: 5567 ---- loss: 0.658429288396363 \n",
            "Iteration: 5568 ---- loss: 0.6584236355297619 \n",
            "Iteration: 5569 ---- loss: 0.658417984086741 \n",
            "Iteration: 5570 ---- loss: 0.6584123339943457 \n",
            "Iteration: 5571 ---- loss: 0.6584066854317586 \n",
            "Iteration: 5572 ---- loss: 0.6584010383247649 \n",
            "Iteration: 5573 ---- loss: 0.6583953927430445 \n",
            "Iteration: 5574 ---- loss: 0.6583897487691366 \n",
            "Iteration: 5575 ---- loss: 0.6583841063204909 \n",
            "Iteration: 5576 ---- loss: 0.6583784697868514 \n",
            "Iteration: 5577 ---- loss: 0.6583728371330865 \n",
            "Iteration: 5578 ---- loss: 0.6583672059623155 \n",
            "Iteration: 5579 ---- loss: 0.6583615763932058 \n",
            "Iteration: 5580 ---- loss: 0.6583559485611917 \n",
            "Iteration: 5581 ---- loss: 0.6583503221482455 \n",
            "Iteration: 5582 ---- loss: 0.6583446984574328 \n",
            "Iteration: 5583 ---- loss: 0.6583390764817632 \n",
            "Iteration: 5584 ---- loss: 0.6583334559973181 \n",
            "Iteration: 5585 ---- loss: 0.6583278371164973 \n",
            "Iteration: 5586 ---- loss: 0.658322219779271 \n",
            "Iteration: 5587 ---- loss: 0.6583166039652576 \n",
            "Iteration: 5588 ---- loss: 0.6583109895895317 \n",
            "Iteration: 5589 ---- loss: 0.6583053769162782 \n",
            "Iteration: 5590 ---- loss: 0.6582997657513509 \n",
            "Iteration: 5591 ---- loss: 0.6582941560654475 \n",
            "Iteration: 5592 ---- loss: 0.6582885479182601 \n",
            "Iteration: 5593 ---- loss: 0.6582829414485482 \n",
            "Iteration: 5594 ---- loss: 0.6582773364457561 \n",
            "Iteration: 5595 ---- loss: 0.6582717329201461 \n",
            "Iteration: 5596 ---- loss: 0.6582661310465143 \n",
            "Iteration: 5597 ---- loss: 0.6582605306876287 \n",
            "Iteration: 5598 ---- loss: 0.658254931869201 \n",
            "Iteration: 5599 ---- loss: 0.6582493346102781 \n",
            "Iteration: 5601 ---- loss: 0.658238144709465 \n",
            "Iteration: 5602 ---- loss: 0.6582325520942333 \n",
            "Iteration: 5603 ---- loss: 0.6582269610211409 \n",
            "Iteration: 5604 ---- loss: 0.6582213714356303 \n",
            "Iteration: 5605 ---- loss: 0.6582157835156766 \n",
            "Iteration: 5606 ---- loss: 0.6582101971098566 \n",
            "Iteration: 5607 ---- loss: 0.6582046121784737 \n",
            "Iteration: 5608 ---- loss: 0.6581990292274854 \n",
            "Iteration: 5609 ---- loss: 0.6581934495605702 \n",
            "Iteration: 5610 ---- loss: 0.6581878723028157 \n",
            "Iteration: 5611 ---- loss: 0.6581822966852894 \n",
            "Iteration: 5612 ---- loss: 0.6581767226322554 \n",
            "Iteration: 5613 ---- loss: 0.6581711500714185 \n",
            "Iteration: 5614 ---- loss: 0.6581655792386389 \n",
            "Iteration: 5615 ---- loss: 0.6581600098948276 \n",
            "Iteration: 5616 ---- loss: 0.6581544421387896 \n",
            "Iteration: 5617 ---- loss: 0.658148876133566 \n",
            "Iteration: 5618 ---- loss: 0.6581433115341304 \n",
            "Iteration: 5619 ---- loss: 0.6581377484357256 \n",
            "Iteration: 5620 ---- loss: 0.6581321868880617 \n",
            "Iteration: 5621 ---- loss: 0.6581266269851891 \n",
            "Iteration: 5622 ---- loss: 0.6581210685242508 \n",
            "Iteration: 5623 ---- loss: 0.6581155117950692 \n",
            "Iteration: 5624 ---- loss: 0.6581099579703245 \n",
            "Iteration: 5625 ---- loss: 0.658104405597759 \n",
            "Iteration: 5626 ---- loss: 0.6580988549050083 \n",
            "Iteration: 5627 ---- loss: 0.6580933058151305 \n",
            "Iteration: 5628 ---- loss: 0.6580877581285044 \n",
            "Iteration: 5629 ---- loss: 0.6580822123090565 \n",
            "Iteration: 5630 ---- loss: 0.6580766678537344 \n",
            "Iteration: 5631 ---- loss: 0.6580711249372694 \n",
            "Iteration: 5632 ---- loss: 0.6580655837514797 \n",
            "Iteration: 5633 ---- loss: 0.658060044180029 \n",
            "Iteration: 5634 ---- loss: 0.65805450611437 \n",
            "Iteration: 5635 ---- loss: 0.6580489695264096 \n",
            "Iteration: 5636 ---- loss: 0.6580434347294561 \n",
            "Iteration: 5637 ---- loss: 0.6580379013834292 \n",
            "Iteration: 5638 ---- loss: 0.6580323696775532 \n",
            "Iteration: 5639 ---- loss: 0.6580268395065655 \n",
            "Iteration: 5640 ---- loss: 0.6580213108969292 \n",
            "Iteration: 5641 ---- loss: 0.6580157839945424 \n",
            "Iteration: 5642 ---- loss: 0.6580102585896571 \n",
            "Iteration: 5643 ---- loss: 0.6580047345757913 \n",
            "Iteration: 5644 ---- loss: 0.6579992119075064 \n",
            "Iteration: 5645 ---- loss: 0.6579936907944278 \n",
            "Iteration: 5646 ---- loss: 0.6579881713556417 \n",
            "Iteration: 5647 ---- loss: 0.6579826533935189 \n",
            "Iteration: 5648 ---- loss: 0.6579771369874596 \n",
            "Iteration: 5649 ---- loss: 0.6579716222482618 \n",
            "Iteration: 5651 ---- loss: 0.6579605974218753 \n",
            "Iteration: 5652 ---- loss: 0.6579550872594044 \n",
            "Iteration: 5653 ---- loss: 0.6579495786324275 \n",
            "Iteration: 5654 ---- loss: 0.6579440716133909 \n",
            "Iteration: 5655 ---- loss: 0.6579385662274599 \n",
            "Iteration: 5656 ---- loss: 0.6579330623475304 \n",
            "Iteration: 5657 ---- loss: 0.6579275601306 \n",
            "Iteration: 5658 ---- loss: 0.6579220594008991 \n",
            "Iteration: 5659 ---- loss: 0.657916560204763 \n",
            "Iteration: 5660 ---- loss: 0.6579110628006095 \n",
            "Iteration: 5661 ---- loss: 0.6579055668376316 \n",
            "Iteration: 5662 ---- loss: 0.6579000720743254 \n",
            "Iteration: 5663 ---- loss: 0.6578945781628429 \n",
            "Iteration: 5664 ---- loss: 0.6578890859922271 \n",
            "Iteration: 5665 ---- loss: 0.6578835953531162 \n",
            "Iteration: 5666 ---- loss: 0.6578781062175402 \n",
            "Iteration: 5667 ---- loss: 0.6578726191498735 \n",
            "Iteration: 5668 ---- loss: 0.6578671343998678 \n",
            "Iteration: 5669 ---- loss: 0.6578616514385837 \n",
            "Iteration: 5670 ---- loss: 0.6578561697881231 \n",
            "Iteration: 5671 ---- loss: 0.6578506898744779 \n",
            "Iteration: 5672 ---- loss: 0.657845211496618 \n",
            "Iteration: 5673 ---- loss: 0.65783973478933 \n",
            "Iteration: 5674 ---- loss: 0.6578342595537645 \n",
            "Iteration: 5675 ---- loss: 0.6578287860018636 \n",
            "Iteration: 5676 ---- loss: 0.6578233139648562 \n",
            "Iteration: 5677 ---- loss: 0.657817843605413 \n",
            "Iteration: 5678 ---- loss: 0.6578123747151815 \n",
            "Iteration: 5679 ---- loss: 0.657806907480464 \n",
            "Iteration: 5680 ---- loss: 0.6578014417897808 \n",
            "Iteration: 5681 ---- loss: 0.6577959777560766 \n",
            "Iteration: 5682 ---- loss: 0.6577905152358 \n",
            "Iteration: 5683 ---- loss: 0.6577850546018272 \n",
            "Iteration: 5684 ---- loss: 0.6577795974681148 \n",
            "Iteration: 5685 ---- loss: 0.6577741421381619 \n",
            "Iteration: 5686 ---- loss: 0.6577686884485597 \n",
            "Iteration: 5687 ---- loss: 0.6577632362591889 \n",
            "Iteration: 5688 ---- loss: 0.6577577857689267 \n",
            "Iteration: 5689 ---- loss: 0.6577523367353604 \n",
            "Iteration: 5690 ---- loss: 0.6577468894394772 \n",
            "Iteration: 5691 ---- loss: 0.6577414435874059 \n",
            "Iteration: 5692 ---- loss: 0.6577359994314813 \n",
            "Iteration: 5693 ---- loss: 0.6577305567973751 \n",
            "Iteration: 5694 ---- loss: 0.6577251156577653 \n",
            "Iteration: 5695 ---- loss: 0.6577196751650847 \n",
            "Iteration: 5696 ---- loss: 0.6577142361794054 \n",
            "Iteration: 5697 ---- loss: 0.6577087989114992 \n",
            "Iteration: 5698 ---- loss: 0.6577033630872171 \n",
            "Iteration: 5699 ---- loss: 0.6576979289653402 \n",
            "Iteration: 5701 ---- loss: 0.6576870653499981 \n",
            "Iteration: 5702 ---- loss: 0.6576816359816925 \n",
            "Iteration: 5703 ---- loss: 0.657676208100861 \n",
            "Iteration: 5704 ---- loss: 0.6576707819376153 \n",
            "Iteration: 5705 ---- loss: 0.6576653573277472 \n",
            "Iteration: 5706 ---- loss: 0.6576599342549595 \n",
            "Iteration: 5707 ---- loss: 0.6576545127623823 \n",
            "Iteration: 5708 ---- loss: 0.6576490928199403 \n",
            "Iteration: 5709 ---- loss: 0.6576436745849591 \n",
            "Iteration: 5710 ---- loss: 0.6576382578591045 \n",
            "Iteration: 5711 ---- loss: 0.6576328427858293 \n",
            "Iteration: 5712 ---- loss: 0.6576274293206622 \n",
            "Iteration: 5713 ---- loss: 0.6576220172641803 \n",
            "Iteration: 5714 ---- loss: 0.6576166069284295 \n",
            "Iteration: 5715 ---- loss: 0.6576111982588999 \n",
            "Iteration: 5716 ---- loss: 0.6576057910470747 \n",
            "Iteration: 5717 ---- loss: 0.657600385440546 \n",
            "Iteration: 5718 ---- loss: 0.6575949814550752 \n",
            "Iteration: 5719 ---- loss: 0.6575895790921861 \n",
            "Iteration: 5720 ---- loss: 0.6575841786076748 \n",
            "Iteration: 5721 ---- loss: 0.6575787794633755 \n",
            "Iteration: 5722 ---- loss: 0.6575733820281688 \n",
            "Iteration: 5723 ---- loss: 0.6575679859854517 \n",
            "Iteration: 5724 ---- loss: 0.6575625913169892 \n",
            "Iteration: 5725 ---- loss: 0.6575571982864681 \n",
            "Iteration: 5726 ---- loss: 0.6575518062927155 \n",
            "Iteration: 5727 ---- loss: 0.6575464157391455 \n",
            "Iteration: 5728 ---- loss: 0.6575410266718099 \n",
            "Iteration: 5729 ---- loss: 0.6575356393678864 \n",
            "Iteration: 5730 ---- loss: 0.6575302536510257 \n",
            "Iteration: 5731 ---- loss: 0.6575248694064557 \n",
            "Iteration: 5732 ---- loss: 0.6575194867207811 \n",
            "Iteration: 5733 ---- loss: 0.6575141055220103 \n",
            "Iteration: 5734 ---- loss: 0.6575087259829375 \n",
            "Iteration: 5735 ---- loss: 0.657503347965097 \n",
            "Iteration: 5736 ---- loss: 0.6574979715463523 \n",
            "Iteration: 5737 ---- loss: 0.6574925967677416 \n",
            "Iteration: 5738 ---- loss: 0.657487223589029 \n",
            "Iteration: 5739 ---- loss: 0.6574818521180389 \n",
            "Iteration: 5740 ---- loss: 0.6574764827601344 \n",
            "Iteration: 5741 ---- loss: 0.6574711161358523 \n",
            "Iteration: 5742 ---- loss: 0.6574657510953414 \n",
            "Iteration: 5743 ---- loss: 0.6574603876762675 \n",
            "Iteration: 5744 ---- loss: 0.6574550259320883 \n",
            "Iteration: 5745 ---- loss: 0.6574496656877864 \n",
            "Iteration: 5746 ---- loss: 0.6574443070616779 \n",
            "Iteration: 5747 ---- loss: 0.6574389499936025 \n",
            "Iteration: 5748 ---- loss: 0.6574335946449985 \n",
            "Iteration: 5749 ---- loss: 0.6574282407421449 \n",
            "Iteration: 5751 ---- loss: 0.6574175379014714 \n",
            "Iteration: 5752 ---- loss: 0.6574121904277189 \n",
            "Iteration: 5753 ---- loss: 0.6574068448005103 \n",
            "Iteration: 5754 ---- loss: 0.6574015007670956 \n",
            "Iteration: 5755 ---- loss: 0.6573961585205418 \n",
            "Iteration: 5756 ---- loss: 0.6573908201566734 \n",
            "Iteration: 5757 ---- loss: 0.6573854833556073 \n",
            "Iteration: 5758 ---- loss: 0.6573801482571027 \n",
            "Iteration: 5759 ---- loss: 0.6573748146664049 \n",
            "Iteration: 5760 ---- loss: 0.6573694827182301 \n",
            "Iteration: 5761 ---- loss: 0.6573641529099764 \n",
            "Iteration: 5762 ---- loss: 0.6573588247246724 \n",
            "Iteration: 5763 ---- loss: 0.6573534981565293 \n",
            "Iteration: 5764 ---- loss: 0.6573481731862626 \n",
            "Iteration: 5765 ---- loss: 0.6573428497700229 \n",
            "Iteration: 5766 ---- loss: 0.6573375280294036 \n",
            "Iteration: 5767 ---- loss: 0.6573322078742527 \n",
            "Iteration: 5768 ---- loss: 0.6573268893439136 \n",
            "Iteration: 5769 ---- loss: 0.6573215723880496 \n",
            "Iteration: 5770 ---- loss: 0.6573162569982844 \n",
            "Iteration: 5771 ---- loss: 0.6573109432722982 \n",
            "Iteration: 5772 ---- loss: 0.6573056311280268 \n",
            "Iteration: 5773 ---- loss: 0.6573003207548094 \n",
            "Iteration: 5774 ---- loss: 0.657295012393714 \n",
            "Iteration: 5775 ---- loss: 0.657289705464274 \n",
            "Iteration: 5776 ---- loss: 0.6572844001911982 \n",
            "Iteration: 5777 ---- loss: 0.657279096497812 \n",
            "Iteration: 5778 ---- loss: 0.6572737944835039 \n",
            "Iteration: 5779 ---- loss: 0.6572684938486538 \n",
            "Iteration: 5780 ---- loss: 0.6572631947288553 \n",
            "Iteration: 5781 ---- loss: 0.6572578972663389 \n",
            "Iteration: 5782 ---- loss: 0.6572526014016038 \n",
            "Iteration: 5783 ---- loss: 0.6572473071385164 \n",
            "Iteration: 5784 ---- loss: 0.657242014512063 \n",
            "Iteration: 5785 ---- loss: 0.657236723526326 \n",
            "Iteration: 5786 ---- loss: 0.6572314341465112 \n",
            "Iteration: 5787 ---- loss: 0.6572261463913269 \n",
            "Iteration: 5788 ---- loss: 0.6572208602349001 \n",
            "Iteration: 5789 ---- loss: 0.6572155756831889 \n",
            "Iteration: 5790 ---- loss: 0.6572102927510687 \n",
            "Iteration: 5791 ---- loss: 0.6572050117148596 \n",
            "Iteration: 5792 ---- loss: 0.6571997327534679 \n",
            "Iteration: 5793 ---- loss: 0.6571944554145622 \n",
            "Iteration: 5794 ---- loss: 0.657189179709875 \n",
            "Iteration: 5795 ---- loss: 0.6571839056148657 \n",
            "Iteration: 5796 ---- loss: 0.6571786331361658 \n",
            "Iteration: 5797 ---- loss: 0.6571733622661922 \n",
            "Iteration: 5798 ---- loss: 0.6571680930172797 \n",
            "Iteration: 5799 ---- loss: 0.6571628254140518 \n",
            "Iteration: 5801 ---- loss: 0.6571522950252999 \n",
            "Iteration: 5802 ---- loss: 0.6571470322579708 \n",
            "Iteration: 5803 ---- loss: 0.6571417711115877 \n",
            "Iteration: 5804 ---- loss: 0.6571365115991437 \n",
            "Iteration: 5805 ---- loss: 0.6571312537230808 \n",
            "Iteration: 5806 ---- loss: 0.6571259977703161 \n",
            "Iteration: 5807 ---- loss: 0.657120744223787 \n",
            "Iteration: 5808 ---- loss: 0.6571154922997539 \n",
            "Iteration: 5809 ---- loss: 0.6571102419981885 \n",
            "Iteration: 5810 ---- loss: 0.6571049933267745 \n",
            "Iteration: 5811 ---- loss: 0.657099746275139 \n",
            "Iteration: 5812 ---- loss: 0.6570945008390511 \n",
            "Iteration: 5813 ---- loss: 0.6570892570253163 \n",
            "Iteration: 5814 ---- loss: 0.6570840148339057 \n",
            "Iteration: 5815 ---- loss: 0.65707877426479 \n",
            "Iteration: 5816 ---- loss: 0.6570735353179401 \n",
            "Iteration: 5817 ---- loss: 0.6570682979933263 \n",
            "Iteration: 5818 ---- loss: 0.6570630622130824 \n",
            "Iteration: 5819 ---- loss: 0.6570578271289668 \n",
            "Iteration: 5820 ---- loss: 0.6570525936247855 \n",
            "Iteration: 5821 ---- loss: 0.6570473618423326 \n",
            "Iteration: 5822 ---- loss: 0.6570421316001726 \n",
            "Iteration: 5823 ---- loss: 0.6570369030746872 \n",
            "Iteration: 5824 ---- loss: 0.6570316760960615 \n",
            "Iteration: 5825 ---- loss: 0.6570264508198969 \n",
            "Iteration: 5826 ---- loss: 0.6570212271105081 \n",
            "Iteration: 5827 ---- loss: 0.657016005117162 \n",
            "Iteration: 5828 ---- loss: 0.6570107847090094 \n",
            "Iteration: 5829 ---- loss: 0.6570055659614349 \n",
            "Iteration: 5830 ---- loss: 0.6570003488314416 \n",
            "Iteration: 5831 ---- loss: 0.6569951333250851 \n",
            "Iteration: 5832 ---- loss: 0.6569899194774123 \n",
            "Iteration: 5833 ---- loss: 0.6569847072077284 \n",
            "Iteration: 5834 ---- loss: 0.65697949664653 \n",
            "Iteration: 5835 ---- loss: 0.6569742876269488 \n",
            "Iteration: 5836 ---- loss: 0.6569690803215116 \n",
            "Iteration: 5837 ---- loss: 0.6569638745822054 \n",
            "Iteration: 5838 ---- loss: 0.6569586705021057 \n",
            "Iteration: 5839 ---- loss: 0.656953468059618 \n",
            "Iteration: 5840 ---- loss: 0.6569482672003345 \n",
            "Iteration: 5841 ---- loss: 0.656943068058793 \n",
            "Iteration: 5842 ---- loss: 0.6569378719851017 \n",
            "Iteration: 5843 ---- loss: 0.6569326797690836 \n",
            "Iteration: 5844 ---- loss: 0.6569274889970341 \n",
            "Iteration: 5845 ---- loss: 0.6569222998110614 \n",
            "Iteration: 5846 ---- loss: 0.656917112341642 \n",
            "Iteration: 5847 ---- loss: 0.6569119263941998 \n",
            "Iteration: 5848 ---- loss: 0.6569067420844921 \n",
            "Iteration: 5849 ---- loss: 0.6569015594254196 \n",
            "Iteration: 5851 ---- loss: 0.6568911989319715 \n",
            "Iteration: 5852 ---- loss: 0.6568860211271771 \n",
            "Iteration: 5853 ---- loss: 0.6568808449197884 \n",
            "Iteration: 5854 ---- loss: 0.6568756704094664 \n",
            "Iteration: 5855 ---- loss: 0.6568704974673957 \n",
            "Iteration: 5856 ---- loss: 0.6568653261402646 \n",
            "Iteration: 5857 ---- loss: 0.6568601565161735 \n",
            "Iteration: 5858 ---- loss: 0.6568549884452551 \n",
            "Iteration: 5859 ---- loss: 0.6568498219980962 \n",
            "Iteration: 5860 ---- loss: 0.6568446572609504 \n",
            "Iteration: 5861 ---- loss: 0.6568394940748994 \n",
            "Iteration: 5862 ---- loss: 0.6568343325082586 \n",
            "Iteration: 5863 ---- loss: 0.656829172588711 \n",
            "Iteration: 5864 ---- loss: 0.6568240140643048 \n",
            "Iteration: 5865 ---- loss: 0.656818857146686 \n",
            "Iteration: 5866 ---- loss: 0.6568137019055067 \n",
            "Iteration: 5867 ---- loss: 0.6568085482796484 \n",
            "Iteration: 5868 ---- loss: 0.65680339624278 \n",
            "Iteration: 5869 ---- loss: 0.6567982458580761 \n",
            "Iteration: 5870 ---- loss: 0.656793097135292 \n",
            "Iteration: 5871 ---- loss: 0.6567879499802024 \n",
            "Iteration: 5872 ---- loss: 0.6567828044690748 \n",
            "Iteration: 5873 ---- loss: 0.6567776605513254 \n",
            "Iteration: 5874 ---- loss: 0.656772517921261 \n",
            "Iteration: 5875 ---- loss: 0.6567673769198321 \n",
            "Iteration: 5876 ---- loss: 0.6567622375455223 \n",
            "Iteration: 5877 ---- loss: 0.656757099972293 \n",
            "Iteration: 5878 ---- loss: 0.6567519640797806 \n",
            "Iteration: 5879 ---- loss: 0.6567468298298526 \n",
            "Iteration: 5880 ---- loss: 0.6567416971915699 \n",
            "Iteration: 5881 ---- loss: 0.6567365662519632 \n",
            "Iteration: 5882 ---- loss: 0.6567314368897152 \n",
            "Iteration: 5883 ---- loss: 0.6567263091673632 \n",
            "Iteration: 5884 ---- loss: 0.6567211830603786 \n",
            "Iteration: 5885 ---- loss: 0.6567160586281459 \n",
            "Iteration: 5886 ---- loss: 0.6567109358110067 \n",
            "Iteration: 5887 ---- loss: 0.6567058146197199 \n",
            "Iteration: 5888 ---- loss: 0.6567006950355966 \n",
            "Iteration: 5889 ---- loss: 0.6566955770742848 \n",
            "Iteration: 5890 ---- loss: 0.6566904607649033 \n",
            "Iteration: 5891 ---- loss: 0.6566853459800213 \n",
            "Iteration: 5892 ---- loss: 0.6566802324882348 \n",
            "Iteration: 5893 ---- loss: 0.6566751207995504 \n",
            "Iteration: 5894 ---- loss: 0.6566700111500845 \n",
            "Iteration: 5895 ---- loss: 0.6566649031357594 \n",
            "Iteration: 5896 ---- loss: 0.6566597967617384 \n",
            "Iteration: 5897 ---- loss: 0.6566546919869154 \n",
            "Iteration: 5898 ---- loss: 0.6566495888367174 \n",
            "Iteration: 5899 ---- loss: 0.6566444873303873 \n",
            "Iteration: 5901 ---- loss: 0.6566342891888621 \n",
            "Iteration: 5902 ---- loss: 0.6566291922746649 \n",
            "Iteration: 5903 ---- loss: 0.6566240961721299 \n",
            "Iteration: 5904 ---- loss: 0.6566190018046666 \n",
            "Iteration: 5905 ---- loss: 0.6566139119930587 \n",
            "Iteration: 5906 ---- loss: 0.656608824232902 \n",
            "Iteration: 5907 ---- loss: 0.6566037386790207 \n",
            "Iteration: 5908 ---- loss: 0.6565986547307608 \n",
            "Iteration: 5909 ---- loss: 0.6565935724078542 \n",
            "Iteration: 5910 ---- loss: 0.6565884917144589 \n",
            "Iteration: 5911 ---- loss: 0.6565834126537764 \n",
            "Iteration: 5912 ---- loss: 0.6565783352702236 \n",
            "Iteration: 5913 ---- loss: 0.6565732595680825 \n",
            "Iteration: 5914 ---- loss: 0.6565681857775763 \n",
            "Iteration: 5915 ---- loss: 0.6565631143564934 \n",
            "Iteration: 5916 ---- loss: 0.6565580445749272 \n",
            "Iteration: 5917 ---- loss: 0.6565529764235898 \n",
            "Iteration: 5918 ---- loss: 0.6565479099016173 \n",
            "Iteration: 5919 ---- loss: 0.6565428450127487 \n",
            "Iteration: 5920 ---- loss: 0.6565377817576413 \n",
            "Iteration: 5921 ---- loss: 0.6565327201393751 \n",
            "Iteration: 5922 ---- loss: 0.6565276600381225 \n",
            "Iteration: 5923 ---- loss: 0.6565226007258361 \n",
            "Iteration: 5924 ---- loss: 0.6565175430428489 \n",
            "Iteration: 5925 ---- loss: 0.6565124869800265 \n",
            "Iteration: 5926 ---- loss: 0.6565074321224674 \n",
            "Iteration: 5927 ---- loss: 0.6565023785676936 \n",
            "Iteration: 5928 ---- loss: 0.6564973266367694 \n",
            "Iteration: 5929 ---- loss: 0.6564922763317594 \n",
            "Iteration: 5930 ---- loss: 0.6564872276508456 \n",
            "Iteration: 5931 ---- loss: 0.6564821805402767 \n",
            "Iteration: 5932 ---- loss: 0.656477134976923 \n",
            "Iteration: 5933 ---- loss: 0.6564720910772611 \n",
            "Iteration: 5934 ---- loss: 0.6564670488545506 \n",
            "Iteration: 5935 ---- loss: 0.6564620082588163 \n",
            "Iteration: 5936 ---- loss: 0.6564569692899505 \n",
            "Iteration: 5937 ---- loss: 0.6564519319478458 \n",
            "Iteration: 5938 ---- loss: 0.6564468960323367 \n",
            "Iteration: 5939 ---- loss: 0.6564418603792332 \n",
            "Iteration: 5940 ---- loss: 0.6564368260284182 \n",
            "Iteration: 5941 ---- loss: 0.6564317932999278 \n",
            "Iteration: 5942 ---- loss: 0.6564267621936531 \n",
            "Iteration: 5943 ---- loss: 0.6564217327094838 \n",
            "Iteration: 5944 ---- loss: 0.6564167048473102 \n",
            "Iteration: 5945 ---- loss: 0.6564116786070209 \n",
            "Iteration: 5946 ---- loss: 0.6564066539885046 \n",
            "Iteration: 5947 ---- loss: 0.6564016309916492 \n",
            "Iteration: 5948 ---- loss: 0.6563966096163419 \n",
            "Iteration: 5949 ---- loss: 0.6563915894004505 \n",
            "Iteration: 5951 ---- loss: 0.6563815522478983 \n",
            "Iteration: 5952 ---- loss: 0.6563765361097033 \n",
            "Iteration: 5953 ---- loss: 0.6563715215963576 \n",
            "Iteration: 5954 ---- loss: 0.6563665087077419 \n",
            "Iteration: 5955 ---- loss: 0.6563614974259802 \n",
            "Iteration: 5956 ---- loss: 0.6563564877273759 \n",
            "Iteration: 5957 ---- loss: 0.6563514796532174 \n",
            "Iteration: 5958 ---- loss: 0.6563464732033827 \n",
            "Iteration: 5959 ---- loss: 0.6563414683777492 \n",
            "Iteration: 5960 ---- loss: 0.6563364651761938 \n",
            "Iteration: 5961 ---- loss: 0.656331463598593 \n",
            "Iteration: 5962 ---- loss: 0.6563264636448227 \n",
            "Iteration: 5963 ---- loss: 0.6563214653147575 \n",
            "Iteration: 5964 ---- loss: 0.6563164686082733 \n",
            "Iteration: 5965 ---- loss: 0.6563114734069816 \n",
            "Iteration: 5966 ---- loss: 0.6563064796214992 \n",
            "Iteration: 5967 ---- loss: 0.6563014874581191 \n",
            "Iteration: 5968 ---- loss: 0.6562964974381474 \n",
            "Iteration: 5969 ---- loss: 0.6562915157831901 \n",
            "Iteration: 5970 ---- loss: 0.6562865357717402 \n",
            "Iteration: 5971 ---- loss: 0.6562815574036154 \n",
            "Iteration: 5972 ---- loss: 0.6562765806786334 \n",
            "Iteration: 5973 ---- loss: 0.6562716055966121 \n",
            "Iteration: 5974 ---- loss: 0.6562666321522682 \n",
            "Iteration: 5975 ---- loss: 0.6562616603382352 \n",
            "Iteration: 5976 ---- loss: 0.6562566901659986 \n",
            "Iteration: 5977 ---- loss: 0.6562517216353771 \n",
            "Iteration: 5978 ---- loss: 0.6562467547461898 \n",
            "Iteration: 5979 ---- loss: 0.6562417894982555 \n",
            "Iteration: 5980 ---- loss: 0.6562368258913936 \n",
            "Iteration: 5981 ---- loss: 0.6562318639254229 \n",
            "Iteration: 5982 ---- loss: 0.6562269036001632 \n",
            "Iteration: 5983 ---- loss: 0.6562219449154337 \n",
            "Iteration: 5984 ---- loss: 0.6562169878710543 \n",
            "Iteration: 5985 ---- loss: 0.6562120324668439 \n",
            "Iteration: 5986 ---- loss: 0.6562070787026224 \n",
            "Iteration: 5987 ---- loss: 0.6562021265782094 \n",
            "Iteration: 5988 ---- loss: 0.6561971760934245 \n",
            "Iteration: 5989 ---- loss: 0.6561922272447527 \n",
            "Iteration: 5990 ---- loss: 0.6561872800250226 \n",
            "Iteration: 5991 ---- loss: 0.6561823344440931 \n",
            "Iteration: 5992 ---- loss: 0.6561773905327184 \n",
            "Iteration: 5993 ---- loss: 0.6561724478816368 \n",
            "Iteration: 5994 ---- loss: 0.6561675058105033 \n",
            "Iteration: 5995 ---- loss: 0.6561625661234575 \n",
            "Iteration: 5996 ---- loss: 0.6561576301542217 \n",
            "Iteration: 5997 ---- loss: 0.6561526962120742 \n",
            "Iteration: 5998 ---- loss: 0.6561477639301891 \n",
            "Iteration: 5999 ---- loss: 0.6561428333082858 \n",
            "Iteration: 6001 ---- loss: 0.6561329770539104 \n",
            "Iteration: 6002 ---- loss: 0.6561280519766206 \n",
            "Iteration: 6003 ---- loss: 0.6561231285598406 \n",
            "Iteration: 6004 ---- loss: 0.6561182068033258 \n",
            "Iteration: 6005 ---- loss: 0.656113286706834 \n",
            "Iteration: 6006 ---- loss: 0.6561083682701229 \n",
            "Iteration: 6007 ---- loss: 0.6561034514929508 \n",
            "Iteration: 6008 ---- loss: 0.6560985363750774 \n",
            "Iteration: 6009 ---- loss: 0.6560936229162629 \n",
            "Iteration: 6010 ---- loss: 0.6560887111162679 \n",
            "Iteration: 6011 ---- loss: 0.6560838008506987 \n",
            "Iteration: 6012 ---- loss: 0.656078892043994 \n",
            "Iteration: 6013 ---- loss: 0.6560739848711298 \n",
            "Iteration: 6014 ---- loss: 0.6560690793215397 \n",
            "Iteration: 6015 ---- loss: 0.6560641753869533 \n",
            "Iteration: 6016 ---- loss: 0.6560592731085787 \n",
            "Iteration: 6017 ---- loss: 0.656054372476311 \n",
            "Iteration: 6018 ---- loss: 0.6560494732385825 \n",
            "Iteration: 6019 ---- loss: 0.6560445756566778 \n",
            "Iteration: 6020 ---- loss: 0.6560396797303618 \n",
            "Iteration: 6021 ---- loss: 0.6560347854594002 \n",
            "Iteration: 6022 ---- loss: 0.6560298928435588 \n",
            "Iteration: 6023 ---- loss: 0.6560250018826047 \n",
            "Iteration: 6024 ---- loss: 0.6560201124133467 \n",
            "Iteration: 6025 ---- loss: 0.65601522412951 \n",
            "Iteration: 6026 ---- loss: 0.656010337560642 \n",
            "Iteration: 6027 ---- loss: 0.6560054526439721 \n",
            "Iteration: 6028 ---- loss: 0.656000569379271 \n",
            "Iteration: 6029 ---- loss: 0.6559956877112295 \n",
            "Iteration: 6030 ---- loss: 0.6559908084917159 \n",
            "Iteration: 6031 ---- loss: 0.655985931647987 \n",
            "Iteration: 6032 ---- loss: 0.6559810564542954 \n",
            "Iteration: 6033 ---- loss: 0.6559761829104463 \n",
            "Iteration: 6034 ---- loss: 0.6559713110162457 \n",
            "Iteration: 6035 ---- loss: 0.655966440771498 \n",
            "Iteration: 6036 ---- loss: 0.6559615721760077 \n",
            "Iteration: 6037 ---- loss: 0.6559567052475881 \n",
            "Iteration: 6038 ---- loss: 0.6559518401185219 \n",
            "Iteration: 6039 ---- loss: 0.6559469766368949 \n",
            "Iteration: 6040 ---- loss: 0.6559421147878751 \n",
            "Iteration: 6041 ---- loss: 0.6559372544687849 \n",
            "Iteration: 6042 ---- loss: 0.6559323958362762 \n",
            "Iteration: 6043 ---- loss: 0.6559275389349234 \n",
            "Iteration: 6044 ---- loss: 0.6559226849508213 \n",
            "Iteration: 6045 ---- loss: 0.6559178356488304 \n",
            "Iteration: 6046 ---- loss: 0.6559129874780848 \n",
            "Iteration: 6047 ---- loss: 0.6559081406279864 \n",
            "Iteration: 6048 ---- loss: 0.6559032954454123 \n",
            "Iteration: 6049 ---- loss: 0.6558984518911553 \n",
            "Iteration: 6051 ---- loss: 0.6558887695737295 \n",
            "Iteration: 6052 ---- loss: 0.6558839308814981 \n",
            "Iteration: 6053 ---- loss: 0.6558790938434649 \n",
            "Iteration: 6054 ---- loss: 0.6558742584359615 \n",
            "Iteration: 6055 ---- loss: 0.6558694246652744 \n",
            "Iteration: 6056 ---- loss: 0.6558645925228932 \n",
            "Iteration: 6057 ---- loss: 0.6558597620148732 \n",
            "Iteration: 6058 ---- loss: 0.6558549330955699 \n",
            "Iteration: 6059 ---- loss: 0.6558501054267528 \n",
            "Iteration: 6060 ---- loss: 0.6558452794044715 \n",
            "Iteration: 6061 ---- loss: 0.6558404550748747 \n",
            "Iteration: 6062 ---- loss: 0.6558356328562731 \n",
            "Iteration: 6063 ---- loss: 0.6558308124154798 \n",
            "Iteration: 6064 ---- loss: 0.655825993622892 \n",
            "Iteration: 6065 ---- loss: 0.6558211763384243 \n",
            "Iteration: 6066 ---- loss: 0.6558163604048814 \n",
            "Iteration: 6067 ---- loss: 0.6558115461100467 \n",
            "Iteration: 6068 ---- loss: 0.655806733518544 \n",
            "Iteration: 6069 ---- loss: 0.6558019226809538 \n",
            "Iteration: 6070 ---- loss: 0.6557971134734242 \n",
            "Iteration: 6071 ---- loss: 0.6557923059034063 \n",
            "Iteration: 6072 ---- loss: 0.655787499947741 \n",
            "Iteration: 6073 ---- loss: 0.6557826955880914 \n",
            "Iteration: 6074 ---- loss: 0.6557778927385027 \n",
            "Iteration: 6075 ---- loss: 0.6557730912564452 \n",
            "Iteration: 6076 ---- loss: 0.6557682921084843 \n",
            "Iteration: 6077 ---- loss: 0.6557634957098886 \n",
            "Iteration: 6078 ---- loss: 0.655758700950328 \n",
            "Iteration: 6079 ---- loss: 0.6557539078328006 \n",
            "Iteration: 6080 ---- loss: 0.6557491163317818 \n",
            "Iteration: 6081 ---- loss: 0.6557443263822803 \n",
            "Iteration: 6082 ---- loss: 0.6557395380730251 \n",
            "Iteration: 6083 ---- loss: 0.6557347513621621 \n",
            "Iteration: 6084 ---- loss: 0.6557299659074157 \n",
            "Iteration: 6085 ---- loss: 0.655725182175797 \n",
            "Iteration: 6086 ---- loss: 0.6557204001455688 \n",
            "Iteration: 6087 ---- loss: 0.6557156197525014 \n",
            "Iteration: 6088 ---- loss: 0.6557108409967878 \n",
            "Iteration: 6089 ---- loss: 0.6557060638781922 \n",
            "Iteration: 6090 ---- loss: 0.6557012883964783 \n",
            "Iteration: 6091 ---- loss: 0.6556965145514094 \n",
            "Iteration: 6092 ---- loss: 0.6556917430614864 \n",
            "Iteration: 6093 ---- loss: 0.6556869747792016 \n",
            "Iteration: 6094 ---- loss: 0.6556822081441788 \n",
            "Iteration: 6095 ---- loss: 0.6556774431561704 \n",
            "Iteration: 6096 ---- loss: 0.6556726798149287 \n",
            "Iteration: 6097 ---- loss: 0.6556679181202059 \n",
            "Iteration: 6098 ---- loss: 0.6556631580717525 \n",
            "Iteration: 6099 ---- loss: 0.6556583996693193 \n",
            "Iteration: 6101 ---- loss: 0.6556488873404634 \n",
            "Iteration: 6102 ---- loss: 0.6556441327957032 \n",
            "Iteration: 6103 ---- loss: 0.6556393798907548 \n",
            "Iteration: 6104 ---- loss: 0.6556346286253671 \n",
            "Iteration: 6105 ---- loss: 0.655629878999289 \n",
            "Iteration: 6106 ---- loss: 0.6556251310950567 \n",
            "Iteration: 6107 ---- loss: 0.6556203844578425 \n",
            "Iteration: 6108 ---- loss: 0.6556156384594477 \n",
            "Iteration: 6109 ---- loss: 0.6556108940984471 \n",
            "Iteration: 6110 ---- loss: 0.655606151374588 \n",
            "Iteration: 6111 ---- loss: 0.6556014102876168 \n",
            "Iteration: 6112 ---- loss: 0.6555966708372785 \n",
            "Iteration: 6113 ---- loss: 0.6555919330233188 \n",
            "Iteration: 6114 ---- loss: 0.6555871968234684 \n",
            "Iteration: 6115 ---- loss: 0.6555824621690541 \n",
            "Iteration: 6116 ---- loss: 0.6555777278232522 \n",
            "Iteration: 6117 ---- loss: 0.6555729954352273 \n",
            "Iteration: 6118 ---- loss: 0.6555682654003792 \n",
            "Iteration: 6119 ---- loss: 0.65556353693166 \n",
            "Iteration: 6120 ---- loss: 0.6555588099833545 \n",
            "Iteration: 6121 ---- loss: 0.6555540847132595 \n",
            "Iteration: 6122 ---- loss: 0.6555493617648459 \n",
            "Iteration: 6123 ---- loss: 0.6555446404514063 \n",
            "Iteration: 6124 ---- loss: 0.6555399207726483 \n",
            "Iteration: 6125 ---- loss: 0.6555352027142458 \n",
            "Iteration: 6126 ---- loss: 0.6555304863127764 \n",
            "Iteration: 6127 ---- loss: 0.6555257718188154 \n",
            "Iteration: 6128 ---- loss: 0.6555210589621315 \n",
            "Iteration: 6129 ---- loss: 0.655516347741627 \n",
            "Iteration: 6130 ---- loss: 0.6555116381542405 \n",
            "Iteration: 6131 ---- loss: 0.6555069302021048 \n",
            "Iteration: 6132 ---- loss: 0.6555022239114938 \n",
            "Iteration: 6133 ---- loss: 0.6554975193088914 \n",
            "Iteration: 6134 ---- loss: 0.6554928163407729 \n",
            "Iteration: 6135 ---- loss: 0.6554881150038095 \n",
            "Iteration: 6136 ---- loss: 0.6554834154908876 \n",
            "Iteration: 6137 ---- loss: 0.6554787181358995 \n",
            "Iteration: 6138 ---- loss: 0.6554740224462343 \n",
            "Iteration: 6139 ---- loss: 0.6554693288872656 \n",
            "Iteration: 6140 ---- loss: 0.6554646374846843 \n",
            "Iteration: 6141 ---- loss: 0.6554599475848206 \n",
            "Iteration: 6142 ---- loss: 0.6554552593123137 \n",
            "Iteration: 6143 ---- loss: 0.6554505725834723 \n",
            "Iteration: 6144 ---- loss: 0.6554458874877205 \n",
            "Iteration: 6145 ---- loss: 0.6554412040242377 \n",
            "Iteration: 6146 ---- loss: 0.6554365221622193 \n",
            "Iteration: 6147 ---- loss: 0.6554318419320793 \n",
            "Iteration: 6148 ---- loss: 0.6554271633335024 \n",
            "Iteration: 6149 ---- loss: 0.655422486366174 \n",
            "Iteration: 6151 ---- loss: 0.65541313716736 \n",
            "Iteration: 6152 ---- loss: 0.6554084648810191 \n",
            "Iteration: 6153 ---- loss: 0.6554037942240766 \n",
            "Iteration: 6154 ---- loss: 0.6553991255608197 \n",
            "Iteration: 6155 ---- loss: 0.6553944592091047 \n",
            "Iteration: 6156 ---- loss: 0.6553897944878436 \n",
            "Iteration: 6157 ---- loss: 0.6553851313967234 \n",
            "Iteration: 6158 ---- loss: 0.6553804699354302 \n",
            "Iteration: 6159 ---- loss: 0.6553758101036506 \n",
            "Iteration: 6160 ---- loss: 0.655371151901071 \n",
            "Iteration: 6161 ---- loss: 0.6553664953235058 \n",
            "Iteration: 6162 ---- loss: 0.655361840367158 \n",
            "Iteration: 6163 ---- loss: 0.6553571870386611 \n",
            "Iteration: 6164 ---- loss: 0.6553525353377 \n",
            "Iteration: 6165 ---- loss: 0.6553478852755021 \n",
            "Iteration: 6166 ---- loss: 0.6553432369220603 \n",
            "Iteration: 6167 ---- loss: 0.6553385902823416 \n",
            "Iteration: 6168 ---- loss: 0.6553339452695119 \n",
            "Iteration: 6169 ---- loss: 0.6553293019887844 \n",
            "Iteration: 6170 ---- loss: 0.6553246604695283 \n",
            "Iteration: 6171 ---- loss: 0.6553200205774918 \n",
            "Iteration: 6172 ---- loss: 0.6553153823123614 \n",
            "Iteration: 6173 ---- loss: 0.655310745657127 \n",
            "Iteration: 6174 ---- loss: 0.655306110738974 \n",
            "Iteration: 6175 ---- loss: 0.6553014776888632 \n",
            "Iteration: 6176 ---- loss: 0.6552968459662755 \n",
            "Iteration: 6177 ---- loss: 0.6552922158700751 \n",
            "Iteration: 6178 ---- loss: 0.6552875873999494 \n",
            "Iteration: 6179 ---- loss: 0.6552829605634339 \n",
            "Iteration: 6180 ---- loss: 0.6552783354250733 \n",
            "Iteration: 6181 ---- loss: 0.6552737119136851 \n",
            "Iteration: 6182 ---- loss: 0.6552690900289528 \n",
            "Iteration: 6183 ---- loss: 0.6552644697705593 \n",
            "Iteration: 6184 ---- loss: 0.6552598511178577 \n",
            "Iteration: 6185 ---- loss: 0.6552552339775237 \n",
            "Iteration: 6186 ---- loss: 0.6552506184616146 \n",
            "Iteration: 6187 ---- loss: 0.6552460045698101 \n",
            "Iteration: 6188 ---- loss: 0.6552413923017895 \n",
            "Iteration: 6189 ---- loss: 0.6552367816572318 \n",
            "Iteration: 6190 ---- loss: 0.6552321724751531 \n",
            "Iteration: 6191 ---- loss: 0.6552275645626382 \n",
            "Iteration: 6192 ---- loss: 0.6552229581417965 \n",
            "Iteration: 6193 ---- loss: 0.6552183533417483 \n",
            "Iteration: 6194 ---- loss: 0.6552137502878651 \n",
            "Iteration: 6195 ---- loss: 0.6552091502491804 \n",
            "Iteration: 6196 ---- loss: 0.6552045518305105 \n",
            "Iteration: 6197 ---- loss: 0.6551999549368552 \n",
            "Iteration: 6198 ---- loss: 0.6551953593219005 \n",
            "Iteration: 6199 ---- loss: 0.6551907651133296 \n",
            "Iteration: 6201 ---- loss: 0.6551815815405365 \n",
            "Iteration: 6202 ---- loss: 0.6551769921756572 \n",
            "Iteration: 6203 ---- loss: 0.6551724044246847 \n",
            "Iteration: 6204 ---- loss: 0.6551678183591353 \n",
            "Iteration: 6205 ---- loss: 0.6551632340724056 \n",
            "Iteration: 6206 ---- loss: 0.6551586514003795 \n",
            "Iteration: 6207 ---- loss: 0.655154070342724 \n",
            "Iteration: 6208 ---- loss: 0.6551494909114971 \n",
            "Iteration: 6209 ---- loss: 0.6551449131962195 \n",
            "Iteration: 6210 ---- loss: 0.6551403372228788 \n",
            "Iteration: 6211 ---- loss: 0.6551357628548549 \n",
            "Iteration: 6212 ---- loss: 0.6551311901016776 \n",
            "Iteration: 6213 ---- loss: 0.6551266189630104 \n",
            "Iteration: 6214 ---- loss: 0.6551220494385162 \n",
            "Iteration: 6215 ---- loss: 0.6551174815278578 \n",
            "Iteration: 6216 ---- loss: 0.6551129151928751 \n",
            "Iteration: 6217 ---- loss: 0.6551083502198034 \n",
            "Iteration: 6218 ---- loss: 0.655103786555698 \n",
            "Iteration: 6219 ---- loss: 0.6550992246253422 \n",
            "Iteration: 6220 ---- loss: 0.6550946647283882 \n",
            "Iteration: 6221 ---- loss: 0.6550901064404372 \n",
            "Iteration: 6222 ---- loss: 0.655085549761147 \n",
            "Iteration: 6223 ---- loss: 0.6550809946901766 \n",
            "Iteration: 6224 ---- loss: 0.6550764412271824 \n",
            "Iteration: 6225 ---- loss: 0.6550718893718227 \n",
            "Iteration: 6226 ---- loss: 0.6550673391237538 \n",
            "Iteration: 6227 ---- loss: 0.655062790374124 \n",
            "Iteration: 6228 ---- loss: 0.6550582428791374 \n",
            "Iteration: 6229 ---- loss: 0.6550536969868203 \n",
            "Iteration: 6230 ---- loss: 0.6550491527179102 \n",
            "Iteration: 6231 ---- loss: 0.6550446102328192 \n",
            "Iteration: 6232 ---- loss: 0.6550400693494538 \n",
            "Iteration: 6233 ---- loss: 0.6550355301045186 \n",
            "Iteration: 6234 ---- loss: 0.6550309926046128 \n",
            "Iteration: 6235 ---- loss: 0.6550264568459067 \n",
            "Iteration: 6236 ---- loss: 0.6550219228877922 \n",
            "Iteration: 6237 ---- loss: 0.6550173905324418 \n",
            "Iteration: 6238 ---- loss: 0.6550128597795062 \n",
            "Iteration: 6239 ---- loss: 0.655008330628636 \n",
            "Iteration: 6240 ---- loss: 0.6550038030794805 \n",
            "Iteration: 6241 ---- loss: 0.6549992771316896 \n",
            "Iteration: 6242 ---- loss: 0.6549947527849118 \n",
            "Iteration: 6243 ---- loss: 0.6549902300387962 \n",
            "Iteration: 6244 ---- loss: 0.6549857089179046 \n",
            "Iteration: 6245 ---- loss: 0.6549811893997078 \n",
            "Iteration: 6246 ---- loss: 0.6549766715034826 \n",
            "Iteration: 6247 ---- loss: 0.654972155446925 \n",
            "Iteration: 6248 ---- loss: 0.6549676409361532 \n",
            "Iteration: 6249 ---- loss: 0.6549631279609371 \n",
            "Iteration: 6251 ---- loss: 0.6549541069009662 \n",
            "Iteration: 6252 ---- loss: 0.6549495990365212 \n",
            "Iteration: 6253 ---- loss: 0.6549450949439486 \n",
            "Iteration: 6254 ---- loss: 0.6549405924266503 \n",
            "Iteration: 6255 ---- loss: 0.6549360915151968 \n",
            "Iteration: 6256 ---- loss: 0.6549315922131856 \n",
            "Iteration: 6257 ---- loss: 0.654927094520249 \n",
            "Iteration: 6258 ---- loss: 0.6549225984360192 \n",
            "Iteration: 6259 ---- loss: 0.6549181039601274 \n",
            "Iteration: 6260 ---- loss: 0.6549136110922053 \n",
            "Iteration: 6261 ---- loss: 0.6549091198318828 \n",
            "Iteration: 6262 ---- loss: 0.6549046302101111 \n",
            "Iteration: 6263 ---- loss: 0.6549001425794191 \n",
            "Iteration: 6264 ---- loss: 0.6548956564080117 \n",
            "Iteration: 6265 ---- loss: 0.6548911715605168 \n",
            "Iteration: 6266 ---- loss: 0.6548866883188965 \n",
            "Iteration: 6267 ---- loss: 0.6548822066827753 \n",
            "Iteration: 6268 ---- loss: 0.6548777266517777 \n",
            "Iteration: 6269 ---- loss: 0.6548732476639211 \n",
            "Iteration: 6270 ---- loss: 0.6548687688600943 \n",
            "Iteration: 6271 ---- loss: 0.654864291833211 \n",
            "Iteration: 6272 ---- loss: 0.6548598167052353 \n",
            "Iteration: 6273 ---- loss: 0.6548553430906088 \n",
            "Iteration: 6274 ---- loss: 0.6548508712595037 \n",
            "Iteration: 6275 ---- loss: 0.6548464010290765 \n",
            "Iteration: 6276 ---- loss: 0.6548419323989473 \n",
            "Iteration: 6277 ---- loss: 0.6548374653687355 \n",
            "Iteration: 6278 ---- loss: 0.6548329996873925 \n",
            "Iteration: 6279 ---- loss: 0.6548285342614407 \n",
            "Iteration: 6280 ---- loss: 0.6548240703797914 \n",
            "Iteration: 6281 ---- loss: 0.6548196073841609 \n",
            "Iteration: 6282 ---- loss: 0.6548151454852674 \n",
            "Iteration: 6283 ---- loss: 0.6548106851719746 \n",
            "Iteration: 6284 ---- loss: 0.6548062264438865 \n",
            "Iteration: 6285 ---- loss: 0.6548017693006074 \n",
            "Iteration: 6286 ---- loss: 0.6547973138491786 \n",
            "Iteration: 6287 ---- loss: 0.6547928601146448 \n",
            "Iteration: 6288 ---- loss: 0.654788407963887 \n",
            "Iteration: 6289 ---- loss: 0.6547839573965082 \n",
            "Iteration: 6290 ---- loss: 0.6547795084121111 \n",
            "Iteration: 6291 ---- loss: 0.6547750610102978 \n",
            "Iteration: 6292 ---- loss: 0.65477061520485 \n",
            "Iteration: 6293 ---- loss: 0.6547661709952712 \n",
            "Iteration: 6294 ---- loss: 0.6547617283671339 \n",
            "Iteration: 6295 ---- loss: 0.6547572873200396 \n",
            "Iteration: 6296 ---- loss: 0.6547528478535886 \n",
            "Iteration: 6297 ---- loss: 0.6547484099673813 \n",
            "Iteration: 6298 ---- loss: 0.654743973661018 \n",
            "Iteration: 6299 ---- loss: 0.6547395389340986 \n",
            "Iteration: 6301 ---- loss: 0.654730670686019 \n",
            "Iteration: 6302 ---- loss: 0.6547262381163054 \n",
            "Iteration: 6303 ---- loss: 0.6547218071145913 \n",
            "Iteration: 6304 ---- loss: 0.6547173776804615 \n",
            "Iteration: 6305 ---- loss: 0.6547129491497101 \n",
            "Iteration: 6306 ---- loss: 0.6547085207700752 \n",
            "Iteration: 6307 ---- loss: 0.654704093950125 \n",
            "Iteration: 6308 ---- loss: 0.6546996686894319 \n",
            "Iteration: 6309 ---- loss: 0.6546952449875681 \n",
            "Iteration: 6310 ---- loss: 0.6546908228441068 \n",
            "Iteration: 6311 ---- loss: 0.65468640225862 \n",
            "Iteration: 6312 ---- loss: 0.6546819832306805 \n",
            "Iteration: 6313 ---- loss: 0.6546775658192591 \n",
            "Iteration: 6314 ---- loss: 0.6546731500787912 \n",
            "Iteration: 6315 ---- loss: 0.6546687358961674 \n",
            "Iteration: 6316 ---- loss: 0.6546643232591599 \n",
            "Iteration: 6317 ---- loss: 0.6546599119956528 \n",
            "Iteration: 6318 ---- loss: 0.6546555028561487 \n",
            "Iteration: 6319 ---- loss: 0.6546510966665411 \n",
            "Iteration: 6320 ---- loss: 0.65464669224457 \n",
            "Iteration: 6321 ---- loss: 0.6546422896058531 \n",
            "Iteration: 6322 ---- loss: 0.6546378885273249 \n",
            "Iteration: 6323 ---- loss: 0.6546334897167797 \n",
            "Iteration: 6324 ---- loss: 0.6546290931117464 \n",
            "Iteration: 6325 ---- loss: 0.6546246980683567 \n",
            "Iteration: 6326 ---- loss: 0.654620305321587 \n",
            "Iteration: 6327 ---- loss: 0.654615914844766 \n",
            "Iteration: 6328 ---- loss: 0.6546115259315387 \n",
            "Iteration: 6329 ---- loss: 0.6546071385386842 \n",
            "Iteration: 6330 ---- loss: 0.6546027526399268 \n",
            "Iteration: 6331 ---- loss: 0.6545983683027082 \n",
            "Iteration: 6332 ---- loss: 0.654593985526607 \n",
            "Iteration: 6333 ---- loss: 0.6545896043112015 \n",
            "Iteration: 6334 ---- loss: 0.6545852243406141 \n",
            "Iteration: 6335 ---- loss: 0.654580844551708 \n",
            "Iteration: 6336 ---- loss: 0.6545764662619371 \n",
            "Iteration: 6337 ---- loss: 0.654572087495417 \n",
            "Iteration: 6338 ---- loss: 0.6545677102107728 \n",
            "Iteration: 6339 ---- loss: 0.654563334551528 \n",
            "Iteration: 6340 ---- loss: 0.6545589605550914 \n",
            "Iteration: 6341 ---- loss: 0.6545545881096048 \n",
            "Iteration: 6342 ---- loss: 0.6545502171278931 \n",
            "Iteration: 6343 ---- loss: 0.6545458472503538 \n",
            "Iteration: 6344 ---- loss: 0.6545414786114316 \n",
            "Iteration: 6345 ---- loss: 0.6545371113541117 \n",
            "Iteration: 6346 ---- loss: 0.6545327456417227 \n",
            "Iteration: 6347 ---- loss: 0.6545283814738191 \n",
            "Iteration: 6348 ---- loss: 0.6545240188499548 \n",
            "Iteration: 6349 ---- loss: 0.6545196577696841 \n",
            "Iteration: 6351 ---- loss: 0.65451094023814 \n",
            "Iteration: 6352 ---- loss: 0.6545065837859744 \n",
            "Iteration: 6353 ---- loss: 0.6545022288756179 \n",
            "Iteration: 6354 ---- loss: 0.6544978761612645 \n",
            "Iteration: 6355 ---- loss: 0.6544935257205268 \n",
            "Iteration: 6356 ---- loss: 0.6544891767356064 \n",
            "Iteration: 6357 ---- loss: 0.6544848294413131 \n",
            "Iteration: 6358 ---- loss: 0.6544804855020547 \n",
            "Iteration: 6359 ---- loss: 0.6544761431068923 \n",
            "Iteration: 6360 ---- loss: 0.6544718020824911 \n",
            "Iteration: 6361 ---- loss: 0.6544674622056355 \n",
            "Iteration: 6362 ---- loss: 0.654463123870063 \n",
            "Iteration: 6363 ---- loss: 0.6544587868376944 \n",
            "Iteration: 6364 ---- loss: 0.6544544507569734 \n",
            "Iteration: 6365 ---- loss: 0.6544501162128691 \n",
            "Iteration: 6366 ---- loss: 0.6544457840002447 \n",
            "Iteration: 6367 ---- loss: 0.6544414542661335 \n",
            "Iteration: 6368 ---- loss: 0.6544371260778047 \n",
            "Iteration: 6369 ---- loss: 0.6544327994229057 \n",
            "Iteration: 6370 ---- loss: 0.6544284741842864 \n",
            "Iteration: 6371 ---- loss: 0.6544241505022942 \n",
            "Iteration: 6372 ---- loss: 0.6544198285589695 \n",
            "Iteration: 6373 ---- loss: 0.6544155081513322 \n",
            "Iteration: 6374 ---- loss: 0.6544111892874911 \n",
            "Iteration: 6375 ---- loss: 0.6544068719526585 \n",
            "Iteration: 6376 ---- loss: 0.6544025561535881 \n",
            "Iteration: 6377 ---- loss: 0.6543982418960974 \n",
            "Iteration: 6378 ---- loss: 0.6543939291689981 \n",
            "Iteration: 6379 ---- loss: 0.6543896202055572 \n",
            "Iteration: 6380 ---- loss: 0.6543853140970421 \n",
            "Iteration: 6381 ---- loss: 0.6543810095291029 \n",
            "Iteration: 6382 ---- loss: 0.654376706512286 \n",
            "Iteration: 6383 ---- loss: 0.6543724050384173 \n",
            "Iteration: 6384 ---- loss: 0.6543681051062518 \n",
            "Iteration: 6385 ---- loss: 0.6543638067270675 \n",
            "Iteration: 6386 ---- loss: 0.6543595104013389 \n",
            "Iteration: 6387 ---- loss: 0.6543552178037829 \n",
            "Iteration: 6388 ---- loss: 0.6543509267424565 \n",
            "Iteration: 6389 ---- loss: 0.654346637223202 \n",
            "Iteration: 6390 ---- loss: 0.6543423492320376 \n",
            "Iteration: 6391 ---- loss: 0.6543380627753235 \n",
            "Iteration: 6392 ---- loss: 0.6543337778602055 \n",
            "Iteration: 6393 ---- loss: 0.6543294948468887 \n",
            "Iteration: 6394 ---- loss: 0.654325216286083 \n",
            "Iteration: 6395 ---- loss: 0.654320939287919 \n",
            "Iteration: 6396 ---- loss: 0.6543166638350575 \n",
            "Iteration: 6397 ---- loss: 0.6543123899185658 \n",
            "Iteration: 6398 ---- loss: 0.6543081172205425 \n",
            "Iteration: 6399 ---- loss: 0.6543038460747173 \n",
            "Iteration: 6401 ---- loss: 0.6542953056335604 \n",
            "Iteration: 6402 ---- loss: 0.6542910364402599 \n",
            "Iteration: 6403 ---- loss: 0.6542867687844005 \n",
            "Iteration: 6404 ---- loss: 0.6542825027243713 \n",
            "Iteration: 6405 ---- loss: 0.6542782382627595 \n",
            "Iteration: 6406 ---- loss: 0.6542739754563257 \n",
            "Iteration: 6407 ---- loss: 0.6542697143643467 \n",
            "Iteration: 6408 ---- loss: 0.654265454722272 \n",
            "Iteration: 6409 ---- loss: 0.6542611966354871 \n",
            "Iteration: 6410 ---- loss: 0.6542569397515915 \n",
            "Iteration: 6411 ---- loss: 0.6542526844166456 \n",
            "Iteration: 6412 ---- loss: 0.6542484306641253 \n",
            "Iteration: 6413 ---- loss: 0.6542441786464377 \n",
            "Iteration: 6414 ---- loss: 0.6542399285516863 \n",
            "Iteration: 6415 ---- loss: 0.6542356800094915 \n",
            "Iteration: 6416 ---- loss: 0.6542314332002026 \n",
            "Iteration: 6417 ---- loss: 0.6542271879083096 \n",
            "Iteration: 6418 ---- loss: 0.6542229441654193 \n",
            "Iteration: 6419 ---- loss: 0.6542187020341786 \n",
            "Iteration: 6420 ---- loss: 0.6542144626266289 \n",
            "Iteration: 6421 ---- loss: 0.6542102258978475 \n",
            "Iteration: 6422 ---- loss: 0.6542059908035981 \n",
            "Iteration: 6423 ---- loss: 0.6542017572778941 \n",
            "Iteration: 6424 ---- loss: 0.6541975252830318 \n",
            "Iteration: 6425 ---- loss: 0.6541932951272742 \n",
            "Iteration: 6426 ---- loss: 0.6541890665296248 \n",
            "Iteration: 6427 ---- loss: 0.6541848394958532 \n",
            "Iteration: 6428 ---- loss: 0.6541806140061535 \n",
            "Iteration: 6429 ---- loss: 0.6541763900601028 \n",
            "Iteration: 6430 ---- loss: 0.6541721676679153 \n",
            "Iteration: 6431 ---- loss: 0.6541679468269597 \n",
            "Iteration: 6432 ---- loss: 0.6541637279037867 \n",
            "Iteration: 6433 ---- loss: 0.65415951100806 \n",
            "Iteration: 6434 ---- loss: 0.6541552956601138 \n",
            "Iteration: 6435 ---- loss: 0.6541510818685391 \n",
            "Iteration: 6436 ---- loss: 0.6541468696273606 \n",
            "Iteration: 6437 ---- loss: 0.6541426589369401 \n",
            "Iteration: 6438 ---- loss: 0.6541384504205415 \n",
            "Iteration: 6439 ---- loss: 0.6541342444908088 \n",
            "Iteration: 6440 ---- loss: 0.6541300401158405 \n",
            "Iteration: 6441 ---- loss: 0.654125837299317 \n",
            "Iteration: 6442 ---- loss: 0.6541216360266793 \n",
            "Iteration: 6443 ---- loss: 0.6541174361967642 \n",
            "Iteration: 6444 ---- loss: 0.6541132378522964 \n",
            "Iteration: 6445 ---- loss: 0.6541090410631659 \n",
            "Iteration: 6446 ---- loss: 0.6541048458307819 \n",
            "Iteration: 6447 ---- loss: 0.6541006521557337 \n",
            "Iteration: 6448 ---- loss: 0.6540964599795888 \n",
            "Iteration: 6449 ---- loss: 0.6540922691854822 \n",
            "Iteration: 6451 ---- loss: 0.654083892250564 \n",
            "Iteration: 6452 ---- loss: 0.6540797061086693 \n",
            "Iteration: 6453 ---- loss: 0.654075521517969 \n",
            "Iteration: 6454 ---- loss: 0.6540713384843151 \n",
            "Iteration: 6455 ---- loss: 0.6540671570077743 \n",
            "Iteration: 6456 ---- loss: 0.65406297707923 \n",
            "Iteration: 6457 ---- loss: 0.6540587986981424 \n",
            "Iteration: 6458 ---- loss: 0.6540546218639725 \n",
            "Iteration: 6459 ---- loss: 0.6540504465761804 \n",
            "Iteration: 6460 ---- loss: 0.654046272834228 \n",
            "Iteration: 6461 ---- loss: 0.6540421006140171 \n",
            "Iteration: 6462 ---- loss: 0.6540379294335302 \n",
            "Iteration: 6463 ---- loss: 0.6540337590054288 \n",
            "Iteration: 6464 ---- loss: 0.6540295901151281 \n",
            "Iteration: 6465 ---- loss: 0.654025422628841 \n",
            "Iteration: 6466 ---- loss: 0.6540212564733279 \n",
            "Iteration: 6467 ---- loss: 0.6540170918527278 \n",
            "Iteration: 6468 ---- loss: 0.6540129287665161 \n",
            "Iteration: 6469 ---- loss: 0.6540087672141671 \n",
            "Iteration: 6470 ---- loss: 0.6540046071951561 \n",
            "Iteration: 6471 ---- loss: 0.6540004487089577 \n",
            "Iteration: 6472 ---- loss: 0.6539962918100908 \n",
            "Iteration: 6473 ---- loss: 0.6539921367701529 \n",
            "Iteration: 6474 ---- loss: 0.6539879832637303 \n",
            "Iteration: 6475 ---- loss: 0.653983831290295 \n",
            "Iteration: 6476 ---- loss: 0.6539796801862398 \n",
            "Iteration: 6477 ---- loss: 0.6539755291995483 \n",
            "Iteration: 6478 ---- loss: 0.6539713790010514 \n",
            "Iteration: 6479 ---- loss: 0.6539672303723194 \n",
            "Iteration: 6480 ---- loss: 0.6539630835977259 \n",
            "Iteration: 6481 ---- loss: 0.6539589383444199 \n",
            "Iteration: 6482 ---- loss: 0.6539547945629074 \n",
            "Iteration: 6483 ---- loss: 0.6539506522002576 \n",
            "Iteration: 6484 ---- loss: 0.653946511516348 \n",
            "Iteration: 6485 ---- loss: 0.6539423726449327 \n",
            "Iteration: 6486 ---- loss: 0.6539382361221743 \n",
            "Iteration: 6487 ---- loss: 0.6539341011167034 \n",
            "Iteration: 6488 ---- loss: 0.6539299676279924 \n",
            "Iteration: 6489 ---- loss: 0.6539258356555138 \n",
            "Iteration: 6490 ---- loss: 0.6539217051961459 \n",
            "Iteration: 6491 ---- loss: 0.653917576246327 \n",
            "Iteration: 6492 ---- loss: 0.6539134489297386 \n",
            "Iteration: 6493 ---- loss: 0.6539093233066811 \n",
            "Iteration: 6494 ---- loss: 0.6539051991986026 \n",
            "Iteration: 6495 ---- loss: 0.6539010765727803 \n",
            "Iteration: 6496 ---- loss: 0.653896954875879 \n",
            "Iteration: 6497 ---- loss: 0.6538928343369588 \n",
            "Iteration: 6498 ---- loss: 0.6538887153050804 \n",
            "Iteration: 6499 ---- loss: 0.6538845977797141 \n",
            "Iteration: 6501 ---- loss: 0.6538763672339393 \n",
            "Iteration: 6502 ---- loss: 0.6538722541912607 \n",
            "Iteration: 6503 ---- loss: 0.6538681426491384 \n",
            "Iteration: 6504 ---- loss: 0.6538640326099705 \n",
            "Iteration: 6505 ---- loss: 0.6538599240732256 \n",
            "Iteration: 6506 ---- loss: 0.6538558170407393 \n",
            "Iteration: 6507 ---- loss: 0.6538517115591632 \n",
            "Iteration: 6508 ---- loss: 0.6538476075829593 \n",
            "Iteration: 6509 ---- loss: 0.6538435051060237 \n",
            "Iteration: 6510 ---- loss: 0.6538394041295091 \n",
            "Iteration: 6511 ---- loss: 0.6538353046535604 \n",
            "Iteration: 6512 ---- loss: 0.6538312066273263 \n",
            "Iteration: 6513 ---- loss: 0.6538271099934659 \n",
            "Iteration: 6514 ---- loss: 0.6538230155735095 \n",
            "Iteration: 6515 ---- loss: 0.653818925727212 \n",
            "Iteration: 6516 ---- loss: 0.6538148388274604 \n",
            "Iteration: 6517 ---- loss: 0.6538107535224396 \n",
            "Iteration: 6518 ---- loss: 0.6538066698358955 \n",
            "Iteration: 6519 ---- loss: 0.6538025876596005 \n",
            "Iteration: 6520 ---- loss: 0.653798507141061 \n",
            "Iteration: 6521 ---- loss: 0.6537944284446763 \n",
            "Iteration: 6522 ---- loss: 0.6537903511380265 \n",
            "Iteration: 6523 ---- loss: 0.6537862752669313 \n",
            "Iteration: 6524 ---- loss: 0.6537822009051698 \n",
            "Iteration: 6525 ---- loss: 0.6537781280521701 \n",
            "Iteration: 6526 ---- loss: 0.6537740567073597 \n",
            "Iteration: 6527 ---- loss: 0.653769986852915 \n",
            "Iteration: 6528 ---- loss: 0.6537659184194308 \n",
            "Iteration: 6529 ---- loss: 0.6537618515302669 \n",
            "Iteration: 6530 ---- loss: 0.6537577861467894 \n",
            "Iteration: 6531 ---- loss: 0.6537537222684267 \n",
            "Iteration: 6532 ---- loss: 0.6537496598873416 \n",
            "Iteration: 6533 ---- loss: 0.6537455989323245 \n",
            "Iteration: 6534 ---- loss: 0.653741539479745 \n",
            "Iteration: 6535 ---- loss: 0.6537374815290319 \n",
            "Iteration: 6536 ---- loss: 0.653733425079615 \n",
            "Iteration: 6537 ---- loss: 0.6537293701309234 \n",
            "Iteration: 6538 ---- loss: 0.6537253176083558 \n",
            "Iteration: 6539 ---- loss: 0.6537212675339765 \n",
            "Iteration: 6540 ---- loss: 0.6537172188903096 \n",
            "Iteration: 6541 ---- loss: 0.6537131716480665 \n",
            "Iteration: 6542 ---- loss: 0.6537091258896292 \n",
            "Iteration: 6543 ---- loss: 0.6537050816685157 \n",
            "Iteration: 6544 ---- loss: 0.6537010393293248 \n",
            "Iteration: 6545 ---- loss: 0.6536969985248013 \n",
            "Iteration: 6546 ---- loss: 0.6536929594216631 \n",
            "Iteration: 6547 ---- loss: 0.6536889216899867 \n",
            "Iteration: 6548 ---- loss: 0.6536848854463491 \n",
            "Iteration: 6549 ---- loss: 0.6536808507544573 \n",
            "Iteration: 6551 ---- loss: 0.6536727862695559 \n",
            "Iteration: 6552 ---- loss: 0.6536687563024957 \n",
            "Iteration: 6553 ---- loss: 0.6536647278354002 \n",
            "Iteration: 6554 ---- loss: 0.653660700863083 \n",
            "Iteration: 6555 ---- loss: 0.6536566754956656 \n",
            "Iteration: 6556 ---- loss: 0.6536526522093136 \n",
            "Iteration: 6557 ---- loss: 0.6536486311696998 \n",
            "Iteration: 6558 ---- loss: 0.6536446116285543 \n",
            "Iteration: 6559 ---- loss: 0.6536405936022301 \n",
            "Iteration: 6560 ---- loss: 0.6536365771059043 \n",
            "Iteration: 6561 ---- loss: 0.6536325621063412 \n",
            "Iteration: 6562 ---- loss: 0.653628548603137 \n",
            "Iteration: 6563 ---- loss: 0.6536245366007554 \n",
            "Iteration: 6564 ---- loss: 0.6536205261089948 \n",
            "Iteration: 6565 ---- loss: 0.6536165180352662 \n",
            "Iteration: 6566 ---- loss: 0.6536125126310568 \n",
            "Iteration: 6567 ---- loss: 0.6536085087208772 \n",
            "Iteration: 6568 ---- loss: 0.6536045063041385 \n",
            "Iteration: 6569 ---- loss: 0.6536005053802518 \n",
            "Iteration: 6570 ---- loss: 0.6535965059062174 \n",
            "Iteration: 6571 ---- loss: 0.6535925078221284 \n",
            "Iteration: 6572 ---- loss: 0.6535885112282256 \n",
            "Iteration: 6573 ---- loss: 0.6535845161239208 \n",
            "Iteration: 6574 ---- loss: 0.653580522508626 \n",
            "Iteration: 6575 ---- loss: 0.6535765301425049 \n",
            "Iteration: 6576 ---- loss: 0.6535725384009797 \n",
            "Iteration: 6577 ---- loss: 0.6535685473689312 \n",
            "Iteration: 6578 ---- loss: 0.6535645578164451 \n",
            "Iteration: 6579 ---- loss: 0.6535605697429359 \n",
            "Iteration: 6580 ---- loss: 0.6535565831478174 \n",
            "Iteration: 6581 ---- loss: 0.6535525980335962 \n",
            "Iteration: 6582 ---- loss: 0.6535486146323837 \n",
            "Iteration: 6583 ---- loss: 0.6535446329778046 \n",
            "Iteration: 6584 ---- loss: 0.653540652793817 \n",
            "Iteration: 6585 ---- loss: 0.6535366740798374 \n",
            "Iteration: 6586 ---- loss: 0.6535326968994629 \n",
            "Iteration: 6587 ---- loss: 0.6535287211887693 \n",
            "Iteration: 6588 ---- loss: 0.6535247469454192 \n",
            "Iteration: 6589 ---- loss: 0.6535207741688285 \n",
            "Iteration: 6590 ---- loss: 0.6535168028584131 \n",
            "Iteration: 6591 ---- loss: 0.6535128330135885 \n",
            "Iteration: 6592 ---- loss: 0.6535088646337704 \n",
            "Iteration: 6593 ---- loss: 0.6535048977183746 \n",
            "Iteration: 6594 ---- loss: 0.6535009326474961 \n",
            "Iteration: 6595 ---- loss: 0.6534969693267876 \n",
            "Iteration: 6596 ---- loss: 0.6534930074685538 \n",
            "Iteration: 6597 ---- loss: 0.6534890470675733 \n",
            "Iteration: 6598 ---- loss: 0.653485088127501 \n",
            "Iteration: 6599 ---- loss: 0.6534811307656816 \n",
            "Iteration: 6601 ---- loss: 0.6534732221424362 \n",
            "Iteration: 6602 ---- loss: 0.6534692704033014 \n",
            "Iteration: 6603 ---- loss: 0.653465320133931 \n",
            "Iteration: 6604 ---- loss: 0.6534613719594738 \n",
            "Iteration: 6605 ---- loss: 0.6534574267522095 \n",
            "Iteration: 6606 ---- loss: 0.6534534829842068 \n",
            "Iteration: 6607 ---- loss: 0.6534495405910729 \n",
            "Iteration: 6608 ---- loss: 0.6534455996614552 \n",
            "Iteration: 6609 ---- loss: 0.6534416601918258 \n",
            "Iteration: 6610 ---- loss: 0.6534377221816561 \n",
            "Iteration: 6611 ---- loss: 0.653433785653 \n",
            "Iteration: 6612 ---- loss: 0.6534298505960363 \n",
            "Iteration: 6613 ---- loss: 0.6534259168714747 \n",
            "Iteration: 6614 ---- loss: 0.6534219840302479 \n",
            "Iteration: 6615 ---- loss: 0.6534180526388673 \n",
            "Iteration: 6616 ---- loss: 0.653414122703636 \n",
            "Iteration: 6617 ---- loss: 0.6534101943056769 \n",
            "Iteration: 6618 ---- loss: 0.6534062676358038 \n",
            "Iteration: 6619 ---- loss: 0.6534023424188383 \n",
            "Iteration: 6620 ---- loss: 0.653398418654183 \n",
            "Iteration: 6621 ---- loss: 0.6533944963432888 \n",
            "Iteration: 6622 ---- loss: 0.6533905754925695 \n",
            "Iteration: 6623 ---- loss: 0.6533866560849213 \n",
            "Iteration: 6624 ---- loss: 0.6533827381233749 \n",
            "Iteration: 6625 ---- loss: 0.6533788215660888 \n",
            "Iteration: 6626 ---- loss: 0.6533749056923255 \n",
            "Iteration: 6627 ---- loss: 0.6533709909747204 \n",
            "Iteration: 6628 ---- loss: 0.6533670776925603 \n",
            "Iteration: 6629 ---- loss: 0.6533631658402667 \n",
            "Iteration: 6630 ---- loss: 0.6533592546516435 \n",
            "Iteration: 6631 ---- loss: 0.653355342559599 \n",
            "Iteration: 6632 ---- loss: 0.6533514319112372 \n",
            "Iteration: 6633 ---- loss: 0.6533475226716067 \n",
            "Iteration: 6634 ---- loss: 0.6533436148571805 \n",
            "Iteration: 6635 ---- loss: 0.6533397084673513 \n",
            "Iteration: 6636 ---- loss: 0.6533358035121772 \n",
            "Iteration: 6637 ---- loss: 0.6533318999776415 \n",
            "Iteration: 6638 ---- loss: 0.6533279978579174 \n",
            "Iteration: 6639 ---- loss: 0.6533240971603613 \n",
            "Iteration: 6640 ---- loss: 0.6533201978912531 \n",
            "Iteration: 6641 ---- loss: 0.6533163000029825 \n",
            "Iteration: 6642 ---- loss: 0.6533124033855127 \n",
            "Iteration: 6643 ---- loss: 0.653308508004104 \n",
            "Iteration: 6644 ---- loss: 0.653304613811468 \n",
            "Iteration: 6645 ---- loss: 0.6533007210420204 \n",
            "Iteration: 6646 ---- loss: 0.6532968296739329 \n",
            "Iteration: 6647 ---- loss: 0.6532929393032115 \n",
            "Iteration: 6648 ---- loss: 0.6532890484884026 \n",
            "Iteration: 6649 ---- loss: 0.6532851590963044 \n",
            "Iteration: 6651 ---- loss: 0.6532773842772285 \n",
            "Iteration: 6652 ---- loss: 0.6532734989320176 \n",
            "Iteration: 6653 ---- loss: 0.6532696149731508 \n",
            "Iteration: 6654 ---- loss: 0.6532657324076492 \n",
            "Iteration: 6655 ---- loss: 0.6532618511745572 \n",
            "Iteration: 6656 ---- loss: 0.6532579711825098 \n",
            "Iteration: 6657 ---- loss: 0.6532540927564467 \n",
            "Iteration: 6658 ---- loss: 0.6532502157216977 \n",
            "Iteration: 6659 ---- loss: 0.6532463401000195 \n",
            "Iteration: 6660 ---- loss: 0.6532424658855571 \n",
            "Iteration: 6661 ---- loss: 0.6532385923432878 \n",
            "Iteration: 6662 ---- loss: 0.6532347175409206 \n",
            "Iteration: 6663 ---- loss: 0.6532308434542287 \n",
            "Iteration: 6664 ---- loss: 0.6532269707446124 \n",
            "Iteration: 6665 ---- loss: 0.6532230997789661 \n",
            "Iteration: 6666 ---- loss: 0.6532192313297778 \n",
            "Iteration: 6667 ---- loss: 0.6532153642589452 \n",
            "Iteration: 6668 ---- loss: 0.6532114985647719 \n",
            "Iteration: 6669 ---- loss: 0.6532076342241396 \n",
            "Iteration: 6670 ---- loss: 0.6532037710821644 \n",
            "Iteration: 6671 ---- loss: 0.6531999096465589 \n",
            "Iteration: 6672 ---- loss: 0.6531960504172133 \n",
            "Iteration: 6673 ---- loss: 0.6531921925524824 \n",
            "Iteration: 6674 ---- loss: 0.6531883360763164 \n",
            "Iteration: 6675 ---- loss: 0.6531844810040792 \n",
            "Iteration: 6676 ---- loss: 0.653180627308658 \n",
            "Iteration: 6677 ---- loss: 0.6531767774669828 \n",
            "Iteration: 6678 ---- loss: 0.6531729349109526 \n",
            "Iteration: 6679 ---- loss: 0.653169093786165 \n",
            "Iteration: 6680 ---- loss: 0.6531652541262429 \n",
            "Iteration: 6681 ---- loss: 0.6531614163569665 \n",
            "Iteration: 6682 ---- loss: 0.6531575802540529 \n",
            "Iteration: 6683 ---- loss: 0.6531537455693877 \n",
            "Iteration: 6684 ---- loss: 0.6531499122243307 \n",
            "Iteration: 6685 ---- loss: 0.6531460800174821 \n",
            "Iteration: 6686 ---- loss: 0.6531422492151915 \n",
            "Iteration: 6687 ---- loss: 0.6531384198795247 \n",
            "Iteration: 6688 ---- loss: 0.6531345915284543 \n",
            "Iteration: 6689 ---- loss: 0.6531307645012708 \n",
            "Iteration: 6690 ---- loss: 0.653126938873712 \n",
            "Iteration: 6691 ---- loss: 0.6531231145899979 \n",
            "Iteration: 6692 ---- loss: 0.6531192917826614 \n",
            "Iteration: 6693 ---- loss: 0.6531154703215425 \n",
            "Iteration: 6694 ---- loss: 0.653111650238511 \n",
            "Iteration: 6695 ---- loss: 0.6531078316404852 \n",
            "Iteration: 6696 ---- loss: 0.6531040147789107 \n",
            "Iteration: 6697 ---- loss: 0.6531002000910044 \n",
            "Iteration: 6698 ---- loss: 0.6530963867792838 \n",
            "Iteration: 6699 ---- loss: 0.6530925748410497 \n",
            "Iteration: 6701 ---- loss: 0.653084955208461 \n",
            "Iteration: 6702 ---- loss: 0.6530811475764173 \n",
            "Iteration: 6703 ---- loss: 0.6530773421851602 \n",
            "Iteration: 6704 ---- loss: 0.6530735395388524 \n",
            "Iteration: 6705 ---- loss: 0.6530697397736943 \n",
            "Iteration: 6706 ---- loss: 0.6530659413632462 \n",
            "Iteration: 6707 ---- loss: 0.6530621444408339 \n",
            "Iteration: 6708 ---- loss: 0.6530583490122679 \n",
            "Iteration: 6709 ---- loss: 0.6530545550675151 \n",
            "Iteration: 6710 ---- loss: 0.6530507624318865 \n",
            "Iteration: 6711 ---- loss: 0.6530469713170511 \n",
            "Iteration: 6712 ---- loss: 0.6530431815067743 \n",
            "Iteration: 6713 ---- loss: 0.6530393931820817 \n",
            "Iteration: 6714 ---- loss: 0.6530356061737524 \n",
            "Iteration: 6715 ---- loss: 0.6530318217412598 \n",
            "Iteration: 6716 ---- loss: 0.6530280404421773 \n",
            "Iteration: 6717 ---- loss: 0.6530242606498814 \n",
            "Iteration: 6718 ---- loss: 0.6530204821558628 \n",
            "Iteration: 6719 ---- loss: 0.6530167050902654 \n",
            "Iteration: 6720 ---- loss: 0.653012929217456 \n",
            "Iteration: 6721 ---- loss: 0.6530091548413773 \n",
            "Iteration: 6722 ---- loss: 0.653005381820687 \n",
            "Iteration: 6723 ---- loss: 0.6530016104725482 \n",
            "Iteration: 6724 ---- loss: 0.6529978419548326 \n",
            "Iteration: 6725 ---- loss: 0.6529940768869252 \n",
            "Iteration: 6726 ---- loss: 0.6529903131566698 \n",
            "Iteration: 6727 ---- loss: 0.652986550895033 \n",
            "Iteration: 6728 ---- loss: 0.6529827901165188 \n",
            "Iteration: 6729 ---- loss: 0.6529790307460543 \n",
            "Iteration: 6730 ---- loss: 0.6529752727621754 \n",
            "Iteration: 6731 ---- loss: 0.6529715162709676 \n",
            "Iteration: 6732 ---- loss: 0.6529677611363972 \n",
            "Iteration: 6733 ---- loss: 0.6529640074153849 \n",
            "Iteration: 6734 ---- loss: 0.6529602550761237 \n",
            "Iteration: 6735 ---- loss: 0.6529565042434373 \n",
            "Iteration: 6736 ---- loss: 0.6529527547267446 \n",
            "Iteration: 6737 ---- loss: 0.6529490066491989 \n",
            "Iteration: 6738 ---- loss: 0.65294525994416 \n",
            "Iteration: 6739 ---- loss: 0.6529415147376589 \n",
            "Iteration: 6740 ---- loss: 0.6529377708601309 \n",
            "Iteration: 6741 ---- loss: 0.6529340283848055 \n",
            "Iteration: 6742 ---- loss: 0.6529302870601047 \n",
            "Iteration: 6743 ---- loss: 0.6529265469884706 \n",
            "Iteration: 6744 ---- loss: 0.6529228080667061 \n",
            "Iteration: 6745 ---- loss: 0.6529190705091147 \n",
            "Iteration: 6746 ---- loss: 0.6529153344677459 \n",
            "Iteration: 6747 ---- loss: 0.6529116005883377 \n",
            "Iteration: 6748 ---- loss: 0.6529078681669321 \n",
            "Iteration: 6749 ---- loss: 0.6529041371142665 \n",
            "Iteration: 6751 ---- loss: 0.6528966792990816 \n",
            "Iteration: 6752 ---- loss: 0.6528929524584671 \n",
            "Iteration: 6753 ---- loss: 0.6528892270046155 \n",
            "Iteration: 6754 ---- loss: 0.6528855029787319 \n",
            "Iteration: 6755 ---- loss: 0.6528817803568249 \n",
            "Iteration: 6756 ---- loss: 0.6528780590722199 \n",
            "Iteration: 6757 ---- loss: 0.6528743392080132 \n",
            "Iteration: 6758 ---- loss: 0.6528706210078538 \n",
            "Iteration: 6759 ---- loss: 0.6528669053647845 \n",
            "Iteration: 6760 ---- loss: 0.6528631910932542 \n",
            "Iteration: 6761 ---- loss: 0.6528594781942784 \n",
            "Iteration: 6762 ---- loss: 0.6528557667952599 \n",
            "Iteration: 6763 ---- loss: 0.6528520566863372 \n",
            "Iteration: 6764 ---- loss: 0.6528483479738773 \n",
            "Iteration: 6765 ---- loss: 0.6528446404592485 \n",
            "Iteration: 6766 ---- loss: 0.6528409339087786 \n",
            "Iteration: 6767 ---- loss: 0.6528372287912003 \n",
            "Iteration: 6768 ---- loss: 0.6528335250004941 \n",
            "Iteration: 6769 ---- loss: 0.6528298225941856 \n",
            "Iteration: 6770 ---- loss: 0.6528261216385899 \n",
            "Iteration: 6771 ---- loss: 0.6528224214709875 \n",
            "Iteration: 6772 ---- loss: 0.6528187219099088 \n",
            "Iteration: 6773 ---- loss: 0.6528150237439903 \n",
            "Iteration: 6774 ---- loss: 0.6528113269744187 \n",
            "Iteration: 6775 ---- loss: 0.6528076316491841 \n",
            "Iteration: 6776 ---- loss: 0.6528039376797309 \n",
            "Iteration: 6777 ---- loss: 0.6528002450885239 \n",
            "Iteration: 6778 ---- loss: 0.6527965542264672 \n",
            "Iteration: 6779 ---- loss: 0.6527928673833001 \n",
            "Iteration: 6780 ---- loss: 0.6527891819390591 \n",
            "Iteration: 6781 ---- loss: 0.6527854979957681 \n",
            "Iteration: 6782 ---- loss: 0.65278181524102 \n",
            "Iteration: 6783 ---- loss: 0.652778132684222 \n",
            "Iteration: 6784 ---- loss: 0.6527744515357363 \n",
            "Iteration: 6785 ---- loss: 0.652770771775184 \n",
            "Iteration: 6786 ---- loss: 0.6527670933734183 \n",
            "Iteration: 6787 ---- loss: 0.6527634163484352 \n",
            "Iteration: 6788 ---- loss: 0.6527597407227854 \n",
            "Iteration: 6789 ---- loss: 0.6527560665791887 \n",
            "Iteration: 6790 ---- loss: 0.6527523951636712 \n",
            "Iteration: 6791 ---- loss: 0.6527487262382695 \n",
            "Iteration: 6792 ---- loss: 0.652745058636323 \n",
            "Iteration: 6793 ---- loss: 0.6527413924417645 \n",
            "Iteration: 6794 ---- loss: 0.6527377276571127 \n",
            "Iteration: 6795 ---- loss: 0.6527340642591969 \n",
            "Iteration: 6796 ---- loss: 0.6527304017813123 \n",
            "Iteration: 6797 ---- loss: 0.6527267400331889 \n",
            "Iteration: 6798 ---- loss: 0.6527230795737118 \n",
            "Iteration: 6799 ---- loss: 0.652719420255687 \n",
            "Iteration: 6801 ---- loss: 0.652712105463175 \n",
            "Iteration: 6802 ---- loss: 0.6527084499839333 \n",
            "Iteration: 6803 ---- loss: 0.6527047967792718 \n",
            "Iteration: 6804 ---- loss: 0.652701147113748 \n",
            "Iteration: 6805 ---- loss: 0.6526974987990294 \n",
            "Iteration: 6806 ---- loss: 0.6526938516493418 \n",
            "Iteration: 6807 ---- loss: 0.6526902056270424 \n",
            "Iteration: 6808 ---- loss: 0.6526865610107621 \n",
            "Iteration: 6809 ---- loss: 0.6526829177629891 \n",
            "Iteration: 6810 ---- loss: 0.6526792758565457 \n",
            "Iteration: 6811 ---- loss: 0.6526756349849111 \n",
            "Iteration: 6812 ---- loss: 0.6526719945550746 \n",
            "Iteration: 6813 ---- loss: 0.6526683550962358 \n",
            "Iteration: 6814 ---- loss: 0.6526647171537531 \n",
            "Iteration: 6815 ---- loss: 0.6526610805714705 \n",
            "Iteration: 6816 ---- loss: 0.652657445364812 \n",
            "Iteration: 6817 ---- loss: 0.652653811378491 \n",
            "Iteration: 6818 ---- loss: 0.6526501782580444 \n",
            "Iteration: 6819 ---- loss: 0.6526465457612517 \n",
            "Iteration: 6820 ---- loss: 0.6526429146193686 \n",
            "Iteration: 6821 ---- loss: 0.6526392848317571 \n",
            "Iteration: 6822 ---- loss: 0.652635656366517 \n",
            "Iteration: 6823 ---- loss: 0.6526320291347152 \n",
            "Iteration: 6824 ---- loss: 0.6526284029560235 \n",
            "Iteration: 6825 ---- loss: 0.6526247767371814 \n",
            "Iteration: 6826 ---- loss: 0.6526211519422961 \n",
            "Iteration: 6827 ---- loss: 0.6526175285825472 \n",
            "Iteration: 6828 ---- loss: 0.6526139065045199 \n",
            "Iteration: 6829 ---- loss: 0.6526102863463198 \n",
            "Iteration: 6830 ---- loss: 0.6526066675434219 \n",
            "Iteration: 6831 ---- loss: 0.6526030501067092 \n",
            "Iteration: 6832 ---- loss: 0.6525994340105365 \n",
            "Iteration: 6833 ---- loss: 0.6525958192586243 \n",
            "Iteration: 6834 ---- loss: 0.6525922058593706 \n",
            "Iteration: 6835 ---- loss: 0.6525885938121011 \n",
            "Iteration: 6836 ---- loss: 0.6525849831199106 \n",
            "Iteration: 6837 ---- loss: 0.6525813735512318 \n",
            "Iteration: 6838 ---- loss: 0.6525777648377525 \n",
            "Iteration: 6839 ---- loss: 0.6525741574707455 \n",
            "Iteration: 6840 ---- loss: 0.6525705514497449 \n",
            "Iteration: 6841 ---- loss: 0.6525669467735952 \n",
            "Iteration: 6842 ---- loss: 0.6525633434416247 \n",
            "Iteration: 6843 ---- loss: 0.6525597414531619 \n",
            "Iteration: 6844 ---- loss: 0.6525561405925364 \n",
            "Iteration: 6845 ---- loss: 0.6525525407103259 \n",
            "Iteration: 6846 ---- loss: 0.6525489424424911 \n",
            "Iteration: 6847 ---- loss: 0.6525453455196017 \n",
            "Iteration: 6848 ---- loss: 0.652541749940983 \n",
            "Iteration: 6849 ---- loss: 0.6525381557059601 \n",
            "Iteration: 6851 ---- loss: 0.6525309712640049 \n",
            "Iteration: 6852 ---- loss: 0.6525273810557239 \n",
            "Iteration: 6853 ---- loss: 0.6525237921883422 \n",
            "Iteration: 6854 ---- loss: 0.6525202046611854 \n",
            "Iteration: 6855 ---- loss: 0.6525166187738237 \n",
            "Iteration: 6856 ---- loss: 0.6525130357052137 \n",
            "Iteration: 6857 ---- loss: 0.6525094539788334 \n",
            "Iteration: 6858 ---- loss: 0.6525058740460846 \n",
            "Iteration: 6859 ---- loss: 0.652502299697246 \n",
            "Iteration: 6860 ---- loss: 0.6524987266684915 \n",
            "Iteration: 6861 ---- loss: 0.6524951549908854 \n",
            "Iteration: 6862 ---- loss: 0.6524915848073598 \n",
            "Iteration: 6863 ---- loss: 0.6524880164866744 \n",
            "Iteration: 6864 ---- loss: 0.6524844496921178 \n",
            "Iteration: 6865 ---- loss: 0.6524808841639718 \n",
            "Iteration: 6866 ---- loss: 0.6524773198350421 \n",
            "Iteration: 6867 ---- loss: 0.6524737568558655 \n",
            "Iteration: 6868 ---- loss: 0.652470195225725 \n",
            "Iteration: 6869 ---- loss: 0.6524666351610562 \n",
            "Iteration: 6870 ---- loss: 0.6524630775459858 \n",
            "Iteration: 6871 ---- loss: 0.652459520990274 \n",
            "Iteration: 6872 ---- loss: 0.6524559643666621 \n",
            "Iteration: 6873 ---- loss: 0.6524524090981493 \n",
            "Iteration: 6874 ---- loss: 0.6524488554120211 \n",
            "Iteration: 6875 ---- loss: 0.6524453034509843 \n",
            "Iteration: 6876 ---- loss: 0.6524417528404196 \n",
            "Iteration: 6877 ---- loss: 0.6524382035811921 \n",
            "Iteration: 6878 ---- loss: 0.6524346556867939 \n",
            "Iteration: 6879 ---- loss: 0.6524311091415996 \n",
            "Iteration: 6880 ---- loss: 0.6524275635221269 \n",
            "Iteration: 6881 ---- loss: 0.6524240182240768 \n",
            "Iteration: 6882 ---- loss: 0.6524204744222345 \n",
            "Iteration: 6883 ---- loss: 0.652416932271406 \n",
            "Iteration: 6884 ---- loss: 0.652413391465359 \n",
            "Iteration: 6885 ---- loss: 0.6524098520576267 \n",
            "Iteration: 6886 ---- loss: 0.6524063140854881 \n",
            "Iteration: 6887 ---- loss: 0.6524027774516127 \n",
            "Iteration: 6888 ---- loss: 0.6523992421509033 \n",
            "Iteration: 6889 ---- loss: 0.6523957084035917 \n",
            "Iteration: 6890 ---- loss: 0.6523921759892626 \n",
            "Iteration: 6891 ---- loss: 0.6523886449260982 \n",
            "Iteration: 6892 ---- loss: 0.6523851154910145 \n",
            "Iteration: 6893 ---- loss: 0.6523815877780951 \n",
            "Iteration: 6894 ---- loss: 0.6523780614222997 \n",
            "Iteration: 6895 ---- loss: 0.6523745363893442 \n",
            "Iteration: 6896 ---- loss: 0.6523710126988291 \n",
            "Iteration: 6897 ---- loss: 0.6523674900654615 \n",
            "Iteration: 6898 ---- loss: 0.6523639698191696 \n",
            "Iteration: 6899 ---- loss: 0.6523604526163713 \n",
            "Iteration: 6901 ---- loss: 0.6523534224121681 \n",
            "Iteration: 6902 ---- loss: 0.6523499097008506 \n",
            "Iteration: 6903 ---- loss: 0.6523463983511707 \n",
            "Iteration: 6904 ---- loss: 0.6523428883252419 \n",
            "Iteration: 6905 ---- loss: 0.65233937965725 \n",
            "Iteration: 6906 ---- loss: 0.6523358723123163 \n",
            "Iteration: 6907 ---- loss: 0.6523323662698673 \n",
            "Iteration: 6908 ---- loss: 0.6523288613480925 \n",
            "Iteration: 6909 ---- loss: 0.6523253577852143 \n",
            "Iteration: 6910 ---- loss: 0.6523218557418091 \n",
            "Iteration: 6911 ---- loss: 0.6523183556095445 \n",
            "Iteration: 6912 ---- loss: 0.6523148568095859 \n",
            "Iteration: 6913 ---- loss: 0.6523113593010718 \n",
            "Iteration: 6914 ---- loss: 0.6523078630817496 \n",
            "Iteration: 6915 ---- loss: 0.6523043678559077 \n",
            "Iteration: 6916 ---- loss: 0.652300873444892 \n",
            "Iteration: 6917 ---- loss: 0.6522973803545801 \n",
            "Iteration: 6918 ---- loss: 0.6522938886005014 \n",
            "Iteration: 6919 ---- loss: 0.6522903981888952 \n",
            "Iteration: 6920 ---- loss: 0.6522869090972578 \n",
            "Iteration: 6921 ---- loss: 0.6522834215010315 \n",
            "Iteration: 6922 ---- loss: 0.6522799348736579 \n",
            "Iteration: 6923 ---- loss: 0.6522764495003961 \n",
            "Iteration: 6924 ---- loss: 0.6522729654583241 \n",
            "Iteration: 6925 ---- loss: 0.6522694826778649 \n",
            "Iteration: 6926 ---- loss: 0.6522660008008174 \n",
            "Iteration: 6927 ---- loss: 0.6522625208219823 \n",
            "Iteration: 6928 ---- loss: 0.6522590439683055 \n",
            "Iteration: 6929 ---- loss: 0.6522555684388522 \n",
            "Iteration: 6930 ---- loss: 0.6522520942338772 \n",
            "Iteration: 6931 ---- loss: 0.6522486213566994 \n",
            "Iteration: 6932 ---- loss: 0.6522451498129587 \n",
            "Iteration: 6933 ---- loss: 0.6522416795922747 \n",
            "Iteration: 6934 ---- loss: 0.6522382109090645 \n",
            "Iteration: 6935 ---- loss: 0.6522347439058702 \n",
            "Iteration: 6936 ---- loss: 0.6522312782284382 \n",
            "Iteration: 6937 ---- loss: 0.6522278133026406 \n",
            "Iteration: 6938 ---- loss: 0.6522243475612339 \n",
            "Iteration: 6939 ---- loss: 0.652220882881872 \n",
            "Iteration: 6940 ---- loss: 0.65221741952097 \n",
            "Iteration: 6941 ---- loss: 0.6522139574777979 \n",
            "Iteration: 6942 ---- loss: 0.6522104967516255 \n",
            "Iteration: 6943 ---- loss: 0.6522070373417229 \n",
            "Iteration: 6944 ---- loss: 0.6522035792506038 \n",
            "Iteration: 6945 ---- loss: 0.6522001226610126 \n",
            "Iteration: 6946 ---- loss: 0.6521966678025358 \n",
            "Iteration: 6947 ---- loss: 0.6521932146198731 \n",
            "Iteration: 6948 ---- loss: 0.6521897627655047 \n",
            "Iteration: 6949 ---- loss: 0.6521863119988491 \n",
            "Iteration: 6951 ---- loss: 0.6521794144067757 \n",
            "Iteration: 6952 ---- loss: 0.6521759675798938 \n",
            "Iteration: 6953 ---- loss: 0.6521725220667044 \n",
            "Iteration: 6954 ---- loss: 0.6521690778661255 \n",
            "Iteration: 6955 ---- loss: 0.6521656349720908 \n",
            "Iteration: 6956 ---- loss: 0.6521621933876496 \n",
            "Iteration: 6957 ---- loss: 0.6521587531120717 \n",
            "Iteration: 6958 ---- loss: 0.6521553141467487 \n",
            "Iteration: 6959 ---- loss: 0.6521518764921556 \n",
            "Iteration: 6960 ---- loss: 0.652148439983158 \n",
            "Iteration: 6961 ---- loss: 0.6521450039733191 \n",
            "Iteration: 6962 ---- loss: 0.652141569098757 \n",
            "Iteration: 6963 ---- loss: 0.6521381355250977 \n",
            "Iteration: 6964 ---- loss: 0.6521347031792163 \n",
            "Iteration: 6965 ---- loss: 0.6521312719616194 \n",
            "Iteration: 6966 ---- loss: 0.6521278420420173 \n",
            "Iteration: 6967 ---- loss: 0.6521244131356217 \n",
            "Iteration: 6968 ---- loss: 0.6521209836506692 \n",
            "Iteration: 6969 ---- loss: 0.6521175554556324 \n",
            "Iteration: 6970 ---- loss: 0.6521141286762074 \n",
            "Iteration: 6971 ---- loss: 0.6521107034571271 \n",
            "Iteration: 6972 ---- loss: 0.6521072796392704 \n",
            "Iteration: 6973 ---- loss: 0.6521038571101688 \n",
            "Iteration: 6974 ---- loss: 0.6521004358691047 \n",
            "Iteration: 6975 ---- loss: 0.6520970159153597 \n",
            "Iteration: 6976 ---- loss: 0.6520935972482161 \n",
            "Iteration: 6977 ---- loss: 0.6520901798669567 \n",
            "Iteration: 6978 ---- loss: 0.6520867637708638 \n",
            "Iteration: 6979 ---- loss: 0.6520833490681082 \n",
            "Iteration: 6980 ---- loss: 0.6520799361988066 \n",
            "Iteration: 6981 ---- loss: 0.6520765246140793 \n",
            "Iteration: 6982 ---- loss: 0.6520731143132087 \n",
            "Iteration: 6983 ---- loss: 0.6520697052954761 \n",
            "Iteration: 6984 ---- loss: 0.6520662975601643 \n",
            "Iteration: 6985 ---- loss: 0.6520628911065556 \n",
            "Iteration: 6986 ---- loss: 0.6520594859339331 \n",
            "Iteration: 6987 ---- loss: 0.652056082041579 \n",
            "Iteration: 6988 ---- loss: 0.6520526794287773 \n",
            "Iteration: 6989 ---- loss: 0.652049278011818 \n",
            "Iteration: 6990 ---- loss: 0.6520458777334737 \n",
            "Iteration: 6991 ---- loss: 0.6520424799503751 \n",
            "Iteration: 6992 ---- loss: 0.6520390834463439 \n",
            "Iteration: 6993 ---- loss: 0.652035688213401 \n",
            "Iteration: 6994 ---- loss: 0.6520322942734705 \n",
            "Iteration: 6995 ---- loss: 0.6520289015979607 \n",
            "Iteration: 6996 ---- loss: 0.6520255101963804 \n",
            "Iteration: 6997 ---- loss: 0.6520221200566814 \n",
            "Iteration: 6998 ---- loss: 0.6520187311835793 \n",
            "Iteration: 6999 ---- loss: 0.6520153436017104 \n",
            "Iteration: 7001 ---- loss: 0.6520085717892111 \n",
            "Iteration: 7002 ---- loss: 0.6520051849950889 \n",
            "Iteration: 7003 ---- loss: 0.6520017994725587 \n",
            "Iteration: 7004 ---- loss: 0.6519984152047704 \n",
            "Iteration: 7005 ---- loss: 0.651995032205131 \n",
            "Iteration: 7006 ---- loss: 0.6519916504590877 \n",
            "Iteration: 7007 ---- loss: 0.6519882699812267 \n",
            "Iteration: 7008 ---- loss: 0.6519848907523441 \n",
            "Iteration: 7009 ---- loss: 0.6519815127951472 \n",
            "Iteration: 7010 ---- loss: 0.6519781360801258 \n",
            "Iteration: 7011 ---- loss: 0.6519747614363449 \n",
            "Iteration: 7012 ---- loss: 0.6519713911417171 \n",
            "Iteration: 7013 ---- loss: 0.6519680221244952 \n",
            "Iteration: 7014 ---- loss: 0.6519646543632901 \n",
            "Iteration: 7015 ---- loss: 0.6519612878650796 \n",
            "Iteration: 7016 ---- loss: 0.6519579226626054 \n",
            "Iteration: 7017 ---- loss: 0.6519545592728339 \n",
            "Iteration: 7018 ---- loss: 0.6519511976988492 \n",
            "Iteration: 7019 ---- loss: 0.6519478374091388 \n",
            "Iteration: 7020 ---- loss: 0.6519444790135507 \n",
            "Iteration: 7021 ---- loss: 0.6519411215877746 \n",
            "Iteration: 7022 ---- loss: 0.651937765421623 \n",
            "Iteration: 7023 ---- loss: 0.6519344105128606 \n",
            "Iteration: 7024 ---- loss: 0.6519310568716135 \n",
            "Iteration: 7025 ---- loss: 0.6519277044927293 \n",
            "Iteration: 7026 ---- loss: 0.6519243533672443 \n",
            "Iteration: 7027 ---- loss: 0.651921003685681 \n",
            "Iteration: 7028 ---- loss: 0.6519176556332477 \n",
            "Iteration: 7029 ---- loss: 0.6519143088258029 \n",
            "Iteration: 7030 ---- loss: 0.6519109630453543 \n",
            "Iteration: 7031 ---- loss: 0.6519076181831431 \n",
            "Iteration: 7032 ---- loss: 0.65190427334566 \n",
            "Iteration: 7033 ---- loss: 0.6519009297389172 \n",
            "Iteration: 7034 ---- loss: 0.6518975873935301 \n",
            "Iteration: 7035 ---- loss: 0.6518942463009028 \n",
            "Iteration: 7036 ---- loss: 0.6518909064748175 \n",
            "Iteration: 7037 ---- loss: 0.6518875678781358 \n",
            "Iteration: 7038 ---- loss: 0.6518842305398524 \n",
            "Iteration: 7039 ---- loss: 0.651880894451713 \n",
            "Iteration: 7040 ---- loss: 0.6518775596128983 \n",
            "Iteration: 7041 ---- loss: 0.6518742265601356 \n",
            "Iteration: 7042 ---- loss: 0.6518708953107416 \n",
            "Iteration: 7043 ---- loss: 0.6518675653127477 \n",
            "Iteration: 7044 ---- loss: 0.6518642365644829 \n",
            "Iteration: 7045 ---- loss: 0.6518609090652223 \n",
            "Iteration: 7046 ---- loss: 0.6518575830198493 \n",
            "Iteration: 7047 ---- loss: 0.6518542598470473 \n",
            "Iteration: 7048 ---- loss: 0.6518509379219879 \n",
            "Iteration: 7049 ---- loss: 0.6518476173377048 \n",
            "Iteration: 7051 ---- loss: 0.6518409802029869 \n",
            "Iteration: 7052 ---- loss: 0.6518376635029348 \n",
            "Iteration: 7053 ---- loss: 0.6518343480467874 \n",
            "Iteration: 7054 ---- loss: 0.6518310338338213 \n",
            "Iteration: 7055 ---- loss: 0.6518277209312017 \n",
            "Iteration: 7056 ---- loss: 0.6518244104109904 \n",
            "Iteration: 7057 ---- loss: 0.6518211011327679 \n",
            "Iteration: 7058 ---- loss: 0.6518177930958168 \n",
            "Iteration: 7059 ---- loss: 0.6518144862994188 \n",
            "Iteration: 7060 ---- loss: 0.6518111807307717 \n",
            "Iteration: 7061 ---- loss: 0.6518078763089306 \n",
            "Iteration: 7062 ---- loss: 0.6518045730213948 \n",
            "Iteration: 7063 ---- loss: 0.6518012699661316 \n",
            "Iteration: 7064 ---- loss: 0.6517979681796688 \n",
            "Iteration: 7065 ---- loss: 0.6517946676307451 \n",
            "Iteration: 7066 ---- loss: 0.6517913682276624 \n",
            "Iteration: 7067 ---- loss: 0.6517880697573553 \n",
            "Iteration: 7068 ---- loss: 0.6517847706102318 \n",
            "Iteration: 7069 ---- loss: 0.6517814703174194 \n",
            "Iteration: 7070 ---- loss: 0.6517781714732497 \n",
            "Iteration: 7071 ---- loss: 0.651774874037689 \n",
            "Iteration: 7072 ---- loss: 0.6517715777087453 \n",
            "Iteration: 7073 ---- loss: 0.6517682822823939 \n",
            "Iteration: 7074 ---- loss: 0.651764987333534 \n",
            "Iteration: 7075 ---- loss: 0.6517616917789575 \n",
            "Iteration: 7076 ---- loss: 0.6517583952204408 \n",
            "Iteration: 7077 ---- loss: 0.6517551009946552 \n",
            "Iteration: 7078 ---- loss: 0.651751809499857 \n",
            "Iteration: 7079 ---- loss: 0.651748519125257 \n",
            "Iteration: 7080 ---- loss: 0.6517452298579176 \n",
            "Iteration: 7081 ---- loss: 0.6517419417365545 \n",
            "Iteration: 7082 ---- loss: 0.6517386547000164 \n",
            "Iteration: 7083 ---- loss: 0.6517353688822348 \n",
            "Iteration: 7084 ---- loss: 0.6517320842657129 \n",
            "Iteration: 7085 ---- loss: 0.6517288008395459 \n",
            "Iteration: 7086 ---- loss: 0.6517255180829143 \n",
            "Iteration: 7087 ---- loss: 0.6517222365235672 \n",
            "Iteration: 7088 ---- loss: 0.651718955772254 \n",
            "Iteration: 7089 ---- loss: 0.6517156746991042 \n",
            "Iteration: 7090 ---- loss: 0.6517123948423981 \n",
            "Iteration: 7091 ---- loss: 0.6517091164308374 \n",
            "Iteration: 7092 ---- loss: 0.651705839540996 \n",
            "Iteration: 7093 ---- loss: 0.651702563619238 \n",
            "Iteration: 7094 ---- loss: 0.6516992888993101 \n",
            "Iteration: 7095 ---- loss: 0.6516960153714828 \n",
            "Iteration: 7096 ---- loss: 0.6516927430315017 \n",
            "Iteration: 7097 ---- loss: 0.6516894726812261 \n",
            "Iteration: 7098 ---- loss: 0.6516862058276769 \n",
            "Iteration: 7099 ---- loss: 0.6516829401623329 \n",
            "Iteration: 7101 ---- loss: 0.6516764127146388 \n",
            "Iteration: 7102 ---- loss: 0.6516731519105965 \n",
            "Iteration: 7103 ---- loss: 0.6516698922770722 \n",
            "Iteration: 7104 ---- loss: 0.65166663358215 \n",
            "Iteration: 7105 ---- loss: 0.651663374580324 \n",
            "Iteration: 7106 ---- loss: 0.6516601167716612 \n",
            "Iteration: 7107 ---- loss: 0.6516568601455324 \n",
            "Iteration: 7108 ---- loss: 0.6516536047086453 \n",
            "Iteration: 7109 ---- loss: 0.6516503505948114 \n",
            "Iteration: 7110 ---- loss: 0.6516470991386815 \n",
            "Iteration: 7111 ---- loss: 0.6516438494968299 \n",
            "Iteration: 7112 ---- loss: 0.651640600143459 \n",
            "Iteration: 7113 ---- loss: 0.6516373498668941 \n",
            "Iteration: 7114 ---- loss: 0.6516340985168027 \n",
            "Iteration: 7115 ---- loss: 0.651630848383119 \n",
            "Iteration: 7116 ---- loss: 0.6516275994263434 \n",
            "Iteration: 7117 ---- loss: 0.6516243516504342 \n",
            "Iteration: 7118 ---- loss: 0.6516211051320464 \n",
            "Iteration: 7119 ---- loss: 0.6516178599583267 \n",
            "Iteration: 7120 ---- loss: 0.6516146159597239 \n",
            "Iteration: 7121 ---- loss: 0.6516113731355035 \n",
            "Iteration: 7122 ---- loss: 0.6516081314849318 \n",
            "Iteration: 7123 ---- loss: 0.6516048910081986 \n",
            "Iteration: 7124 ---- loss: 0.6516016517040126 \n",
            "Iteration: 7125 ---- loss: 0.6515984135693312 \n",
            "Iteration: 7126 ---- loss: 0.651595176561863 \n",
            "Iteration: 7127 ---- loss: 0.6515919397715506 \n",
            "Iteration: 7128 ---- loss: 0.6515887041492975 \n",
            "Iteration: 7129 ---- loss: 0.651585469694374 \n",
            "Iteration: 7130 ---- loss: 0.6515822364577538 \n",
            "Iteration: 7131 ---- loss: 0.651579004098541 \n",
            "Iteration: 7132 ---- loss: 0.6515757718028601 \n",
            "Iteration: 7133 ---- loss: 0.6515725405466417 \n",
            "Iteration: 7134 ---- loss: 0.6515693107217649 \n",
            "Iteration: 7135 ---- loss: 0.6515660819602269 \n",
            "Iteration: 7136 ---- loss: 0.6515628539983053 \n",
            "Iteration: 7137 ---- loss: 0.651559627202503 \n",
            "Iteration: 7138 ---- loss: 0.6515564014596498 \n",
            "Iteration: 7139 ---- loss: 0.6515531767685312 \n",
            "Iteration: 7140 ---- loss: 0.6515499530423488 \n",
            "Iteration: 7141 ---- loss: 0.651546730475867 \n",
            "Iteration: 7142 ---- loss: 0.6515435090683611 \n",
            "Iteration: 7143 ---- loss: 0.651540288819107 \n",
            "Iteration: 7144 ---- loss: 0.6515370697273805 \n",
            "Iteration: 7145 ---- loss: 0.6515338517924592 \n",
            "Iteration: 7146 ---- loss: 0.6515306346260118 \n",
            "Iteration: 7147 ---- loss: 0.6515274190015721 \n",
            "Iteration: 7148 ---- loss: 0.6515242054629712 \n",
            "Iteration: 7149 ---- loss: 0.6515209930767526 \n",
            "Iteration: 7151 ---- loss: 0.6515145717585874 \n",
            "Iteration: 7152 ---- loss: 0.6515113628252052 \n",
            "Iteration: 7153 ---- loss: 0.6515081549689048 \n",
            "Iteration: 7154 ---- loss: 0.6515049470488069 \n",
            "Iteration: 7155 ---- loss: 0.6515017387779545 \n",
            "Iteration: 7156 ---- loss: 0.651498530011594 \n",
            "Iteration: 7157 ---- loss: 0.6514953223837725 \n",
            "Iteration: 7158 ---- loss: 0.6514921156852983 \n",
            "Iteration: 7159 ---- loss: 0.6514889098717014 \n",
            "Iteration: 7160 ---- loss: 0.651485704864254 \n",
            "Iteration: 7161 ---- loss: 0.6514824992250001 \n",
            "Iteration: 7162 ---- loss: 0.6514792950634112 \n",
            "Iteration: 7163 ---- loss: 0.6514760930045257 \n",
            "Iteration: 7164 ---- loss: 0.6514728952211933 \n",
            "Iteration: 7165 ---- loss: 0.6514696987303807 \n",
            "Iteration: 7166 ---- loss: 0.6514665033759871 \n",
            "Iteration: 7167 ---- loss: 0.6514633091573009 \n",
            "Iteration: 7168 ---- loss: 0.6514601166785862 \n",
            "Iteration: 7169 ---- loss: 0.6514569259231634 \n",
            "Iteration: 7170 ---- loss: 0.6514537363033062 \n",
            "Iteration: 7171 ---- loss: 0.6514505478183054 \n",
            "Iteration: 7172 ---- loss: 0.6514473604674519 \n",
            "Iteration: 7173 ---- loss: 0.6514441740920656 \n",
            "Iteration: 7174 ---- loss: 0.6514409884863297 \n",
            "Iteration: 7175 ---- loss: 0.65143780439456 \n",
            "Iteration: 7176 ---- loss: 0.651434621435966 \n",
            "Iteration: 7177 ---- loss: 0.6514314397183741 \n",
            "Iteration: 7178 ---- loss: 0.6514282592361211 \n",
            "Iteration: 7179 ---- loss: 0.6514250809064671 \n",
            "Iteration: 7180 ---- loss: 0.6514219088222031 \n",
            "Iteration: 7181 ---- loss: 0.6514187394706206 \n",
            "Iteration: 7182 ---- loss: 0.6514155716251051 \n",
            "Iteration: 7183 ---- loss: 0.6514124057496725 \n",
            "Iteration: 7184 ---- loss: 0.651409241018396 \n",
            "Iteration: 7185 ---- loss: 0.6514060775451498 \n",
            "Iteration: 7186 ---- loss: 0.6514029149285164 \n",
            "Iteration: 7187 ---- loss: 0.6513997528922553 \n",
            "Iteration: 7188 ---- loss: 0.6513965919985165 \n",
            "Iteration: 7189 ---- loss: 0.6513934322465746 \n",
            "Iteration: 7190 ---- loss: 0.6513902736357042 \n",
            "Iteration: 7191 ---- loss: 0.651387116165181 \n",
            "Iteration: 7192 ---- loss: 0.651383961397839 \n",
            "Iteration: 7193 ---- loss: 0.6513808145840794 \n",
            "Iteration: 7194 ---- loss: 0.6513776689286607 \n",
            "Iteration: 7195 ---- loss: 0.6513745258966729 \n",
            "Iteration: 7196 ---- loss: 0.651371385672492 \n",
            "Iteration: 7197 ---- loss: 0.65136824659877 \n",
            "Iteration: 7198 ---- loss: 0.6513651090040911 \n",
            "Iteration: 7199 ---- loss: 0.651361975458823 \n",
            "Iteration: 7201 ---- loss: 0.651355711818018 \n",
            "Iteration: 7202 ---- loss: 0.6513525817201147 \n",
            "Iteration: 7203 ---- loss: 0.6513494527667927 \n",
            "Iteration: 7204 ---- loss: 0.6513463249606637 \n",
            "Iteration: 7205 ---- loss: 0.6513431979464865 \n",
            "Iteration: 7206 ---- loss: 0.6513400707069511 \n",
            "Iteration: 7207 ---- loss: 0.6513369446096688 \n",
            "Iteration: 7208 ---- loss: 0.6513338196539051 \n",
            "Iteration: 7209 ---- loss: 0.6513306958389246 \n",
            "Iteration: 7210 ---- loss: 0.6513275731639931 \n",
            "Iteration: 7211 ---- loss: 0.6513244511191157 \n",
            "Iteration: 7212 ---- loss: 0.6513213292643927 \n",
            "Iteration: 7213 ---- loss: 0.6513182089257977 \n",
            "Iteration: 7214 ---- loss: 0.651315089719357 \n",
            "Iteration: 7215 ---- loss: 0.6513119716443405 \n",
            "Iteration: 7216 ---- loss: 0.6513088547000188 \n",
            "Iteration: 7217 ---- loss: 0.6513057401993996 \n",
            "Iteration: 7218 ---- loss: 0.6513026292027473 \n",
            "Iteration: 7219 ---- loss: 0.6512995193813478 \n",
            "Iteration: 7220 ---- loss: 0.651296412330175 \n",
            "Iteration: 7221 ---- loss: 0.6512933064115427 \n",
            "Iteration: 7222 ---- loss: 0.6512902016323322 \n",
            "Iteration: 7223 ---- loss: 0.6512870979917923 \n",
            "Iteration: 7224 ---- loss: 0.6512839954891718 \n",
            "Iteration: 7225 ---- loss: 0.6512808936640924 \n",
            "Iteration: 7226 ---- loss: 0.6512777919521631 \n",
            "Iteration: 7227 ---- loss: 0.6512746913713502 \n",
            "Iteration: 7228 ---- loss: 0.6512715919182855 \n",
            "Iteration: 7229 ---- loss: 0.6512684934879847 \n",
            "Iteration: 7230 ---- loss: 0.6512653964445148 \n",
            "Iteration: 7231 ---- loss: 0.651262300913733 \n",
            "Iteration: 7232 ---- loss: 0.6512592054832833 \n",
            "Iteration: 7233 ---- loss: 0.6512561098048396 \n",
            "Iteration: 7234 ---- loss: 0.6512530151620395 \n",
            "Iteration: 7235 ---- loss: 0.6512499213612297 \n",
            "Iteration: 7236 ---- loss: 0.6512468286816516 \n",
            "Iteration: 7237 ---- loss: 0.6512437371613156 \n",
            "Iteration: 7238 ---- loss: 0.6512406467956066 \n",
            "Iteration: 7239 ---- loss: 0.6512375575510716 \n",
            "Iteration: 7240 ---- loss: 0.6512344693529122 \n",
            "Iteration: 7241 ---- loss: 0.651231380926001 \n",
            "Iteration: 7242 ---- loss: 0.6512282936134225 \n",
            "Iteration: 7243 ---- loss: 0.6512252074144422 \n",
            "Iteration: 7244 ---- loss: 0.6512221223283262 \n",
            "Iteration: 7245 ---- loss: 0.6512190383543408 \n",
            "Iteration: 7246 ---- loss: 0.6512159554917527 \n",
            "Iteration: 7247 ---- loss: 0.6512128737393558 \n",
            "Iteration: 7248 ---- loss: 0.6512097923520139 \n",
            "Iteration: 7249 ---- loss: 0.6512067103695471 \n",
            "Iteration: 7251 ---- loss: 0.6512005491064313 \n",
            "Iteration: 7252 ---- loss: 0.6511974697770223 \n",
            "Iteration: 7253 ---- loss: 0.6511943915456218 \n",
            "Iteration: 7254 ---- loss: 0.6511913144115059 \n",
            "Iteration: 7255 ---- loss: 0.6511882387144575 \n",
            "Iteration: 7256 ---- loss: 0.651185161544468 \n",
            "Iteration: 7257 ---- loss: 0.6511820844679568 \n",
            "Iteration: 7258 ---- loss: 0.6511790119074955 \n",
            "Iteration: 7259 ---- loss: 0.6511759406230719 \n",
            "Iteration: 7260 ---- loss: 0.651172870839881 \n",
            "Iteration: 7261 ---- loss: 0.6511698021242036 \n",
            "Iteration: 7262 ---- loss: 0.6511667345110457 \n",
            "Iteration: 7263 ---- loss: 0.6511636678751104 \n",
            "Iteration: 7264 ---- loss: 0.6511606017362851 \n",
            "Iteration: 7265 ---- loss: 0.6511575351923722 \n",
            "Iteration: 7266 ---- loss: 0.6511544702322373 \n",
            "Iteration: 7267 ---- loss: 0.6511514069977764 \n",
            "Iteration: 7268 ---- loss: 0.6511483448375516 \n",
            "Iteration: 7269 ---- loss: 0.651145283771262 \n",
            "Iteration: 7270 ---- loss: 0.6511422237588655 \n",
            "Iteration: 7271 ---- loss: 0.6511391638804913 \n",
            "Iteration: 7272 ---- loss: 0.65113610399108 \n",
            "Iteration: 7273 ---- loss: 0.6511330450878413 \n",
            "Iteration: 7274 ---- loss: 0.6511299870313096 \n",
            "Iteration: 7275 ---- loss: 0.6511269301955152 \n",
            "Iteration: 7276 ---- loss: 0.6511238744211795 \n",
            "Iteration: 7277 ---- loss: 0.6511208197091162 \n",
            "Iteration: 7278 ---- loss: 0.6511177660976466 \n",
            "Iteration: 7279 ---- loss: 0.651114713516697 \n",
            "Iteration: 7280 ---- loss: 0.6511116620427104 \n",
            "Iteration: 7281 ---- loss: 0.6511086116136672 \n",
            "Iteration: 7282 ---- loss: 0.651105562249514 \n",
            "Iteration: 7283 ---- loss: 0.6511025163531924 \n",
            "Iteration: 7284 ---- loss: 0.6510994824198637 \n",
            "Iteration: 7285 ---- loss: 0.6510964495904205 \n",
            "Iteration: 7286 ---- loss: 0.6510934178356668 \n",
            "Iteration: 7287 ---- loss: 0.6510903871373168 \n",
            "Iteration: 7288 ---- loss: 0.6510873575503049 \n",
            "Iteration: 7289 ---- loss: 0.6510843290144814 \n",
            "Iteration: 7290 ---- loss: 0.6510813015114677 \n",
            "Iteration: 7291 ---- loss: 0.6510782774317833 \n",
            "Iteration: 7292 ---- loss: 0.6510752594560958 \n",
            "Iteration: 7293 ---- loss: 0.6510722418845222 \n",
            "Iteration: 7294 ---- loss: 0.651069225950419 \n",
            "Iteration: 7295 ---- loss: 0.6510662110645883 \n",
            "Iteration: 7296 ---- loss: 0.6510631970356618 \n",
            "Iteration: 7297 ---- loss: 0.6510601838119726 \n",
            "Iteration: 7298 ---- loss: 0.6510571716481968 \n",
            "Iteration: 7299 ---- loss: 0.6510541605507616 \n",
            "Iteration: 7301 ---- loss: 0.6510481416188802 \n",
            "Iteration: 7302 ---- loss: 0.6510451334761322 \n",
            "Iteration: 7303 ---- loss: 0.6510421259048075 \n",
            "Iteration: 7304 ---- loss: 0.6510391194311308 \n",
            "Iteration: 7305 ---- loss: 0.651036114228771 \n",
            "Iteration: 7306 ---- loss: 0.6510331048342252 \n",
            "Iteration: 7307 ---- loss: 0.6510300964953727 \n",
            "Iteration: 7308 ---- loss: 0.6510270892545436 \n",
            "Iteration: 7309 ---- loss: 0.6510240834238674 \n",
            "Iteration: 7310 ---- loss: 0.651021080647454 \n",
            "Iteration: 7311 ---- loss: 0.6510180789274992 \n",
            "Iteration: 7312 ---- loss: 0.6510150777865559 \n",
            "Iteration: 7313 ---- loss: 0.651012076439475 \n",
            "Iteration: 7314 ---- loss: 0.6510090761366071 \n",
            "Iteration: 7315 ---- loss: 0.6510060768827807 \n",
            "Iteration: 7316 ---- loss: 0.6510030786809722 \n",
            "Iteration: 7317 ---- loss: 0.6510000815412476 \n",
            "Iteration: 7318 ---- loss: 0.6509970854277527 \n",
            "Iteration: 7319 ---- loss: 0.6509940903604591 \n",
            "Iteration: 7320 ---- loss: 0.6509910963282171 \n",
            "Iteration: 7321 ---- loss: 0.6509881031954748 \n",
            "Iteration: 7322 ---- loss: 0.6509851108550769 \n",
            "Iteration: 7323 ---- loss: 0.6509821195360035 \n",
            "Iteration: 7324 ---- loss: 0.6509791292581246 \n",
            "Iteration: 7325 ---- loss: 0.6509761400215387 \n",
            "Iteration: 7326 ---- loss: 0.6509731518255353 \n",
            "Iteration: 7327 ---- loss: 0.6509701648455497 \n",
            "Iteration: 7328 ---- loss: 0.6509671790299034 \n",
            "Iteration: 7329 ---- loss: 0.6509641940708458 \n",
            "Iteration: 7330 ---- loss: 0.6509612094896972 \n",
            "Iteration: 7331 ---- loss: 0.6509582251986517 \n",
            "Iteration: 7332 ---- loss: 0.650955241941312 \n",
            "Iteration: 7333 ---- loss: 0.6509522597169743 \n",
            "Iteration: 7334 ---- loss: 0.6509492781964762 \n",
            "Iteration: 7335 ---- loss: 0.6509462972697037 \n",
            "Iteration: 7336 ---- loss: 0.6509433167868256 \n",
            "Iteration: 7337 ---- loss: 0.6509403343872782 \n",
            "Iteration: 7338 ---- loss: 0.650937353004983 \n",
            "Iteration: 7339 ---- loss: 0.6509343726429369 \n",
            "Iteration: 7340 ---- loss: 0.6509313933004434 \n",
            "Iteration: 7341 ---- loss: 0.6509284149768072 \n",
            "Iteration: 7342 ---- loss: 0.6509254376684409 \n",
            "Iteration: 7343 ---- loss: 0.6509224620406941 \n",
            "Iteration: 7344 ---- loss: 0.6509194874305142 \n",
            "Iteration: 7345 ---- loss: 0.6509165139231551 \n",
            "Iteration: 7346 ---- loss: 0.6509135432316661 \n",
            "Iteration: 7347 ---- loss: 0.650910576746314 \n",
            "Iteration: 7348 ---- loss: 0.6509076107280488 \n",
            "Iteration: 7349 ---- loss: 0.6509046434528925 \n",
            "Iteration: 7351 ---- loss: 0.6508987119477864 \n",
            "Iteration: 7352 ---- loss: 0.650895747716687 \n",
            "Iteration: 7353 ---- loss: 0.6508927844993803 \n",
            "Iteration: 7354 ---- loss: 0.6508898225756229 \n",
            "Iteration: 7355 ---- loss: 0.6508868626114939 \n",
            "Iteration: 7356 ---- loss: 0.6508839036605178 \n",
            "Iteration: 7357 ---- loss: 0.6508809457219936 \n",
            "Iteration: 7358 ---- loss: 0.6508779887952203 \n",
            "Iteration: 7359 ---- loss: 0.6508750329657685 \n",
            "Iteration: 7360 ---- loss: 0.6508720804881444 \n",
            "Iteration: 7361 ---- loss: 0.6508691293982511 \n",
            "Iteration: 7362 ---- loss: 0.650866179321836 \n",
            "Iteration: 7363 ---- loss: 0.6508632303605838 \n",
            "Iteration: 7364 ---- loss: 0.6508602826018335 \n",
            "Iteration: 7365 ---- loss: 0.6508573361648957 \n",
            "Iteration: 7366 ---- loss: 0.6508543914161473 \n",
            "Iteration: 7367 ---- loss: 0.6508514499749729 \n",
            "Iteration: 7368 ---- loss: 0.6508485124829676 \n",
            "Iteration: 7369 ---- loss: 0.6508455754773282 \n",
            "Iteration: 7370 ---- loss: 0.6508426394246466 \n",
            "Iteration: 7371 ---- loss: 0.6508397039076668 \n",
            "Iteration: 7372 ---- loss: 0.65083676908675 \n",
            "Iteration: 7373 ---- loss: 0.6508338348340207 \n",
            "Iteration: 7374 ---- loss: 0.650830901913099 \n",
            "Iteration: 7375 ---- loss: 0.6508279707744744 \n",
            "Iteration: 7376 ---- loss: 0.6508250409080996 \n",
            "Iteration: 7377 ---- loss: 0.6508221132184135 \n",
            "Iteration: 7378 ---- loss: 0.6508191886043911 \n",
            "Iteration: 7379 ---- loss: 0.6508162650122213 \n",
            "Iteration: 7380 ---- loss: 0.6508133424394917 \n",
            "Iteration: 7381 ---- loss: 0.6508104208855132 \n",
            "Iteration: 7382 ---- loss: 0.6508075009945067 \n",
            "Iteration: 7383 ---- loss: 0.6508045826531216 \n",
            "Iteration: 7384 ---- loss: 0.6508016622517245 \n",
            "Iteration: 7385 ---- loss: 0.6507987416315513 \n",
            "Iteration: 7386 ---- loss: 0.6507958220200107 \n",
            "Iteration: 7387 ---- loss: 0.6507929034179766 \n",
            "Iteration: 7388 ---- loss: 0.6507899858247348 \n",
            "Iteration: 7389 ---- loss: 0.6507870690559431 \n",
            "Iteration: 7390 ---- loss: 0.6507841529893168 \n",
            "Iteration: 7391 ---- loss: 0.6507812379280107 \n",
            "Iteration: 7392 ---- loss: 0.6507783238715985 \n",
            "Iteration: 7393 ---- loss: 0.650775410814347 \n",
            "Iteration: 7394 ---- loss: 0.650772499056509 \n",
            "Iteration: 7395 ---- loss: 0.6507695938149917 \n",
            "Iteration: 7396 ---- loss: 0.6507666891925453 \n",
            "Iteration: 7397 ---- loss: 0.6507637855799441 \n",
            "Iteration: 7398 ---- loss: 0.6507608829955394 \n",
            "Iteration: 7399 ---- loss: 0.650757981486543 \n",
            "Iteration: 7401 ---- loss: 0.6507521817613289 \n",
            "Iteration: 7402 ---- loss: 0.6507492840439251 \n",
            "Iteration: 7403 ---- loss: 0.6507463873414625 \n",
            "Iteration: 7404 ---- loss: 0.6507434916959426 \n",
            "Iteration: 7405 ---- loss: 0.6507405970582022 \n",
            "Iteration: 7406 ---- loss: 0.6507377034274855 \n",
            "Iteration: 7407 ---- loss: 0.6507348107492021 \n",
            "Iteration: 7408 ---- loss: 0.6507319183993081 \n",
            "Iteration: 7409 ---- loss: 0.6507290267594344 \n",
            "Iteration: 7410 ---- loss: 0.6507261359028104 \n",
            "Iteration: 7411 ---- loss: 0.6507232460470249 \n",
            "Iteration: 7412 ---- loss: 0.6507203571913284 \n",
            "Iteration: 7413 ---- loss: 0.6507174689514748 \n",
            "Iteration: 7414 ---- loss: 0.6507145791117918 \n",
            "Iteration: 7415 ---- loss: 0.6507116904623674 \n",
            "Iteration: 7416 ---- loss: 0.6507088027749489 \n",
            "Iteration: 7417 ---- loss: 0.6507059160757404 \n",
            "Iteration: 7418 ---- loss: 0.6507030321323074 \n",
            "Iteration: 7419 ---- loss: 0.65070015585066 \n",
            "Iteration: 7420 ---- loss: 0.6506972805712389 \n",
            "Iteration: 7421 ---- loss: 0.6506944062932664 \n",
            "Iteration: 7422 ---- loss: 0.6506915329294264 \n",
            "Iteration: 7423 ---- loss: 0.6506886603362112 \n",
            "Iteration: 7424 ---- loss: 0.6506857887441904 \n",
            "Iteration: 7425 ---- loss: 0.6506829181842276 \n",
            "Iteration: 7426 ---- loss: 0.6506800486214976 \n",
            "Iteration: 7427 ---- loss: 0.6506771800552322 \n",
            "Iteration: 7428 ---- loss: 0.6506743124846645 \n",
            "Iteration: 7429 ---- loss: 0.6506714459128611 \n",
            "Iteration: 7430 ---- loss: 0.6506685806538408 \n",
            "Iteration: 7431 ---- loss: 0.6506657159013332 \n",
            "Iteration: 7432 ---- loss: 0.6506628521528294 \n",
            "Iteration: 7433 ---- loss: 0.6506599899363522 \n",
            "Iteration: 7434 ---- loss: 0.6506571293031952 \n",
            "Iteration: 7435 ---- loss: 0.6506542707236939 \n",
            "Iteration: 7436 ---- loss: 0.6506514138370838 \n",
            "Iteration: 7437 ---- loss: 0.6506485579452443 \n",
            "Iteration: 7438 ---- loss: 0.6506457034781825 \n",
            "Iteration: 7439 ---- loss: 0.6506428530693675 \n",
            "Iteration: 7440 ---- loss: 0.6506400036559985 \n",
            "Iteration: 7441 ---- loss: 0.650637155359421 \n",
            "Iteration: 7442 ---- loss: 0.650634308453997 \n",
            "Iteration: 7443 ---- loss: 0.6506314623304378 \n",
            "Iteration: 7444 ---- loss: 0.6506286161784643 \n",
            "Iteration: 7445 ---- loss: 0.6506257746452989 \n",
            "Iteration: 7446 ---- loss: 0.6506229348245537 \n",
            "Iteration: 7447 ---- loss: 0.6506200956607082 \n",
            "Iteration: 7448 ---- loss: 0.650617257513895 \n",
            "Iteration: 7449 ---- loss: 0.6506144203611597 \n",
            "Iteration: 7451 ---- loss: 0.650608750873237 \n",
            "Iteration: 7452 ---- loss: 0.6506059245318876 \n",
            "Iteration: 7453 ---- loss: 0.6506030965131034 \n",
            "Iteration: 7454 ---- loss: 0.6506002695260331 \n",
            "Iteration: 7455 ---- loss: 0.6505974431923245 \n",
            "Iteration: 7456 ---- loss: 0.6505946157616723 \n",
            "Iteration: 7457 ---- loss: 0.6505917893368796 \n",
            "Iteration: 7458 ---- loss: 0.6505889667148909 \n",
            "Iteration: 7459 ---- loss: 0.6505861494948962 \n",
            "Iteration: 7460 ---- loss: 0.6505833333147595 \n",
            "Iteration: 7461 ---- loss: 0.6505805199254483 \n",
            "Iteration: 7462 ---- loss: 0.6505777083351869 \n",
            "Iteration: 7463 ---- loss: 0.6505748975633167 \n",
            "Iteration: 7464 ---- loss: 0.6505720878226108 \n",
            "Iteration: 7465 ---- loss: 0.6505692790997327 \n",
            "Iteration: 7466 ---- loss: 0.6505664714350177 \n",
            "Iteration: 7467 ---- loss: 0.6505636647981254 \n",
            "Iteration: 7468 ---- loss: 0.6505608595488094 \n",
            "Iteration: 7469 ---- loss: 0.6505580564350248 \n",
            "Iteration: 7470 ---- loss: 0.6505552543439271 \n",
            "Iteration: 7471 ---- loss: 0.6505524534545303 \n",
            "Iteration: 7472 ---- loss: 0.6505496537647002 \n",
            "Iteration: 7473 ---- loss: 0.6505468550890814 \n",
            "Iteration: 7474 ---- loss: 0.6505440574348551 \n",
            "Iteration: 7475 ---- loss: 0.6505412607918277 \n",
            "Iteration: 7476 ---- loss: 0.6505384652064653 \n",
            "Iteration: 7477 ---- loss: 0.6505356706327684 \n",
            "Iteration: 7478 ---- loss: 0.6505328777506998 \n",
            "Iteration: 7479 ---- loss: 0.6505300868657875 \n",
            "Iteration: 7480 ---- loss: 0.6505272975320705 \n",
            "Iteration: 7481 ---- loss: 0.6505245092086607 \n",
            "Iteration: 7482 ---- loss: 0.6505217218946717 \n",
            "Iteration: 7483 ---- loss: 0.6505189355921381 \n",
            "Iteration: 7484 ---- loss: 0.6505161498070734 \n",
            "Iteration: 7485 ---- loss: 0.650513364796947 \n",
            "Iteration: 7486 ---- loss: 0.6505105808082222 \n",
            "Iteration: 7487 ---- loss: 0.6505077992361215 \n",
            "Iteration: 7488 ---- loss: 0.650505020012863 \n",
            "Iteration: 7489 ---- loss: 0.6505022418175187 \n",
            "Iteration: 7490 ---- loss: 0.6504994689348957 \n",
            "Iteration: 7491 ---- loss: 0.6504966998799521 \n",
            "Iteration: 7492 ---- loss: 0.6504939329591141 \n",
            "Iteration: 7493 ---- loss: 0.650491168634183 \n",
            "Iteration: 7494 ---- loss: 0.6504884053451457 \n",
            "Iteration: 7495 ---- loss: 0.6504856430171843 \n",
            "Iteration: 7496 ---- loss: 0.6504828814162468 \n",
            "Iteration: 7497 ---- loss: 0.6504801208469787 \n",
            "Iteration: 7498 ---- loss: 0.6504773617070985 \n",
            "Iteration: 7499 ---- loss: 0.650474604120485 \n",
            "Iteration: 7501 ---- loss: 0.6504690920393564 \n",
            "Iteration: 7502 ---- loss: 0.6504663375427604 \n",
            "Iteration: 7503 ---- loss: 0.6504635840740993 \n",
            "Iteration: 7504 ---- loss: 0.650460831632348 \n",
            "Iteration: 7505 ---- loss: 0.6504580802164871 \n",
            "Iteration: 7506 ---- loss: 0.6504553298255029 \n",
            "Iteration: 7507 ---- loss: 0.6504525804583867 \n",
            "Iteration: 7508 ---- loss: 0.6504498321141365 \n",
            "Iteration: 7509 ---- loss: 0.65044708706569 \n",
            "Iteration: 7510 ---- loss: 0.6504443452668036 \n",
            "Iteration: 7511 ---- loss: 0.6504416042899793 \n",
            "Iteration: 7512 ---- loss: 0.6504388648511651 \n",
            "Iteration: 7513 ---- loss: 0.650436127325506 \n",
            "Iteration: 7514 ---- loss: 0.6504333891958833 \n",
            "Iteration: 7515 ---- loss: 0.650430650997126 \n",
            "Iteration: 7516 ---- loss: 0.6504279145221732 \n",
            "Iteration: 7517 ---- loss: 0.65042517907806 \n",
            "Iteration: 7518 ---- loss: 0.6504224446637185 \n",
            "Iteration: 7519 ---- loss: 0.6504197112780877 \n",
            "Iteration: 7520 ---- loss: 0.6504169786657974 \n",
            "Iteration: 7521 ---- loss: 0.6504142466771831 \n",
            "Iteration: 7522 ---- loss: 0.6504115157118529 \n",
            "Iteration: 7523 ---- loss: 0.6504087857687803 \n",
            "Iteration: 7524 ---- loss: 0.6504060568469444 \n",
            "Iteration: 7525 ---- loss: 0.6504033284138788 \n",
            "Iteration: 7526 ---- loss: 0.6504005994954271 \n",
            "Iteration: 7527 ---- loss: 0.6503978716085076 \n",
            "Iteration: 7528 ---- loss: 0.6503951452342996 \n",
            "Iteration: 7529 ---- loss: 0.6503924203500715 \n",
            "Iteration: 7530 ---- loss: 0.6503896963592396 \n",
            "Iteration: 7531 ---- loss: 0.650386973220631 \n",
            "Iteration: 7532 ---- loss: 0.6503842510849652 \n",
            "Iteration: 7533 ---- loss: 0.6503815299572746 \n",
            "Iteration: 7534 ---- loss: 0.6503788095975399 \n",
            "Iteration: 7535 ---- loss: 0.6503760898114086 \n",
            "Iteration: 7536 ---- loss: 0.6503733710292247 \n",
            "Iteration: 7537 ---- loss: 0.6503706532500912 \n",
            "Iteration: 7538 ---- loss: 0.6503679364731155 \n",
            "Iteration: 7539 ---- loss: 0.650365220697407 \n",
            "Iteration: 7540 ---- loss: 0.6503625066909607 \n",
            "Iteration: 7541 ---- loss: 0.6503597964549623 \n",
            "Iteration: 7542 ---- loss: 0.6503570876302243 \n",
            "Iteration: 7543 ---- loss: 0.6503543816459509 \n",
            "Iteration: 7544 ---- loss: 0.6503516770202274 \n",
            "Iteration: 7545 ---- loss: 0.6503489734022467 \n",
            "Iteration: 7546 ---- loss: 0.6503462707540979 \n",
            "Iteration: 7547 ---- loss: 0.6503435689392385 \n",
            "Iteration: 7548 ---- loss: 0.6503408681289268 \n",
            "Iteration: 7549 ---- loss: 0.6503381683222565 \n",
            "Iteration: 7551 ---- loss: 0.6503327717162296 \n",
            "Iteration: 7552 ---- loss: 0.65033007490665 \n",
            "Iteration: 7553 ---- loss: 0.6503273797672883 \n",
            "Iteration: 7554 ---- loss: 0.6503246881247476 \n",
            "Iteration: 7555 ---- loss: 0.6503219974929524 \n",
            "Iteration: 7556 ---- loss: 0.6503193078700072 \n",
            "Iteration: 7557 ---- loss: 0.6503166191661013 \n",
            "Iteration: 7558 ---- loss: 0.6503139313569338 \n",
            "Iteration: 7559 ---- loss: 0.6503112445544152 \n",
            "Iteration: 7560 ---- loss: 0.6503085587575979 \n",
            "Iteration: 7561 ---- loss: 0.6503058739655374 \n",
            "Iteration: 7562 ---- loss: 0.6503031902818432 \n",
            "Iteration: 7563 ---- loss: 0.6503005081658411 \n",
            "Iteration: 7564 ---- loss: 0.6502978273770115 \n",
            "Iteration: 7565 ---- loss: 0.6502951481399188 \n",
            "Iteration: 7566 ---- loss: 0.6502924703655287 \n",
            "Iteration: 7567 ---- loss: 0.6502897940243819 \n",
            "Iteration: 7568 ---- loss: 0.6502871186912017 \n",
            "Iteration: 7569 ---- loss: 0.6502844443650332 \n",
            "Iteration: 7570 ---- loss: 0.6502817707689175 \n",
            "Iteration: 7571 ---- loss: 0.6502790974038886 \n",
            "Iteration: 7572 ---- loss: 0.6502764249207009 \n",
            "Iteration: 7573 ---- loss: 0.650273753176156 \n",
            "Iteration: 7574 ---- loss: 0.650271082431632 \n",
            "Iteration: 7575 ---- loss: 0.6502684126320302 \n",
            "Iteration: 7576 ---- loss: 0.6502657437310709 \n",
            "Iteration: 7577 ---- loss: 0.6502630758561744 \n",
            "Iteration: 7578 ---- loss: 0.6502604092845123 \n",
            "Iteration: 7579 ---- loss: 0.6502577460564061 \n",
            "Iteration: 7580 ---- loss: 0.6502550838357971 \n",
            "Iteration: 7581 ---- loss: 0.6502524226132332 \n",
            "Iteration: 7582 ---- loss: 0.6502497626595711 \n",
            "Iteration: 7583 ---- loss: 0.6502471036937695 \n",
            "Iteration: 7584 ---- loss: 0.6502444457257978 \n",
            "Iteration: 7585 ---- loss: 0.650241787835871 \n",
            "Iteration: 7586 ---- loss: 0.6502391299861715 \n",
            "Iteration: 7587 ---- loss: 0.6502364733124195 \n",
            "Iteration: 7588 ---- loss: 0.6502338176619687 \n",
            "Iteration: 7589 ---- loss: 0.6502311631486113 \n",
            "Iteration: 7590 ---- loss: 0.650228509993274 \n",
            "Iteration: 7591 ---- loss: 0.6502258578306962 \n",
            "Iteration: 7592 ---- loss: 0.6502232066565362 \n",
            "Iteration: 7593 ---- loss: 0.6502205565010487 \n",
            "Iteration: 7594 ---- loss: 0.6502179076665187 \n",
            "Iteration: 7595 ---- loss: 0.6502152605076863 \n",
            "Iteration: 7596 ---- loss: 0.6502126140865474 \n",
            "Iteration: 7597 ---- loss: 0.6502099667203 \n",
            "Iteration: 7598 ---- loss: 0.6502073203242497 \n",
            "Iteration: 7599 ---- loss: 0.6502046755277084 \n",
            "Iteration: 7601 ---- loss: 0.6501993911407015 \n",
            "Iteration: 7602 ---- loss: 0.6501967505973519 \n",
            "Iteration: 7603 ---- loss: 0.6501941117467207 \n",
            "Iteration: 7604 ---- loss: 0.6501914738899545 \n",
            "Iteration: 7605 ---- loss: 0.6501888370249572 \n",
            "Iteration: 7606 ---- loss: 0.6501862014248301 \n",
            "Iteration: 7607 ---- loss: 0.6501835674985108 \n",
            "Iteration: 7608 ---- loss: 0.6501809345751742 \n",
            "Iteration: 7609 ---- loss: 0.6501783018140801 \n",
            "Iteration: 7610 ---- loss: 0.6501756703257117 \n",
            "Iteration: 7611 ---- loss: 0.6501730421874617 \n",
            "Iteration: 7612 ---- loss: 0.6501704150458937 \n",
            "Iteration: 7613 ---- loss: 0.650167788875563 \n",
            "Iteration: 7614 ---- loss: 0.6501651636894846 \n",
            "Iteration: 7615 ---- loss: 0.6501625396526048 \n",
            "Iteration: 7616 ---- loss: 0.6501599171270925 \n",
            "Iteration: 7617 ---- loss: 0.6501572955703495 \n",
            "Iteration: 7618 ---- loss: 0.6501546749957763 \n",
            "Iteration: 7619 ---- loss: 0.6501520554557275 \n",
            "Iteration: 7620 ---- loss: 0.6501494369875401 \n",
            "Iteration: 7621 ---- loss: 0.6501468207928057 \n",
            "Iteration: 7622 ---- loss: 0.6501442074905547 \n",
            "Iteration: 7623 ---- loss: 0.6501415954464204 \n",
            "Iteration: 7624 ---- loss: 0.6501389843884428 \n",
            "Iteration: 7625 ---- loss: 0.6501363743156829 \n",
            "Iteration: 7626 ---- loss: 0.6501337652319402 \n",
            "Iteration: 7627 ---- loss: 0.6501311571311355 \n",
            "Iteration: 7628 ---- loss: 0.6501285500075741 \n",
            "Iteration: 7629 ---- loss: 0.6501259438655097 \n",
            "Iteration: 7630 ---- loss: 0.650123338704021 \n",
            "Iteration: 7631 ---- loss: 0.6501207345221891 \n",
            "Iteration: 7632 ---- loss: 0.6501181313190989 \n",
            "Iteration: 7633 ---- loss: 0.6501155294146842 \n",
            "Iteration: 7634 ---- loss: 0.6501129289429719 \n",
            "Iteration: 7635 ---- loss: 0.6501103285008423 \n",
            "Iteration: 7636 ---- loss: 0.6501077290364017 \n",
            "Iteration: 7637 ---- loss: 0.6501051305509867 \n",
            "Iteration: 7638 ---- loss: 0.6501025330375912 \n",
            "Iteration: 7639 ---- loss: 0.650099936495323 \n",
            "Iteration: 7640 ---- loss: 0.6500973409232925 \n",
            "Iteration: 7641 ---- loss: 0.650094747897799 \n",
            "Iteration: 7642 ---- loss: 0.6500921574409092 \n",
            "Iteration: 7643 ---- loss: 0.6500895679612319 \n",
            "Iteration: 7644 ---- loss: 0.6500869794578431 \n",
            "Iteration: 7645 ---- loss: 0.6500843919298227 \n",
            "Iteration: 7646 ---- loss: 0.6500818053762538 \n",
            "Iteration: 7647 ---- loss: 0.6500792197962222 \n",
            "Iteration: 7648 ---- loss: 0.6500766355595976 \n",
            "Iteration: 7649 ---- loss: 0.6500740536742317 \n",
            "Iteration: 7651 ---- loss: 0.6500688960580657 \n",
            "Iteration: 7652 ---- loss: 0.6500663192614794 \n",
            "Iteration: 7653 ---- loss: 0.6500637434403398 \n",
            "Iteration: 7654 ---- loss: 0.6500611685937162 \n",
            "Iteration: 7655 ---- loss: 0.6500585947206818 \n",
            "Iteration: 7656 ---- loss: 0.6500560218203121 \n",
            "Iteration: 7657 ---- loss: 0.6500534498916869 \n",
            "Iteration: 7658 ---- loss: 0.6500508789338881 \n",
            "Iteration: 7659 ---- loss: 0.6500483089460016 \n",
            "Iteration: 7660 ---- loss: 0.6500457400549072 \n",
            "Iteration: 7661 ---- loss: 0.6500431728374021 \n",
            "Iteration: 7662 ---- loss: 0.6500406064011445 \n",
            "Iteration: 7663 ---- loss: 0.650038041132028 \n",
            "Iteration: 7664 ---- loss: 0.6500354801253801 \n",
            "Iteration: 7665 ---- loss: 0.6500329200907259 \n",
            "Iteration: 7666 ---- loss: 0.6500303604487828 \n",
            "Iteration: 7667 ---- loss: 0.6500278004259651 \n",
            "Iteration: 7668 ---- loss: 0.6500252413673014 \n",
            "Iteration: 7669 ---- loss: 0.6500226832718963 \n",
            "Iteration: 7670 ---- loss: 0.6500201261388563 \n",
            "Iteration: 7671 ---- loss: 0.650017569967292 \n",
            "Iteration: 7672 ---- loss: 0.6500150147563155 \n",
            "Iteration: 7673 ---- loss: 0.6500124608477934 \n",
            "Iteration: 7674 ---- loss: 0.6500099094725569 \n",
            "Iteration: 7675 ---- loss: 0.6500073590610472 \n",
            "Iteration: 7676 ---- loss: 0.6500048096379354 \n",
            "Iteration: 7677 ---- loss: 0.6500022611733351 \n",
            "Iteration: 7678 ---- loss: 0.6499997136663607 \n",
            "Iteration: 7679 ---- loss: 0.6499971666664867 \n",
            "Iteration: 7680 ---- loss: 0.6499946195578348 \n",
            "Iteration: 7681 ---- loss: 0.6499920736345471 \n",
            "Iteration: 7682 ---- loss: 0.6499895337550304 \n",
            "Iteration: 7683 ---- loss: 0.6499869935051314 \n",
            "Iteration: 7684 ---- loss: 0.6499844542135081 \n",
            "Iteration: 7685 ---- loss: 0.6499819158801777 \n",
            "Iteration: 7686 ---- loss: 0.6499793785202982 \n",
            "Iteration: 7687 ---- loss: 0.6499768420990996 \n",
            "Iteration: 7688 ---- loss: 0.6499743066325911 \n",
            "Iteration: 7689 ---- loss: 0.6499717720744363 \n",
            "Iteration: 7690 ---- loss: 0.6499692379214707 \n",
            "Iteration: 7691 ---- loss: 0.6499667047233819 \n",
            "Iteration: 7692 ---- loss: 0.6499641724643653 \n",
            "Iteration: 7693 ---- loss: 0.6499616420177634 \n",
            "Iteration: 7694 ---- loss: 0.6499591165552325 \n",
            "Iteration: 7695 ---- loss: 0.6499565922529407 \n",
            "Iteration: 7696 ---- loss: 0.6499540691272417 \n",
            "Iteration: 7697 ---- loss: 0.6499515469617633 \n",
            "Iteration: 7698 ---- loss: 0.649949025767512 \n",
            "Iteration: 7699 ---- loss: 0.6499465055331919 \n",
            "Iteration: 7701 ---- loss: 0.6499414679133599 \n",
            "Iteration: 7702 ---- loss: 0.6499389505625921 \n",
            "Iteration: 7703 ---- loss: 0.6499364341382949 \n",
            "Iteration: 7704 ---- loss: 0.6499339186094527 \n",
            "Iteration: 7705 ---- loss: 0.6499314040846915 \n",
            "Iteration: 7706 ---- loss: 0.6499288905757694 \n",
            "Iteration: 7707 ---- loss: 0.649926378009435 \n",
            "Iteration: 7708 ---- loss: 0.6499238665691389 \n",
            "Iteration: 7709 ---- loss: 0.6499213564175793 \n",
            "Iteration: 7710 ---- loss: 0.6499188472016353 \n",
            "Iteration: 7711 ---- loss: 0.6499163395354389 \n",
            "Iteration: 7712 ---- loss: 0.6499138338798973 \n",
            "Iteration: 7713 ---- loss: 0.649911329562576 \n",
            "Iteration: 7714 ---- loss: 0.64990882613922 \n",
            "Iteration: 7715 ---- loss: 0.6499063236521176 \n",
            "Iteration: 7716 ---- loss: 0.6499038221093272 \n",
            "Iteration: 7717 ---- loss: 0.6499013239529986 \n",
            "Iteration: 7718 ---- loss: 0.6498988298983673 \n",
            "Iteration: 7719 ---- loss: 0.6498963368568857 \n",
            "Iteration: 7720 ---- loss: 0.6498938450359287 \n",
            "Iteration: 7721 ---- loss: 0.649891354153205 \n",
            "Iteration: 7722 ---- loss: 0.6498888642603738 \n",
            "Iteration: 7723 ---- loss: 0.6498863753084843 \n",
            "Iteration: 7724 ---- loss: 0.6498838874680439 \n",
            "Iteration: 7725 ---- loss: 0.6498814007188575 \n",
            "Iteration: 7726 ---- loss: 0.6498789153448091 \n",
            "Iteration: 7727 ---- loss: 0.6498764329804443 \n",
            "Iteration: 7728 ---- loss: 0.6498739515420139 \n",
            "Iteration: 7729 ---- loss: 0.6498714710947973 \n",
            "Iteration: 7730 ---- loss: 0.6498689915766726 \n",
            "Iteration: 7731 ---- loss: 0.6498665131234507 \n",
            "Iteration: 7732 ---- loss: 0.6498640358155792 \n",
            "Iteration: 7733 ---- loss: 0.649861559469865 \n",
            "Iteration: 7734 ---- loss: 0.6498590838113463 \n",
            "Iteration: 7735 ---- loss: 0.6498566092644369 \n",
            "Iteration: 7736 ---- loss: 0.6498541396122637 \n",
            "Iteration: 7737 ---- loss: 0.6498516709175226 \n",
            "Iteration: 7738 ---- loss: 0.6498492032103617 \n",
            "Iteration: 7739 ---- loss: 0.6498467364552415 \n",
            "Iteration: 7740 ---- loss: 0.6498442706870903 \n",
            "Iteration: 7741 ---- loss: 0.6498418058693083 \n",
            "Iteration: 7742 ---- loss: 0.6498393420340276 \n",
            "Iteration: 7743 ---- loss: 0.6498368799486149 \n",
            "Iteration: 7744 ---- loss: 0.6498344221533003 \n",
            "Iteration: 7745 ---- loss: 0.6498319653312874 \n",
            "Iteration: 7746 ---- loss: 0.6498295094854409 \n",
            "Iteration: 7747 ---- loss: 0.6498270546229129 \n",
            "Iteration: 7748 ---- loss: 0.6498246007202991 \n",
            "Iteration: 7749 ---- loss: 0.6498221478149321 \n",
            "Iteration: 7751 ---- loss: 0.64981724489695 \n",
            "Iteration: 7752 ---- loss: 0.649814794879518 \n",
            "Iteration: 7753 ---- loss: 0.6498123459648671 \n",
            "Iteration: 7754 ---- loss: 0.6498098989586844 \n",
            "Iteration: 7755 ---- loss: 0.6498074528004844 \n",
            "Iteration: 7756 ---- loss: 0.6498050076315894 \n",
            "Iteration: 7757 ---- loss: 0.6498025634005811 \n",
            "Iteration: 7758 ---- loss: 0.6498001201599467 \n",
            "Iteration: 7759 ---- loss: 0.6497976778693016 \n",
            "Iteration: 7760 ---- loss: 0.649795236536521 \n",
            "Iteration: 7761 ---- loss: 0.6497927961910842 \n",
            "Iteration: 7762 ---- loss: 0.6497903567781296 \n",
            "Iteration: 7763 ---- loss: 0.6497879183404299 \n",
            "Iteration: 7764 ---- loss: 0.6497854808654209 \n",
            "Iteration: 7765 ---- loss: 0.6497830443314746 \n",
            "Iteration: 7766 ---- loss: 0.6497806087755325 \n",
            "Iteration: 7767 ---- loss: 0.6497781741664242 \n",
            "Iteration: 7768 ---- loss: 0.6497757405132585 \n",
            "Iteration: 7769 ---- loss: 0.6497733080244723 \n",
            "Iteration: 7770 ---- loss: 0.6497708766433399 \n",
            "Iteration: 7771 ---- loss: 0.6497684462006176 \n",
            "Iteration: 7772 ---- loss: 0.649766016779 \n",
            "Iteration: 7773 ---- loss: 0.6497635882679181 \n",
            "Iteration: 7774 ---- loss: 0.649761160687393 \n",
            "Iteration: 7775 ---- loss: 0.6497587341149712 \n",
            "Iteration: 7776 ---- loss: 0.6497563085437189 \n",
            "Iteration: 7777 ---- loss: 0.6497538838813749 \n",
            "Iteration: 7778 ---- loss: 0.6497514620862856 \n",
            "Iteration: 7779 ---- loss: 0.6497490435483574 \n",
            "Iteration: 7780 ---- loss: 0.6497466258985223 \n",
            "Iteration: 7781 ---- loss: 0.6497442092062167 \n",
            "Iteration: 7782 ---- loss: 0.6497417934987343 \n",
            "Iteration: 7783 ---- loss: 0.6497393787707441 \n",
            "Iteration: 7784 ---- loss: 0.6497369665232854 \n",
            "Iteration: 7785 ---- loss: 0.6497345571961125 \n",
            "Iteration: 7786 ---- loss: 0.6497321488601752 \n",
            "Iteration: 7787 ---- loss: 0.6497297415148143 \n",
            "Iteration: 7788 ---- loss: 0.6497273350918382 \n",
            "Iteration: 7789 ---- loss: 0.6497249296358825 \n",
            "Iteration: 7790 ---- loss: 0.6497225251459143 \n",
            "Iteration: 7791 ---- loss: 0.6497201208024648 \n",
            "Iteration: 7792 ---- loss: 0.6497177151691627 \n",
            "Iteration: 7793 ---- loss: 0.6497153104784931 \n",
            "Iteration: 7794 ---- loss: 0.6497129067369039 \n",
            "Iteration: 7795 ---- loss: 0.6497105039434533 \n",
            "Iteration: 7796 ---- loss: 0.6497081021739304 \n",
            "Iteration: 7797 ---- loss: 0.6497057015582781 \n",
            "Iteration: 7798 ---- loss: 0.6497033018802616 \n",
            "Iteration: 7799 ---- loss: 0.6497009031468782 \n",
            "Iteration: 7801 ---- loss: 0.649696109294822 \n",
            "Iteration: 7802 ---- loss: 0.6496937170870707 \n",
            "Iteration: 7803 ---- loss: 0.6496913257944185 \n",
            "Iteration: 7804 ---- loss: 0.6496889354687795 \n",
            "Iteration: 7805 ---- loss: 0.6496865461689426 \n",
            "Iteration: 7806 ---- loss: 0.6496841580563996 \n",
            "Iteration: 7807 ---- loss: 0.6496817713595564 \n",
            "Iteration: 7808 ---- loss: 0.64967938552443 \n",
            "Iteration: 7809 ---- loss: 0.6496770012155687 \n",
            "Iteration: 7810 ---- loss: 0.6496746192059752 \n",
            "Iteration: 7811 ---- loss: 0.6496722383273735 \n",
            "Iteration: 7812 ---- loss: 0.6496698586363522 \n",
            "Iteration: 7813 ---- loss: 0.6496674799073395 \n",
            "Iteration: 7814 ---- loss: 0.6496651021392935 \n",
            "Iteration: 7815 ---- loss: 0.6496627253311774 \n",
            "Iteration: 7816 ---- loss: 0.6496603494170423 \n",
            "Iteration: 7817 ---- loss: 0.6496579743140638 \n",
            "Iteration: 7818 ---- loss: 0.6496556001676916 \n",
            "Iteration: 7819 ---- loss: 0.6496532268592807 \n",
            "Iteration: 7820 ---- loss: 0.6496508539226312 \n",
            "Iteration: 7821 ---- loss: 0.6496484819387237 \n",
            "Iteration: 7822 ---- loss: 0.6496461109065533 \n",
            "Iteration: 7823 ---- loss: 0.6496437408273728 \n",
            "Iteration: 7824 ---- loss: 0.6496413716981533 \n",
            "Iteration: 7825 ---- loss: 0.6496390035141841 \n",
            "Iteration: 7826 ---- loss: 0.6496366362759227 \n",
            "Iteration: 7827 ---- loss: 0.6496342699844415 \n",
            "Iteration: 7828 ---- loss: 0.6496319046387613 \n",
            "Iteration: 7829 ---- loss: 0.6496295402749681 \n",
            "Iteration: 7830 ---- loss: 0.6496271771337779 \n",
            "Iteration: 7831 ---- loss: 0.6496248149357052 \n",
            "Iteration: 7832 ---- loss: 0.6496224536797862 \n",
            "Iteration: 7833 ---- loss: 0.6496200933650607 \n",
            "Iteration: 7834 ---- loss: 0.6496177339905723 \n",
            "Iteration: 7835 ---- loss: 0.6496153755553686 \n",
            "Iteration: 7836 ---- loss: 0.6496130172084994 \n",
            "Iteration: 7837 ---- loss: 0.6496106581173936 \n",
            "Iteration: 7838 ---- loss: 0.6496082999583159 \n",
            "Iteration: 7839 ---- loss: 0.649605942702499 \n",
            "Iteration: 7840 ---- loss: 0.649603586377289 \n",
            "Iteration: 7841 ---- loss: 0.6496012309817698 \n",
            "Iteration: 7842 ---- loss: 0.6495988764944537 \n",
            "Iteration: 7843 ---- loss: 0.6495965229112852 \n",
            "Iteration: 7844 ---- loss: 0.6495941700721133 \n",
            "Iteration: 7845 ---- loss: 0.6495918170341333 \n",
            "Iteration: 7846 ---- loss: 0.649589464868626 \n",
            "Iteration: 7847 ---- loss: 0.6495871131356645 \n",
            "Iteration: 7848 ---- loss: 0.6495847625126686 \n",
            "Iteration: 7849 ---- loss: 0.649582413922079 \n",
            "Iteration: 7851 ---- loss: 0.6495777233213269 \n",
            "Iteration: 7852 ---- loss: 0.6495753796302177 \n",
            "Iteration: 7853 ---- loss: 0.6495730372002259 \n",
            "Iteration: 7854 ---- loss: 0.6495706956927174 \n",
            "Iteration: 7855 ---- loss: 0.6495683551067758 \n",
            "Iteration: 7856 ---- loss: 0.6495660154414861 \n",
            "Iteration: 7857 ---- loss: 0.6495636767013063 \n",
            "Iteration: 7858 ---- loss: 0.6495613388771516 \n",
            "Iteration: 7859 ---- loss: 0.6495590019663254 \n",
            "Iteration: 7860 ---- loss: 0.6495566659684285 \n",
            "Iteration: 7861 ---- loss: 0.6495543311228565 \n",
            "Iteration: 7862 ---- loss: 0.6495519975550039 \n",
            "Iteration: 7863 ---- loss: 0.6495496649037698 \n",
            "Iteration: 7864 ---- loss: 0.6495473331725373 \n",
            "Iteration: 7865 ---- loss: 0.6495450023471516 \n",
            "Iteration: 7866 ---- loss: 0.6495426724248508 \n",
            "Iteration: 7867 ---- loss: 0.6495403433305019 \n",
            "Iteration: 7868 ---- loss: 0.6495380151400161 \n",
            "Iteration: 7869 ---- loss: 0.6495356878664332 \n",
            "Iteration: 7870 ---- loss: 0.6495333614970061 \n",
            "Iteration: 7871 ---- loss: 0.6495310360349805 \n",
            "Iteration: 7872 ---- loss: 0.6495287114855554 \n",
            "Iteration: 7873 ---- loss: 0.6495263878782714 \n",
            "Iteration: 7874 ---- loss: 0.6495240669167492 \n",
            "Iteration: 7875 ---- loss: 0.6495217432779807 \n",
            "Iteration: 7876 ---- loss: 0.6495194225657623 \n",
            "Iteration: 7877 ---- loss: 0.6495171027653613 \n",
            "Iteration: 7878 ---- loss: 0.6495147838842911 \n",
            "Iteration: 7879 ---- loss: 0.6495124659159721 \n",
            "Iteration: 7880 ---- loss: 0.6495101488633738 \n",
            "Iteration: 7881 ---- loss: 0.6495078327223682 \n",
            "Iteration: 7882 ---- loss: 0.6495055174955902 \n",
            "Iteration: 7883 ---- loss: 0.6495032031631772 \n",
            "Iteration: 7884 ---- loss: 0.6495008896559579 \n",
            "Iteration: 7885 ---- loss: 0.6494985776164012 \n",
            "Iteration: 7886 ---- loss: 0.6494962721376539 \n",
            "Iteration: 7887 ---- loss: 0.6494939675821237 \n",
            "Iteration: 7888 ---- loss: 0.649491664077358 \n",
            "Iteration: 7889 ---- loss: 0.6494893623599536 \n",
            "Iteration: 7890 ---- loss: 0.6494870616273464 \n",
            "Iteration: 7891 ---- loss: 0.6494847620852837 \n",
            "Iteration: 7892 ---- loss: 0.6494824634673222 \n",
            "Iteration: 7893 ---- loss: 0.6494801657866784 \n",
            "Iteration: 7894 ---- loss: 0.6494778690292198 \n",
            "Iteration: 7895 ---- loss: 0.6494755731972139 \n",
            "Iteration: 7896 ---- loss: 0.6494732783007338 \n",
            "Iteration: 7897 ---- loss: 0.6494709843211165 \n",
            "Iteration: 7898 ---- loss: 0.6494686912659795 \n",
            "Iteration: 7899 ---- loss: 0.6494663999375656 \n",
            "Iteration: 7901 ---- loss: 0.6494618239591057 \n",
            "Iteration: 7902 ---- loss: 0.6494595373597368 \n",
            "Iteration: 7903 ---- loss: 0.649457251681714 \n",
            "Iteration: 7904 ---- loss: 0.6494549669220658 \n",
            "Iteration: 7905 ---- loss: 0.64945268308312 \n",
            "Iteration: 7906 ---- loss: 0.6494504003948506 \n",
            "Iteration: 7907 ---- loss: 0.6494481241270419 \n",
            "Iteration: 7908 ---- loss: 0.6494458484884101 \n",
            "Iteration: 7909 ---- loss: 0.6494435736975194 \n",
            "Iteration: 7910 ---- loss: 0.6494412998792888 \n",
            "Iteration: 7911 ---- loss: 0.6494390282087915 \n",
            "Iteration: 7912 ---- loss: 0.6494367587985355 \n",
            "Iteration: 7913 ---- loss: 0.649434490857623 \n",
            "Iteration: 7914 ---- loss: 0.6494322238502999 \n",
            "Iteration: 7915 ---- loss: 0.6494299577754863 \n",
            "Iteration: 7916 ---- loss: 0.649427692634521 \n",
            "Iteration: 7917 ---- loss: 0.6494254284733968 \n",
            "Iteration: 7918 ---- loss: 0.649423165234787 \n",
            "Iteration: 7919 ---- loss: 0.6494209029254975 \n",
            "Iteration: 7920 ---- loss: 0.649418641489364 \n",
            "Iteration: 7921 ---- loss: 0.6494163809046001 \n",
            "Iteration: 7922 ---- loss: 0.6494141212469945 \n",
            "Iteration: 7923 ---- loss: 0.6494118623979549 \n",
            "Iteration: 7924 ---- loss: 0.6494096034366498 \n",
            "Iteration: 7925 ---- loss: 0.6494073453973076 \n",
            "Iteration: 7926 ---- loss: 0.6494050882789145 \n",
            "Iteration: 7927 ---- loss: 0.649402832080461 \n",
            "Iteration: 7928 ---- loss: 0.6494005768009414 \n",
            "Iteration: 7929 ---- loss: 0.6493983224395103 \n",
            "Iteration: 7930 ---- loss: 0.649396068994575 \n",
            "Iteration: 7931 ---- loss: 0.6493938164653466 \n",
            "Iteration: 7932 ---- loss: 0.6493915648848972 \n",
            "Iteration: 7933 ---- loss: 0.6493893144319525 \n",
            "Iteration: 7934 ---- loss: 0.6493870648921323 \n",
            "Iteration: 7935 ---- loss: 0.6493848162644601 \n",
            "Iteration: 7936 ---- loss: 0.6493825685479628 \n",
            "Iteration: 7937 ---- loss: 0.6493803217477588 \n",
            "Iteration: 7938 ---- loss: 0.649378075867782 \n",
            "Iteration: 7939 ---- loss: 0.6493758308960941 \n",
            "Iteration: 7940 ---- loss: 0.649373585897092 \n",
            "Iteration: 7941 ---- loss: 0.6493713400876922 \n",
            "Iteration: 7942 ---- loss: 0.6493690951793135 \n",
            "Iteration: 7943 ---- loss: 0.6493668506765646 \n",
            "Iteration: 7944 ---- loss: 0.6493646064887096 \n",
            "Iteration: 7945 ---- loss: 0.6493623631958475 \n",
            "Iteration: 7946 ---- loss: 0.649360120797066 \n",
            "Iteration: 7947 ---- loss: 0.6493578792914549 \n",
            "Iteration: 7948 ---- loss: 0.6493556386781072 \n",
            "Iteration: 7949 ---- loss: 0.6493533989561183 \n",
            "Iteration: 7951 ---- loss: 0.6493489221826108 \n",
            "Iteration: 7952 ---- loss: 0.6493466851292949 \n",
            "Iteration: 7953 ---- loss: 0.649344448963744 \n",
            "Iteration: 7954 ---- loss: 0.6493422136850654 \n",
            "Iteration: 7955 ---- loss: 0.649339979657895 \n",
            "Iteration: 7956 ---- loss: 0.649337748613971 \n",
            "Iteration: 7957 ---- loss: 0.6493355184571182 \n",
            "Iteration: 7958 ---- loss: 0.649333289186439 \n",
            "Iteration: 7959 ---- loss: 0.6493310608010385 \n",
            "Iteration: 7960 ---- loss: 0.6493288330663516 \n",
            "Iteration: 7961 ---- loss: 0.649326604713083 \n",
            "Iteration: 7962 ---- loss: 0.6493243769870145 \n",
            "Iteration: 7963 ---- loss: 0.6493221497255227 \n",
            "Iteration: 7964 ---- loss: 0.6493199232688776 \n",
            "Iteration: 7965 ---- loss: 0.6493176989974763 \n",
            "Iteration: 7966 ---- loss: 0.6493154773125303 \n",
            "Iteration: 7967 ---- loss: 0.6493132564052312 \n",
            "Iteration: 7968 ---- loss: 0.6493110362915318 \n",
            "Iteration: 7969 ---- loss: 0.6493088170572379 \n",
            "Iteration: 7970 ---- loss: 0.6493065987014511 \n",
            "Iteration: 7971 ---- loss: 0.6493043812152881 \n",
            "Iteration: 7972 ---- loss: 0.6493021644051313 \n",
            "Iteration: 7973 ---- loss: 0.6492999484704916 \n",
            "Iteration: 7974 ---- loss: 0.64929773341048 \n",
            "Iteration: 7975 ---- loss: 0.6492955192418267 \n",
            "Iteration: 7976 ---- loss: 0.6492933060964193 \n",
            "Iteration: 7977 ---- loss: 0.649291093824944 \n",
            "Iteration: 7978 ---- loss: 0.6492888824265116 \n",
            "Iteration: 7979 ---- loss: 0.6492866719595795 \n",
            "Iteration: 7980 ---- loss: 0.6492844624849131 \n",
            "Iteration: 7981 ---- loss: 0.6492822538566377 \n",
            "Iteration: 7982 ---- loss: 0.6492800460522364 \n",
            "Iteration: 7983 ---- loss: 0.6492778391163049 \n",
            "Iteration: 7984 ---- loss: 0.6492756330479666 \n",
            "Iteration: 7985 ---- loss: 0.6492734278463468 \n",
            "Iteration: 7986 ---- loss: 0.6492712235105735 \n",
            "Iteration: 7987 ---- loss: 0.6492690201656691 \n",
            "Iteration: 7988 ---- loss: 0.6492668180047785 \n",
            "Iteration: 7989 ---- loss: 0.6492646167124451 \n",
            "Iteration: 7990 ---- loss: 0.6492624162833425 \n",
            "Iteration: 7991 ---- loss: 0.6492602167166067 \n",
            "Iteration: 7992 ---- loss: 0.6492580180113766 \n",
            "Iteration: 7993 ---- loss: 0.6492558201667922 \n",
            "Iteration: 7994 ---- loss: 0.6492536231819961 \n",
            "Iteration: 7995 ---- loss: 0.6492514267582953 \n",
            "Iteration: 7996 ---- loss: 0.649249229667937 \n",
            "Iteration: 7997 ---- loss: 0.6492470336552828 \n",
            "Iteration: 7998 ---- loss: 0.6492448394236412 \n",
            "Iteration: 7999 ---- loss: 0.6492426460446452 \n",
            "Iteration: 8001 ---- loss: 0.6492382618413072 \n",
            "Iteration: 8002 ---- loss: 0.6492360710842238 \n",
            "Iteration: 8003 ---- loss: 0.649233881276993 \n",
            "Iteration: 8004 ---- loss: 0.6492316923199499 \n",
            "Iteration: 8005 ---- loss: 0.6492295049002021 \n",
            "Iteration: 8006 ---- loss: 0.6492273191459721 \n",
            "Iteration: 8007 ---- loss: 0.6492251343270844 \n",
            "Iteration: 8008 ---- loss: 0.6492229519404591 \n",
            "Iteration: 8009 ---- loss: 0.64922077120208 \n",
            "Iteration: 8010 ---- loss: 0.6492185909477699 \n",
            "Iteration: 8011 ---- loss: 0.6492164115663173 \n",
            "Iteration: 8012 ---- loss: 0.6492142330264749 \n",
            "Iteration: 8013 ---- loss: 0.6492120553673985 \n",
            "Iteration: 8014 ---- loss: 0.6492098785429262 \n",
            "Iteration: 8015 ---- loss: 0.6492077025929179 \n",
            "Iteration: 8016 ---- loss: 0.649205527487008 \n",
            "Iteration: 8017 ---- loss: 0.6492033532391666 \n",
            "Iteration: 8018 ---- loss: 0.6492011796945837 \n",
            "Iteration: 8019 ---- loss: 0.6491990059590156 \n",
            "Iteration: 8020 ---- loss: 0.6491968338745853 \n",
            "Iteration: 8021 ---- loss: 0.6491946626248081 \n",
            "Iteration: 8022 ---- loss: 0.6491924922323555 \n",
            "Iteration: 8023 ---- loss: 0.6491903226970653 \n",
            "Iteration: 8024 ---- loss: 0.6491881537616799 \n",
            "Iteration: 8025 ---- loss: 0.6491859852214236 \n",
            "Iteration: 8026 ---- loss: 0.6491838188546788 \n",
            "Iteration: 8027 ---- loss: 0.6491816552031588 \n",
            "Iteration: 8028 ---- loss: 0.6491794923993846 \n",
            "Iteration: 8029 ---- loss: 0.6491773304119627 \n",
            "Iteration: 8030 ---- loss: 0.6491751692692093 \n",
            "Iteration: 8031 ---- loss: 0.6491730089876737 \n",
            "Iteration: 8032 ---- loss: 0.6491708495415276 \n",
            "Iteration: 8033 ---- loss: 0.6491686909376019 \n",
            "Iteration: 8034 ---- loss: 0.6491665324995856 \n",
            "Iteration: 8035 ---- loss: 0.6491643739322542 \n",
            "Iteration: 8036 ---- loss: 0.6491622161990194 \n",
            "Iteration: 8037 ---- loss: 0.649160059307777 \n",
            "Iteration: 8038 ---- loss: 0.6491579032656293 \n",
            "Iteration: 8039 ---- loss: 0.6491557480486874 \n",
            "Iteration: 8040 ---- loss: 0.6491535937788508 \n",
            "Iteration: 8041 ---- loss: 0.6491514411178324 \n",
            "Iteration: 8042 ---- loss: 0.6491492932730237 \n",
            "Iteration: 8043 ---- loss: 0.6491471463220526 \n",
            "Iteration: 8044 ---- loss: 0.649145000435473 \n",
            "Iteration: 8045 ---- loss: 0.6491428557560064 \n",
            "Iteration: 8046 ---- loss: 0.6491407119818283 \n",
            "Iteration: 8047 ---- loss: 0.649138569120354 \n",
            "Iteration: 8048 ---- loss: 0.6491364270982133 \n",
            "Iteration: 8049 ---- loss: 0.6491342857611493 \n",
            "Iteration: 8051 ---- loss: 0.6491300040406779 \n",
            "Iteration: 8052 ---- loss: 0.6491278644258317 \n",
            "Iteration: 8053 ---- loss: 0.6491257256438451 \n",
            "Iteration: 8054 ---- loss: 0.6491235876938821 \n",
            "Iteration: 8055 ---- loss: 0.6491214505751084 \n",
            "Iteration: 8056 ---- loss: 0.6491193142922208 \n",
            "Iteration: 8057 ---- loss: 0.6491171788421497 \n",
            "Iteration: 8058 ---- loss: 0.6491150453185547 \n",
            "Iteration: 8059 ---- loss: 0.6491129140969069 \n",
            "Iteration: 8060 ---- loss: 0.6491107837053116 \n",
            "Iteration: 8061 ---- loss: 0.6491086541429332 \n",
            "Iteration: 8062 ---- loss: 0.6491065254089384 \n",
            "Iteration: 8063 ---- loss: 0.6491043975024956 \n",
            "Iteration: 8064 ---- loss: 0.6491022704279589 \n",
            "Iteration: 8065 ---- loss: 0.6491001441718319 \n",
            "Iteration: 8066 ---- loss: 0.6490980190417904 \n",
            "Iteration: 8067 ---- loss: 0.6490958984773414 \n",
            "Iteration: 8068 ---- loss: 0.649093778741706 \n",
            "Iteration: 8069 ---- loss: 0.6490916598604437 \n",
            "Iteration: 8070 ---- loss: 0.6490895419025048 \n",
            "Iteration: 8071 ---- loss: 0.64908742477098 \n",
            "Iteration: 8072 ---- loss: 0.6490853084650318 \n",
            "Iteration: 8073 ---- loss: 0.649083192983824 \n",
            "Iteration: 8074 ---- loss: 0.6490810783265223 \n",
            "Iteration: 8075 ---- loss: 0.6490789644363418 \n",
            "Iteration: 8076 ---- loss: 0.6490768505528528 \n",
            "Iteration: 8077 ---- loss: 0.6490747345440434 \n",
            "Iteration: 8078 ---- loss: 0.6490726189853164 \n",
            "Iteration: 8079 ---- loss: 0.6490705042259374 \n",
            "Iteration: 8080 ---- loss: 0.6490683903750781 \n",
            "Iteration: 8081 ---- loss: 0.6490662785008279 \n",
            "Iteration: 8082 ---- loss: 0.6490641675211372 \n",
            "Iteration: 8083 ---- loss: 0.6490620586139579 \n",
            "Iteration: 8084 ---- loss: 0.6490599505082915 \n",
            "Iteration: 8085 ---- loss: 0.6490578439734498 \n",
            "Iteration: 8086 ---- loss: 0.6490557411757241 \n",
            "Iteration: 8087 ---- loss: 0.6490536391822195 \n",
            "Iteration: 8088 ---- loss: 0.6490515379921424 \n",
            "Iteration: 8089 ---- loss: 0.6490494376047008 \n",
            "Iteration: 8090 ---- loss: 0.649047338019103 \n",
            "Iteration: 8091 ---- loss: 0.6490452392345601 \n",
            "Iteration: 8092 ---- loss: 0.6490431412502833 \n",
            "Iteration: 8093 ---- loss: 0.6490410438675267 \n",
            "Iteration: 8094 ---- loss: 0.6490389468587637 \n",
            "Iteration: 8095 ---- loss: 0.649036850647149 \n",
            "Iteration: 8096 ---- loss: 0.649034756425656 \n",
            "Iteration: 8097 ---- loss: 0.6490326656797396 \n",
            "Iteration: 8098 ---- loss: 0.6490305773369732 \n",
            "Iteration: 8099 ---- loss: 0.6490284897986626 \n",
            "Iteration: 8101 ---- loss: 0.6490243175496989 \n",
            "Iteration: 8102 ---- loss: 0.649022232489638 \n",
            "Iteration: 8103 ---- loss: 0.6490201481571352 \n",
            "Iteration: 8104 ---- loss: 0.6490180645388356 \n",
            "Iteration: 8105 ---- loss: 0.6490159815671092 \n",
            "Iteration: 8106 ---- loss: 0.6490138996832733 \n",
            "Iteration: 8107 ---- loss: 0.6490118191942834 \n",
            "Iteration: 8108 ---- loss: 0.649009739508234 \n",
            "Iteration: 8109 ---- loss: 0.6490076606243016 \n",
            "Iteration: 8110 ---- loss: 0.6490055825416643 \n",
            "Iteration: 8111 ---- loss: 0.6490035052595026 \n",
            "Iteration: 8112 ---- loss: 0.649001428776999 \n",
            "Iteration: 8113 ---- loss: 0.6489993530933377 \n",
            "Iteration: 8114 ---- loss: 0.6489972781976985 \n",
            "Iteration: 8115 ---- loss: 0.6489952041550638 \n",
            "Iteration: 8116 ---- loss: 0.6489931311133182 \n",
            "Iteration: 8117 ---- loss: 0.6489910588982609 \n",
            "Iteration: 8118 ---- loss: 0.6489889875423183 \n",
            "Iteration: 8119 ---- loss: 0.6489869177155876 \n",
            "Iteration: 8120 ---- loss: 0.6489848501114001 \n",
            "Iteration: 8121 ---- loss: 0.6489827833023136 \n",
            "Iteration: 8122 ---- loss: 0.6489807172875124 \n",
            "Iteration: 8123 ---- loss: 0.6489786521247991 \n",
            "Iteration: 8124 ---- loss: 0.6489765878553975 \n",
            "Iteration: 8125 ---- loss: 0.6489745243781804 \n",
            "Iteration: 8126 ---- loss: 0.648972461596138 \n",
            "Iteration: 8127 ---- loss: 0.6489703994003215 \n",
            "Iteration: 8128 ---- loss: 0.648968337995087 \n",
            "Iteration: 8129 ---- loss: 0.64896627737963 \n",
            "Iteration: 8130 ---- loss: 0.6489642175531474 \n",
            "Iteration: 8131 ---- loss: 0.648962158668585 \n",
            "Iteration: 8132 ---- loss: 0.648960102120524 \n",
            "Iteration: 8133 ---- loss: 0.6489580465189114 \n",
            "Iteration: 8134 ---- loss: 0.6489559917087837 \n",
            "Iteration: 8135 ---- loss: 0.648953937552088 \n",
            "Iteration: 8136 ---- loss: 0.6489518837979655 \n",
            "Iteration: 8137 ---- loss: 0.6489498307156655 \n",
            "Iteration: 8138 ---- loss: 0.6489477780734587 \n",
            "Iteration: 8139 ---- loss: 0.6489457262154893 \n",
            "Iteration: 8140 ---- loss: 0.6489436751409629 \n",
            "Iteration: 8141 ---- loss: 0.6489416248260391 \n",
            "Iteration: 8142 ---- loss: 0.6489395751992402 \n",
            "Iteration: 8143 ---- loss: 0.6489375263538245 \n",
            "Iteration: 8144 ---- loss: 0.6489354783984723 \n",
            "Iteration: 8145 ---- loss: 0.6489334317183018 \n",
            "Iteration: 8146 ---- loss: 0.6489313858168456 \n",
            "Iteration: 8147 ---- loss: 0.6489293406933258 \n",
            "Iteration: 8148 ---- loss: 0.6489272963469644 \n",
            "Iteration: 8149 ---- loss: 0.648925252776986 \n",
            "Iteration: 8151 ---- loss: 0.6489211685072155 \n",
            "Iteration: 8152 ---- loss: 0.6489191319502723 \n",
            "Iteration: 8153 ---- loss: 0.6489170961840233 \n",
            "Iteration: 8154 ---- loss: 0.6489150612076231 \n",
            "Iteration: 8155 ---- loss: 0.6489130270202282 \n",
            "Iteration: 8156 ---- loss: 0.6489109935630754 \n",
            "Iteration: 8157 ---- loss: 0.6489089604357888 \n",
            "Iteration: 8158 ---- loss: 0.6489069280945351 \n",
            "Iteration: 8159 ---- loss: 0.6489048965384806 \n",
            "Iteration: 8160 ---- loss: 0.6489028654127147 \n",
            "Iteration: 8161 ---- loss: 0.6489008322505401 \n",
            "Iteration: 8162 ---- loss: 0.6488987999817798 \n",
            "Iteration: 8163 ---- loss: 0.6488967685649534 \n",
            "Iteration: 8164 ---- loss: 0.6488947379270353 \n",
            "Iteration: 8165 ---- loss: 0.648892708058002 \n",
            "Iteration: 8166 ---- loss: 0.6488906797849425 \n",
            "Iteration: 8167 ---- loss: 0.648888652919913 \n",
            "Iteration: 8168 ---- loss: 0.6488866269017945 \n",
            "Iteration: 8169 ---- loss: 0.6488846019675812 \n",
            "Iteration: 8170 ---- loss: 0.648882578184374 \n",
            "Iteration: 8171 ---- loss: 0.6488805551876887 \n",
            "Iteration: 8172 ---- loss: 0.648878532957858 \n",
            "Iteration: 8173 ---- loss: 0.648876514715274 \n",
            "Iteration: 8174 ---- loss: 0.6488745016846297 \n",
            "Iteration: 8175 ---- loss: 0.6488724894368659 \n",
            "Iteration: 8176 ---- loss: 0.6488704779726335 \n",
            "Iteration: 8177 ---- loss: 0.6488684667816638 \n",
            "Iteration: 8178 ---- loss: 0.6488664560358103 \n",
            "Iteration: 8179 ---- loss: 0.648864447297364 \n",
            "Iteration: 8180 ---- loss: 0.6488624393489115 \n",
            "Iteration: 8181 ---- loss: 0.6488604321759738 \n",
            "Iteration: 8182 ---- loss: 0.6488584267163765 \n",
            "Iteration: 8183 ---- loss: 0.6488564250336293 \n",
            "Iteration: 8184 ---- loss: 0.6488544241597081 \n",
            "Iteration: 8185 ---- loss: 0.6488524240886157 \n",
            "Iteration: 8186 ---- loss: 0.648850424689125 \n",
            "Iteration: 8187 ---- loss: 0.6488484258034662 \n",
            "Iteration: 8188 ---- loss: 0.6488464276994365 \n",
            "Iteration: 8189 ---- loss: 0.6488444304069257 \n",
            "Iteration: 8190 ---- loss: 0.6488424338958309 \n",
            "Iteration: 8191 ---- loss: 0.6488404382319154 \n",
            "Iteration: 8192 ---- loss: 0.6488384439171163 \n",
            "Iteration: 8193 ---- loss: 0.6488364504062785 \n",
            "Iteration: 8194 ---- loss: 0.6488344577124955 \n",
            "Iteration: 8195 ---- loss: 0.6488324669977696 \n",
            "Iteration: 8196 ---- loss: 0.6488304770697761 \n",
            "Iteration: 8197 ---- loss: 0.6488284879452891 \n",
            "Iteration: 8198 ---- loss: 0.6488264997787643 \n",
            "Iteration: 8199 ---- loss: 0.6488245131439019 \n",
            "Iteration: 8201 ---- loss: 0.6488205411391381 \n",
            "Iteration: 8202 ---- loss: 0.6488185563813155 \n",
            "Iteration: 8203 ---- loss: 0.6488165724286208 \n",
            "Iteration: 8204 ---- loss: 0.6488145892587261 \n",
            "Iteration: 8205 ---- loss: 0.648812606881861 \n",
            "Iteration: 8206 ---- loss: 0.6488106252976182 \n",
            "Iteration: 8207 ---- loss: 0.6488086444927001 \n",
            "Iteration: 8208 ---- loss: 0.648806664491014 \n",
            "Iteration: 8209 ---- loss: 0.6488046852700469 \n",
            "Iteration: 8210 ---- loss: 0.6488027068393756 \n",
            "Iteration: 8211 ---- loss: 0.6488007291978604 \n",
            "Iteration: 8212 ---- loss: 0.6487987523313332 \n",
            "Iteration: 8213 ---- loss: 0.648796776256749 \n",
            "Iteration: 8214 ---- loss: 0.6487948009610044 \n",
            "Iteration: 8215 ---- loss: 0.6487928264418266 \n",
            "Iteration: 8216 ---- loss: 0.6487908527126495 \n",
            "Iteration: 8217 ---- loss: 0.6487888798275693 \n",
            "Iteration: 8218 ---- loss: 0.6487869078336874 \n",
            "Iteration: 8219 ---- loss: 0.6487849369066688 \n",
            "Iteration: 8220 ---- loss: 0.6487829673925166 \n",
            "Iteration: 8221 ---- loss: 0.6487809986529943 \n",
            "Iteration: 8222 ---- loss: 0.6487790306945007 \n",
            "Iteration: 8223 ---- loss: 0.6487770634807289 \n",
            "Iteration: 8224 ---- loss: 0.6487750964588315 \n",
            "Iteration: 8225 ---- loss: 0.6487731289856323 \n",
            "Iteration: 8226 ---- loss: 0.6487711622873372 \n",
            "Iteration: 8227 ---- loss: 0.6487691963564647 \n",
            "Iteration: 8228 ---- loss: 0.6487672311890585 \n",
            "Iteration: 8229 ---- loss: 0.6487652667887659 \n",
            "Iteration: 8230 ---- loss: 0.648763303788878 \n",
            "Iteration: 8231 ---- loss: 0.6487613425007884 \n",
            "Iteration: 8232 ---- loss: 0.6487593820507133 \n",
            "Iteration: 8233 ---- loss: 0.6487574224570934 \n",
            "Iteration: 8234 ---- loss: 0.6487554642807668 \n",
            "Iteration: 8235 ---- loss: 0.6487535124934045 \n",
            "Iteration: 8236 ---- loss: 0.6487515615019466 \n",
            "Iteration: 8237 ---- loss: 0.6487496113784098 \n",
            "Iteration: 8238 ---- loss: 0.6487476626015304 \n",
            "Iteration: 8239 ---- loss: 0.6487457176775665 \n",
            "Iteration: 8240 ---- loss: 0.6487437735596997 \n",
            "Iteration: 8241 ---- loss: 0.6487418302488899 \n",
            "Iteration: 8242 ---- loss: 0.6487398877430134 \n",
            "Iteration: 8243 ---- loss: 0.6487379460382795 \n",
            "Iteration: 8244 ---- loss: 0.6487360051354296 \n",
            "Iteration: 8245 ---- loss: 0.6487340650334279 \n",
            "Iteration: 8246 ---- loss: 0.6487321261069564 \n",
            "Iteration: 8247 ---- loss: 0.6487301886072826 \n",
            "Iteration: 8248 ---- loss: 0.6487282519092183 \n",
            "Iteration: 8249 ---- loss: 0.648726316011725 \n",
            "Iteration: 8251 ---- loss: 0.6487224466158761 \n",
            "Iteration: 8252 ---- loss: 0.6487205131142337 \n",
            "Iteration: 8253 ---- loss: 0.6487185804081753 \n",
            "Iteration: 8254 ---- loss: 0.6487166484975847 \n",
            "Iteration: 8255 ---- loss: 0.6487147173814585 \n",
            "Iteration: 8256 ---- loss: 0.6487127870587985 \n",
            "Iteration: 8257 ---- loss: 0.6487108578261095 \n",
            "Iteration: 8258 ---- loss: 0.6487089300312228 \n",
            "Iteration: 8259 ---- loss: 0.6487070029143703 \n",
            "Iteration: 8260 ---- loss: 0.6487050765904034 \n",
            "Iteration: 8261 ---- loss: 0.6487031510583278 \n",
            "Iteration: 8262 ---- loss: 0.648701226317154 \n",
            "Iteration: 8263 ---- loss: 0.648699302365898 \n",
            "Iteration: 8264 ---- loss: 0.6486973792035811 \n",
            "Iteration: 8265 ---- loss: 0.648695456829229 \n",
            "Iteration: 8266 ---- loss: 0.648693535084855 \n",
            "Iteration: 8267 ---- loss: 0.6486916137438274 \n",
            "Iteration: 8268 ---- loss: 0.6486896931671792 \n",
            "Iteration: 8269 ---- loss: 0.6486877731341263 \n",
            "Iteration: 8270 ---- loss: 0.6486858538834198 \n",
            "Iteration: 8271 ---- loss: 0.6486839354323704 \n",
            "Iteration: 8272 ---- loss: 0.6486820178431703 \n",
            "Iteration: 8273 ---- loss: 0.648680101036005 \n",
            "Iteration: 8274 ---- loss: 0.6486781850099391 \n",
            "Iteration: 8275 ---- loss: 0.6486762697640418 \n",
            "Iteration: 8276 ---- loss: 0.6486743552973862 \n",
            "Iteration: 8277 ---- loss: 0.648672441573613 \n",
            "Iteration: 8278 ---- loss: 0.6486705283128751 \n",
            "Iteration: 8279 ---- loss: 0.6486686158277021 \n",
            "Iteration: 8280 ---- loss: 0.6486667041171857 \n",
            "Iteration: 8281 ---- loss: 0.6486647931804221 \n",
            "Iteration: 8282 ---- loss: 0.6486628830165101 \n",
            "Iteration: 8283 ---- loss: 0.6486609736245527 \n",
            "Iteration: 8284 ---- loss: 0.6486590650036554 \n",
            "Iteration: 8285 ---- loss: 0.6486571571529285 \n",
            "Iteration: 8286 ---- loss: 0.6486552500714844 \n",
            "Iteration: 8287 ---- loss: 0.6486533437584391 \n",
            "Iteration: 8288 ---- loss: 0.6486514382129122 \n",
            "Iteration: 8289 ---- loss: 0.6486495334340256 \n",
            "Iteration: 8290 ---- loss: 0.6486476294209055 \n",
            "Iteration: 8291 ---- loss: 0.6486457264225115 \n",
            "Iteration: 8292 ---- loss: 0.6486438244959377 \n",
            "Iteration: 8293 ---- loss: 0.6486419233347102 \n",
            "Iteration: 8294 ---- loss: 0.6486400229379542 \n",
            "Iteration: 8295 ---- loss: 0.6486381232266757 \n",
            "Iteration: 8296 ---- loss: 0.6486362241496455 \n",
            "Iteration: 8297 ---- loss: 0.6486343258339492 \n",
            "Iteration: 8298 ---- loss: 0.6486324282787264 \n",
            "Iteration: 8299 ---- loss: 0.6486305314831188 \n",
            "Iteration: 8301 ---- loss: 0.6486267401673326 \n",
            "Iteration: 8302 ---- loss: 0.6486248461207073 \n",
            "Iteration: 8303 ---- loss: 0.6486229574148695 \n",
            "Iteration: 8304 ---- loss: 0.6486210710995365 \n",
            "Iteration: 8305 ---- loss: 0.648619185556983 \n",
            "Iteration: 8306 ---- loss: 0.6486173007862742 \n",
            "Iteration: 8307 ---- loss: 0.6486154167864799 \n",
            "Iteration: 8308 ---- loss: 0.6486135335566741 \n",
            "Iteration: 8309 ---- loss: 0.648611651095935 \n",
            "Iteration: 8310 ---- loss: 0.6486097694420743 \n",
            "Iteration: 8311 ---- loss: 0.6486078885853305 \n",
            "Iteration: 8312 ---- loss: 0.6486060084961082 \n",
            "Iteration: 8313 ---- loss: 0.6486041291417405 \n",
            "Iteration: 8314 ---- loss: 0.6486022504999158 \n",
            "Iteration: 8315 ---- loss: 0.6486003721054137 \n",
            "Iteration: 8316 ---- loss: 0.6485984945076523 \n",
            "Iteration: 8317 ---- loss: 0.6485966190351127 \n",
            "Iteration: 8318 ---- loss: 0.6485947467802053 \n",
            "Iteration: 8319 ---- loss: 0.6485928753434107 \n",
            "Iteration: 8320 ---- loss: 0.6485910046209037 \n",
            "Iteration: 8321 ---- loss: 0.6485891347717638 \n",
            "Iteration: 8322 ---- loss: 0.6485872660957512 \n",
            "Iteration: 8323 ---- loss: 0.6485853981764513 \n",
            "Iteration: 8324 ---- loss: 0.6485835309727408 \n",
            "Iteration: 8325 ---- loss: 0.6485816642293021 \n",
            "Iteration: 8326 ---- loss: 0.6485797978351323 \n",
            "Iteration: 8327 ---- loss: 0.648577933079391 \n",
            "Iteration: 8328 ---- loss: 0.6485760694447965 \n",
            "Iteration: 8329 ---- loss: 0.6485742066657817 \n",
            "Iteration: 8330 ---- loss: 0.6485723445851812 \n",
            "Iteration: 8331 ---- loss: 0.6485704833892124 \n",
            "Iteration: 8332 ---- loss: 0.6485686242870673 \n",
            "Iteration: 8333 ---- loss: 0.6485667659249803 \n",
            "Iteration: 8334 ---- loss: 0.6485649084189333 \n",
            "Iteration: 8335 ---- loss: 0.6485630514772857 \n",
            "Iteration: 8336 ---- loss: 0.6485611953302718 \n",
            "Iteration: 8337 ---- loss: 0.6485593401499181 \n",
            "Iteration: 8338 ---- loss: 0.6485574856757556 \n",
            "Iteration: 8339 ---- loss: 0.6485556319990221 \n",
            "Iteration: 8340 ---- loss: 0.648553779117878 \n",
            "Iteration: 8341 ---- loss: 0.6485519269784413 \n",
            "Iteration: 8342 ---- loss: 0.6485500756082268 \n",
            "Iteration: 8343 ---- loss: 0.6485482250759542 \n",
            "Iteration: 8344 ---- loss: 0.648546376550857 \n",
            "Iteration: 8345 ---- loss: 0.6485445288607157 \n",
            "Iteration: 8346 ---- loss: 0.6485426819159708 \n",
            "Iteration: 8347 ---- loss: 0.6485408357410974 \n",
            "Iteration: 8348 ---- loss: 0.6485389904910772 \n",
            "Iteration: 8349 ---- loss: 0.6485371467656051 \n",
            "Iteration: 8351 ---- loss: 0.6485334639327327 \n",
            "Iteration: 8352 ---- loss: 0.6485316236737274 \n",
            "Iteration: 8353 ---- loss: 0.6485297841759702 \n",
            "Iteration: 8354 ---- loss: 0.6485279454571107 \n",
            "Iteration: 8355 ---- loss: 0.6485261075204753 \n",
            "Iteration: 8356 ---- loss: 0.6485242703137155 \n",
            "Iteration: 8357 ---- loss: 0.6485224339345661 \n",
            "Iteration: 8358 ---- loss: 0.6485205983645885 \n",
            "Iteration: 8359 ---- loss: 0.6485187635030956 \n",
            "Iteration: 8360 ---- loss: 0.6485169294442213 \n",
            "Iteration: 8361 ---- loss: 0.6485150961580383 \n",
            "Iteration: 8362 ---- loss: 0.648513264018736 \n",
            "Iteration: 8363 ---- loss: 0.6485114336395778 \n",
            "Iteration: 8364 ---- loss: 0.648509605348513 \n",
            "Iteration: 8365 ---- loss: 0.648507777805911 \n",
            "Iteration: 8366 ---- loss: 0.6485059510169475 \n",
            "Iteration: 8367 ---- loss: 0.6485041250553129 \n",
            "Iteration: 8368 ---- loss: 0.6485022998448288 \n",
            "Iteration: 8369 ---- loss: 0.648500475384034 \n",
            "Iteration: 8370 ---- loss: 0.6484986518843219 \n",
            "Iteration: 8371 ---- loss: 0.6484968290484965 \n",
            "Iteration: 8372 ---- loss: 0.648495007028546 \n",
            "Iteration: 8373 ---- loss: 0.6484931859849599 \n",
            "Iteration: 8374 ---- loss: 0.6484913657392433 \n",
            "Iteration: 8375 ---- loss: 0.6484895462290101 \n",
            "Iteration: 8376 ---- loss: 0.6484877275042499 \n",
            "Iteration: 8377 ---- loss: 0.648485909544417 \n",
            "Iteration: 8378 ---- loss: 0.6484840923755627 \n",
            "Iteration: 8379 ---- loss: 0.6484822759572225 \n",
            "Iteration: 8380 ---- loss: 0.6484804602928205 \n",
            "Iteration: 8381 ---- loss: 0.648478645642114 \n",
            "Iteration: 8382 ---- loss: 0.6484768320424275 \n",
            "Iteration: 8383 ---- loss: 0.6484750192920479 \n",
            "Iteration: 8384 ---- loss: 0.6484732078491623 \n",
            "Iteration: 8385 ---- loss: 0.6484713972123871 \n",
            "Iteration: 8386 ---- loss: 0.648469587328412 \n",
            "Iteration: 8387 ---- loss: 0.6484677781891525 \n",
            "Iteration: 8388 ---- loss: 0.6484659698169754 \n",
            "Iteration: 8389 ---- loss: 0.6484641622924483 \n",
            "Iteration: 8390 ---- loss: 0.6484623559730665 \n",
            "Iteration: 8391 ---- loss: 0.6484605504023395 \n",
            "Iteration: 8392 ---- loss: 0.648458745595841 \n",
            "Iteration: 8393 ---- loss: 0.648456941594579 \n",
            "Iteration: 8394 ---- loss: 0.6484551383277524 \n",
            "Iteration: 8395 ---- loss: 0.648453335678902 \n",
            "Iteration: 8396 ---- loss: 0.6484515335394104 \n",
            "Iteration: 8397 ---- loss: 0.6484497321917663 \n",
            "Iteration: 8398 ---- loss: 0.6484479315424398 \n",
            "Iteration: 8399 ---- loss: 0.6484461315354276 \n",
            "Iteration: 8401 ---- loss: 0.6484425352602987 \n",
            "Iteration: 8402 ---- loss: 0.6484407382580873 \n",
            "Iteration: 8403 ---- loss: 0.6484389419873204 \n",
            "Iteration: 8404 ---- loss: 0.6484371464718909 \n",
            "Iteration: 8405 ---- loss: 0.6484353517106248 \n",
            "Iteration: 8406 ---- loss: 0.6484335577098403 \n",
            "Iteration: 8407 ---- loss: 0.6484317644232485 \n",
            "Iteration: 8408 ---- loss: 0.6484299719012019 \n",
            "Iteration: 8409 ---- loss: 0.6484281801540491 \n",
            "Iteration: 8410 ---- loss: 0.6484263891770509 \n",
            "Iteration: 8411 ---- loss: 0.6484245989354269 \n",
            "Iteration: 8412 ---- loss: 0.6484228094214226 \n",
            "Iteration: 8413 ---- loss: 0.6484210206454374 \n",
            "Iteration: 8414 ---- loss: 0.6484192332120046 \n",
            "Iteration: 8415 ---- loss: 0.648417449648733 \n",
            "Iteration: 8416 ---- loss: 0.6484156668012524 \n",
            "Iteration: 8417 ---- loss: 0.6484138847064073 \n",
            "Iteration: 8418 ---- loss: 0.6484121033632673 \n",
            "Iteration: 8419 ---- loss: 0.6484103228017787 \n",
            "Iteration: 8420 ---- loss: 0.6484085429781651 \n",
            "Iteration: 8421 ---- loss: 0.6484067637003954 \n",
            "Iteration: 8422 ---- loss: 0.6484049852627564 \n",
            "Iteration: 8423 ---- loss: 0.6484032076090782 \n",
            "Iteration: 8424 ---- loss: 0.6484014304986446 \n",
            "Iteration: 8425 ---- loss: 0.6483996539779872 \n",
            "Iteration: 8426 ---- loss: 0.6483978788212411 \n",
            "Iteration: 8427 ---- loss: 0.6483961044138625 \n",
            "Iteration: 8428 ---- loss: 0.6483943307575518 \n",
            "Iteration: 8429 ---- loss: 0.6483925578522178 \n",
            "Iteration: 8430 ---- loss: 0.6483907855674049 \n",
            "Iteration: 8431 ---- loss: 0.6483890122495015 \n",
            "Iteration: 8432 ---- loss: 0.6483872397228244 \n",
            "Iteration: 8433 ---- loss: 0.6483854679935398 \n",
            "Iteration: 8434 ---- loss: 0.6483836970048308 \n",
            "Iteration: 8435 ---- loss: 0.6483819267841738 \n",
            "Iteration: 8436 ---- loss: 0.6483801572783411 \n",
            "Iteration: 8437 ---- loss: 0.6483783885056332 \n",
            "Iteration: 8438 ---- loss: 0.6483766204700139 \n",
            "Iteration: 8439 ---- loss: 0.6483748530867172 \n",
            "Iteration: 8440 ---- loss: 0.6483730861097007 \n",
            "Iteration: 8441 ---- loss: 0.6483713196152919 \n",
            "Iteration: 8442 ---- loss: 0.6483695535165993 \n",
            "Iteration: 8443 ---- loss: 0.6483677881496076 \n",
            "Iteration: 8444 ---- loss: 0.6483660235431588 \n",
            "Iteration: 8445 ---- loss: 0.6483642627286219 \n",
            "Iteration: 8446 ---- loss: 0.6483625026799926 \n",
            "Iteration: 8447 ---- loss: 0.6483607433694702 \n",
            "Iteration: 8448 ---- loss: 0.648358986481238 \n",
            "Iteration: 8449 ---- loss: 0.6483572329926489 \n",
            "Iteration: 8451 ---- loss: 0.6483537283376708 \n",
            "Iteration: 8452 ---- loss: 0.6483519771665044 \n",
            "Iteration: 8453 ---- loss: 0.6483502267503437 \n",
            "Iteration: 8454 ---- loss: 0.6483484769806487 \n",
            "Iteration: 8455 ---- loss: 0.648346727716345 \n",
            "Iteration: 8456 ---- loss: 0.6483449789983952 \n",
            "Iteration: 8457 ---- loss: 0.6483432307324984 \n",
            "Iteration: 8458 ---- loss: 0.6483414832176704 \n",
            "Iteration: 8459 ---- loss: 0.6483397364844227 \n",
            "Iteration: 8460 ---- loss: 0.648337993302352 \n",
            "Iteration: 8461 ---- loss: 0.6483362512914266 \n",
            "Iteration: 8462 ---- loss: 0.648334510034725 \n",
            "Iteration: 8463 ---- loss: 0.6483327695312382 \n",
            "Iteration: 8464 ---- loss: 0.6483310297799629 \n",
            "Iteration: 8465 ---- loss: 0.6483292907799019 \n",
            "Iteration: 8466 ---- loss: 0.6483275525373146 \n",
            "Iteration: 8467 ---- loss: 0.6483258150417122 \n",
            "Iteration: 8468 ---- loss: 0.6483240780227422 \n",
            "Iteration: 8469 ---- loss: 0.6483223410866154 \n",
            "Iteration: 8470 ---- loss: 0.6483206048959054 \n",
            "Iteration: 8471 ---- loss: 0.6483188694496427 \n",
            "Iteration: 8472 ---- loss: 0.6483171358702351 \n",
            "Iteration: 8473 ---- loss: 0.6483154057388484 \n",
            "Iteration: 8474 ---- loss: 0.648313676364268 \n",
            "Iteration: 8475 ---- loss: 0.6483119477634403 \n",
            "Iteration: 8476 ---- loss: 0.6483102213659692 \n",
            "Iteration: 8477 ---- loss: 0.6483084970301379 \n",
            "Iteration: 8478 ---- loss: 0.6483067734551904 \n",
            "Iteration: 8479 ---- loss: 0.6483050506400534 \n",
            "Iteration: 8480 ---- loss: 0.6483033285836621 \n",
            "Iteration: 8481 ---- loss: 0.6483016072849564 \n",
            "Iteration: 8482 ---- loss: 0.6482998861285518 \n",
            "Iteration: 8483 ---- loss: 0.6482981605031197 \n",
            "Iteration: 8484 ---- loss: 0.6482964356033603 \n",
            "Iteration: 8485 ---- loss: 0.6482947114325275 \n",
            "Iteration: 8486 ---- loss: 0.6482929879912471 \n",
            "Iteration: 8487 ---- loss: 0.6482912652176019 \n",
            "Iteration: 8488 ---- loss: 0.6482895430973002 \n",
            "Iteration: 8489 ---- loss: 0.6482878217007944 \n",
            "Iteration: 8490 ---- loss: 0.6482861007022683 \n",
            "Iteration: 8491 ---- loss: 0.6482843781368285 \n",
            "Iteration: 8492 ---- loss: 0.6482826535476347 \n",
            "Iteration: 8493 ---- loss: 0.6482809320724545 \n",
            "Iteration: 8494 ---- loss: 0.648279214614407 \n",
            "Iteration: 8495 ---- loss: 0.6482774981247459 \n",
            "Iteration: 8496 ---- loss: 0.6482757839015983 \n",
            "Iteration: 8497 ---- loss: 0.648274070430562 \n",
            "Iteration: 8498 ---- loss: 0.6482723576806589 \n",
            "Iteration: 8499 ---- loss: 0.64827064565613 \n",
            "Iteration: 8501 ---- loss: 0.6482672237370091 \n",
            "Iteration: 8502 ---- loss: 0.6482655139629339 \n",
            "Iteration: 8503 ---- loss: 0.6482638052443171 \n",
            "Iteration: 8504 ---- loss: 0.6482620972401677 \n",
            "Iteration: 8505 ---- loss: 0.6482603899496292 \n",
            "Iteration: 8506 ---- loss: 0.6482586833718482 \n",
            "Iteration: 8507 ---- loss: 0.6482569775059744 \n",
            "Iteration: 8508 ---- loss: 0.6482552723511591 \n",
            "Iteration: 8509 ---- loss: 0.6482535677081317 \n",
            "Iteration: 8510 ---- loss: 0.6482518635571527 \n",
            "Iteration: 8511 ---- loss: 0.6482501601140798 \n",
            "Iteration: 8512 ---- loss: 0.6482484573777247 \n",
            "Iteration: 8513 ---- loss: 0.6482467553477997 \n",
            "Iteration: 8514 ---- loss: 0.6482450546226246 \n",
            "Iteration: 8515 ---- loss: 0.6482433566459075 \n",
            "Iteration: 8516 ---- loss: 0.6482416604643773 \n",
            "Iteration: 8517 ---- loss: 0.6482399672620828 \n",
            "Iteration: 8518 ---- loss: 0.6482382748052923 \n",
            "Iteration: 8519 ---- loss: 0.6482365830443397 \n",
            "Iteration: 8520 ---- loss: 0.6482348920379287 \n",
            "Iteration: 8521 ---- loss: 0.6482332017299864 \n",
            "Iteration: 8522 ---- loss: 0.6482315121550903 \n",
            "Iteration: 8523 ---- loss: 0.6482298233033469 \n",
            "Iteration: 8524 ---- loss: 0.6482281351579848 \n",
            "Iteration: 8525 ---- loss: 0.6482264477570234 \n",
            "Iteration: 8526 ---- loss: 0.6482247610412042 \n",
            "Iteration: 8527 ---- loss: 0.6482230750812957 \n",
            "Iteration: 8528 ---- loss: 0.6482213898007674 \n",
            "Iteration: 8529 ---- loss: 0.6482197052655762 \n",
            "Iteration: 8530 ---- loss: 0.6482180214224351 \n",
            "Iteration: 8531 ---- loss: 0.6482163383100887 \n",
            "Iteration: 8532 ---- loss: 0.6482146558990302 \n",
            "Iteration: 8533 ---- loss: 0.6482129742076843 \n",
            "Iteration: 8534 ---- loss: 0.6482112932234328 \n",
            "Iteration: 8535 ---- loss: 0.6482096128738657 \n",
            "Iteration: 8536 ---- loss: 0.6482079330100936 \n",
            "Iteration: 8537 ---- loss: 0.648206253860032 \n",
            "Iteration: 8538 ---- loss: 0.648204574953336 \n",
            "Iteration: 8539 ---- loss: 0.6482028940339042 \n",
            "Iteration: 8540 ---- loss: 0.6482012138039029 \n",
            "Iteration: 8541 ---- loss: 0.6481995342682526 \n",
            "Iteration: 8542 ---- loss: 0.6481978554131248 \n",
            "Iteration: 8543 ---- loss: 0.6481961772586746 \n",
            "Iteration: 8544 ---- loss: 0.6481945001561922 \n",
            "Iteration: 8545 ---- loss: 0.6481928246304028 \n",
            "Iteration: 8546 ---- loss: 0.6481911497621593 \n",
            "Iteration: 8547 ---- loss: 0.6481894756189037 \n",
            "Iteration: 8548 ---- loss: 0.6481878021116243 \n",
            "Iteration: 8549 ---- loss: 0.6481861293291231 \n",
            "Iteration: 8551 ---- loss: 0.6481827857790163 \n",
            "Iteration: 8552 ---- loss: 0.6481811150129609 \n",
            "Iteration: 8553 ---- loss: 0.6481794448843375 \n",
            "Iteration: 8554 ---- loss: 0.648177775163029 \n",
            "Iteration: 8555 ---- loss: 0.6481761060988748 \n",
            "Iteration: 8556 ---- loss: 0.6481744377402898 \n",
            "Iteration: 8557 ---- loss: 0.6481727700448061 \n",
            "Iteration: 8558 ---- loss: 0.6481711030277092 \n",
            "Iteration: 8559 ---- loss: 0.648169436707457 \n",
            "Iteration: 8560 ---- loss: 0.6481677710382232 \n",
            "Iteration: 8561 ---- loss: 0.6481661060699674 \n",
            "Iteration: 8562 ---- loss: 0.6481644418040651 \n",
            "Iteration: 8563 ---- loss: 0.6481627790841132 \n",
            "Iteration: 8564 ---- loss: 0.6481611170948828 \n",
            "Iteration: 8565 ---- loss: 0.6481594558113464 \n",
            "Iteration: 8566 ---- loss: 0.6481577953821587 \n",
            "Iteration: 8567 ---- loss: 0.6481561365935489 \n",
            "Iteration: 8568 ---- loss: 0.6481544818317407 \n",
            "Iteration: 8569 ---- loss: 0.6481528271825907 \n",
            "Iteration: 8570 ---- loss: 0.6481511720862345 \n",
            "Iteration: 8571 ---- loss: 0.6481495176563021 \n",
            "Iteration: 8572 ---- loss: 0.6481478639078031 \n",
            "Iteration: 8573 ---- loss: 0.648146210835683 \n",
            "Iteration: 8574 ---- loss: 0.6481445584253973 \n",
            "Iteration: 8575 ---- loss: 0.6481429067031034 \n",
            "Iteration: 8576 ---- loss: 0.6481412556595142 \n",
            "Iteration: 8577 ---- loss: 0.6481396055506343 \n",
            "Iteration: 8578 ---- loss: 0.6481379568500447 \n",
            "Iteration: 8579 ---- loss: 0.6481363088427932 \n",
            "Iteration: 8580 ---- loss: 0.6481346614901761 \n",
            "Iteration: 8581 ---- loss: 0.6481330148134494 \n",
            "Iteration: 8582 ---- loss: 0.6481313689321008 \n",
            "Iteration: 8583 ---- loss: 0.6481297240608265 \n",
            "Iteration: 8584 ---- loss: 0.6481280798582467 \n",
            "Iteration: 8585 ---- loss: 0.6481264363336615 \n",
            "Iteration: 8586 ---- loss: 0.6481247937021868 \n",
            "Iteration: 8587 ---- loss: 0.6481231518727771 \n",
            "Iteration: 8588 ---- loss: 0.6481215107160125 \n",
            "Iteration: 8589 ---- loss: 0.6481198702393197 \n",
            "Iteration: 8590 ---- loss: 0.6481182304391904 \n",
            "Iteration: 8591 ---- loss: 0.6481165912948443 \n",
            "Iteration: 8592 ---- loss: 0.6481149530199978 \n",
            "Iteration: 8593 ---- loss: 0.6481133160742937 \n",
            "Iteration: 8594 ---- loss: 0.648111679807888 \n",
            "Iteration: 8595 ---- loss: 0.6481100448563403 \n",
            "Iteration: 8596 ---- loss: 0.6481084132192145 \n",
            "Iteration: 8597 ---- loss: 0.6481067822622415 \n",
            "Iteration: 8598 ---- loss: 0.6481051519983733 \n",
            "Iteration: 8599 ---- loss: 0.6481035224035375 \n",
            "Iteration: 8601 ---- loss: 0.6481002652356692 \n",
            "Iteration: 8602 ---- loss: 0.6480986377475895 \n",
            "Iteration: 8603 ---- loss: 0.6480970126176867 \n",
            "Iteration: 8604 ---- loss: 0.6480953881590016 \n",
            "Iteration: 8605 ---- loss: 0.6480937645107425 \n",
            "Iteration: 8606 ---- loss: 0.6480921416860245 \n",
            "Iteration: 8607 ---- loss: 0.6480905195390261 \n",
            "Iteration: 8608 ---- loss: 0.6480888980692864 \n",
            "Iteration: 8609 ---- loss: 0.648087277407815 \n",
            "Iteration: 8610 ---- loss: 0.6480856575243139 \n",
            "Iteration: 8611 ---- loss: 0.6480840383176739 \n",
            "Iteration: 8612 ---- loss: 0.6480824197870529 \n",
            "Iteration: 8613 ---- loss: 0.6480808020071128 \n",
            "Iteration: 8614 ---- loss: 0.6480791856297271 \n",
            "Iteration: 8615 ---- loss: 0.6480775699226515 \n",
            "Iteration: 8616 ---- loss: 0.6480759548404863 \n",
            "Iteration: 8617 ---- loss: 0.6480743404213204 \n",
            "Iteration: 8618 ---- loss: 0.6480727266739356 \n",
            "Iteration: 8619 ---- loss: 0.6480711135975097 \n",
            "Iteration: 8620 ---- loss: 0.6480695011912244 \n",
            "Iteration: 8621 ---- loss: 0.648067891708956 \n",
            "Iteration: 8622 ---- loss: 0.6480662847076908 \n",
            "Iteration: 8623 ---- loss: 0.6480646781825254 \n",
            "Iteration: 8624 ---- loss: 0.6480630723372225 \n",
            "Iteration: 8625 ---- loss: 0.6480614673583667 \n",
            "Iteration: 8626 ---- loss: 0.6480598635106358 \n",
            "Iteration: 8627 ---- loss: 0.6480582602214389 \n",
            "Iteration: 8628 ---- loss: 0.648056656226768 \n",
            "Iteration: 8629 ---- loss: 0.6480550516670959 \n",
            "Iteration: 8630 ---- loss: 0.6480534477738396 \n",
            "Iteration: 8631 ---- loss: 0.6480518445461942 \n",
            "Iteration: 8632 ---- loss: 0.6480502419833575 \n",
            "Iteration: 8633 ---- loss: 0.6480486400404868 \n",
            "Iteration: 8634 ---- loss: 0.6480470384874314 \n",
            "Iteration: 8635 ---- loss: 0.6480454373150383 \n",
            "Iteration: 8636 ---- loss: 0.6480438368551859 \n",
            "Iteration: 8637 ---- loss: 0.6480422370528898 \n",
            "Iteration: 8638 ---- loss: 0.6480406379073763 \n",
            "Iteration: 8639 ---- loss: 0.648039039417873 \n",
            "Iteration: 8640 ---- loss: 0.6480374416004044 \n",
            "Iteration: 8641 ---- loss: 0.6480358444646716 \n",
            "Iteration: 8642 ---- loss: 0.6480342479828668 \n",
            "Iteration: 8643 ---- loss: 0.6480326514370311 \n",
            "Iteration: 8644 ---- loss: 0.6480310498456099 \n",
            "Iteration: 8645 ---- loss: 0.64802944909586 \n",
            "Iteration: 8646 ---- loss: 0.6480278505678039 \n",
            "Iteration: 8647 ---- loss: 0.64802625270027 \n",
            "Iteration: 8648 ---- loss: 0.6480246554395637 \n",
            "Iteration: 8649 ---- loss: 0.6480230588270289 \n",
            "Iteration: 8651 ---- loss: 0.6480198685646876 \n",
            "Iteration: 8652 ---- loss: 0.6480182743240109 \n",
            "Iteration: 8653 ---- loss: 0.6480166807183889 \n",
            "Iteration: 8654 ---- loss: 0.6480150877471033 \n",
            "Iteration: 8655 ---- loss: 0.6480134954094369 \n",
            "Iteration: 8656 ---- loss: 0.6480119037046729 \n",
            "Iteration: 8657 ---- loss: 0.6480103126320962 \n",
            "Iteration: 8658 ---- loss: 0.6480087221909926 \n",
            "Iteration: 8659 ---- loss: 0.6480071323824873 \n",
            "Iteration: 8660 ---- loss: 0.6480055445275745 \n",
            "Iteration: 8661 ---- loss: 0.648003960653724 \n",
            "Iteration: 8662 ---- loss: 0.6480023782833999 \n",
            "Iteration: 8663 ---- loss: 0.6480007968646737 \n",
            "Iteration: 8664 ---- loss: 0.6479992160864025 \n",
            "Iteration: 8665 ---- loss: 0.647997635823246 \n",
            "Iteration: 8666 ---- loss: 0.6479960558346674 \n",
            "Iteration: 8667 ---- loss: 0.6479944764833574 \n",
            "Iteration: 8668 ---- loss: 0.6479928977685852 \n",
            "Iteration: 8669 ---- loss: 0.6479913196896203 \n",
            "Iteration: 8670 ---- loss: 0.6479897422457342 \n",
            "Iteration: 8671 ---- loss: 0.6479881654361996 \n",
            "Iteration: 8672 ---- loss: 0.6479865892602908 \n",
            "Iteration: 8673 ---- loss: 0.6479850137172826 \n",
            "Iteration: 8674 ---- loss: 0.6479834388064516 \n",
            "Iteration: 8675 ---- loss: 0.647981864471879 \n",
            "Iteration: 8676 ---- loss: 0.6479802906892246 \n",
            "Iteration: 8677 ---- loss: 0.6479787175363906 \n",
            "Iteration: 8678 ---- loss: 0.647977145012658 \n",
            "Iteration: 8679 ---- loss: 0.64797557311731 \n",
            "Iteration: 8680 ---- loss: 0.647974001849631 \n",
            "Iteration: 8681 ---- loss: 0.647972431208906 \n",
            "Iteration: 8682 ---- loss: 0.6479708611985907 \n",
            "Iteration: 8683 ---- loss: 0.6479692918207812 \n",
            "Iteration: 8684 ---- loss: 0.647967723137322 \n",
            "Iteration: 8685 ---- loss: 0.6479661554375122 \n",
            "Iteration: 8686 ---- loss: 0.6479645903386246 \n",
            "Iteration: 8687 ---- loss: 0.6479630258369583 \n",
            "Iteration: 8688 ---- loss: 0.6479614620453308 \n",
            "Iteration: 8689 ---- loss: 0.6479598990742391 \n",
            "Iteration: 8690 ---- loss: 0.6479583367305197 \n",
            "Iteration: 8691 ---- loss: 0.6479567752943827 \n",
            "Iteration: 8692 ---- loss: 0.6479552150112883 \n",
            "Iteration: 8693 ---- loss: 0.6479536593254683 \n",
            "Iteration: 8694 ---- loss: 0.6479521043994567 \n",
            "Iteration: 8695 ---- loss: 0.6479505501229595 \n",
            "Iteration: 8696 ---- loss: 0.6479489965058748 \n",
            "Iteration: 8697 ---- loss: 0.6479474435356462 \n",
            "Iteration: 8698 ---- loss: 0.6479458931439671 \n",
            "Iteration: 8699 ---- loss: 0.6479443452982426 \n",
            "Iteration: 8701 ---- loss: 0.6479412515997535 \n",
            "Iteration: 8702 ---- loss: 0.6479397058069979 \n",
            "Iteration: 8703 ---- loss: 0.6479381611451946 \n",
            "Iteration: 8704 ---- loss: 0.6479366171567491 \n",
            "Iteration: 8705 ---- loss: 0.6479350738016302 \n",
            "Iteration: 8706 ---- loss: 0.6479335310992308 \n",
            "Iteration: 8707 ---- loss: 0.6479319890653646 \n",
            "Iteration: 8708 ---- loss: 0.6479304476615092 \n",
            "Iteration: 8709 ---- loss: 0.6479289070378147 \n",
            "Iteration: 8710 ---- loss: 0.647927368193971 \n",
            "Iteration: 8711 ---- loss: 0.6479258300051156 \n",
            "Iteration: 8712 ---- loss: 0.6479242924550518 \n",
            "Iteration: 8713 ---- loss: 0.6479227555664047 \n",
            "Iteration: 8714 ---- loss: 0.6479212200345417 \n",
            "Iteration: 8715 ---- loss: 0.6479196870924399 \n",
            "Iteration: 8716 ---- loss: 0.6479181553023953 \n",
            "Iteration: 8717 ---- loss: 0.6479166241814743 \n",
            "Iteration: 8718 ---- loss: 0.6479150937099635 \n",
            "Iteration: 8719 ---- loss: 0.6479135642515066 \n",
            "Iteration: 8720 ---- loss: 0.6479120381808904 \n",
            "Iteration: 8721 ---- loss: 0.6479105114073667 \n",
            "Iteration: 8722 ---- loss: 0.6479089855155303 \n",
            "Iteration: 8723 ---- loss: 0.6479074603129654 \n",
            "Iteration: 8724 ---- loss: 0.6479059358378564 \n",
            "Iteration: 8725 ---- loss: 0.6479044120570995 \n",
            "Iteration: 8726 ---- loss: 0.6479028894102707 \n",
            "Iteration: 8727 ---- loss: 0.6479013680210916 \n",
            "Iteration: 8728 ---- loss: 0.6478998473013798 \n",
            "Iteration: 8729 ---- loss: 0.6478983272568051 \n",
            "Iteration: 8730 ---- loss: 0.647896807910303 \n",
            "Iteration: 8731 ---- loss: 0.6478952892151842 \n",
            "Iteration: 8732 ---- loss: 0.6478937712242763 \n",
            "Iteration: 8733 ---- loss: 0.6478922538936004 \n",
            "Iteration: 8734 ---- loss: 0.6478907374054454 \n",
            "Iteration: 8735 ---- loss: 0.6478892217248811 \n",
            "Iteration: 8736 ---- loss: 0.6478877067779711 \n",
            "Iteration: 8737 ---- loss: 0.6478861926450994 \n",
            "Iteration: 8738 ---- loss: 0.6478846791549263 \n",
            "Iteration: 8739 ---- loss: 0.6478831663684994 \n",
            "Iteration: 8740 ---- loss: 0.6478816542244009 \n",
            "Iteration: 8741 ---- loss: 0.6478801427962382 \n",
            "Iteration: 8742 ---- loss: 0.6478786320557363 \n",
            "Iteration: 8743 ---- loss: 0.647877121951252 \n",
            "Iteration: 8744 ---- loss: 0.6478756125641079 \n",
            "Iteration: 8745 ---- loss: 0.6478741037841771 \n",
            "Iteration: 8746 ---- loss: 0.6478725957417211 \n",
            "Iteration: 8747 ---- loss: 0.6478710882126486 \n",
            "Iteration: 8748 ---- loss: 0.6478695808867131 \n",
            "Iteration: 8749 ---- loss: 0.647868073111988 \n",
            "Iteration: 8751 ---- loss: 0.6478650588432482 \n",
            "Iteration: 8752 ---- loss: 0.6478635527277564 \n",
            "Iteration: 8753 ---- loss: 0.64786204731841 \n",
            "Iteration: 8754 ---- loss: 0.6478605426197068 \n",
            "Iteration: 8755 ---- loss: 0.6478590385162094 \n",
            "Iteration: 8756 ---- loss: 0.6478575350959076 \n",
            "Iteration: 8757 ---- loss: 0.6478560322996697 \n",
            "Iteration: 8758 ---- loss: 0.6478545301792066 \n",
            "Iteration: 8759 ---- loss: 0.647853028975165 \n",
            "Iteration: 8760 ---- loss: 0.6478515281175329 \n",
            "Iteration: 8761 ---- loss: 0.6478500260811095 \n",
            "Iteration: 8762 ---- loss: 0.6478485222075214 \n",
            "Iteration: 8763 ---- loss: 0.6478470190409022 \n",
            "Iteration: 8764 ---- loss: 0.6478455164201349 \n",
            "Iteration: 8765 ---- loss: 0.6478440144710402 \n",
            "Iteration: 8766 ---- loss: 0.6478425131260319 \n",
            "Iteration: 8767 ---- loss: 0.6478410123820695 \n",
            "Iteration: 8768 ---- loss: 0.6478395123097109 \n",
            "Iteration: 8769 ---- loss: 0.6478380127872015 \n",
            "Iteration: 8770 ---- loss: 0.6478365139776118 \n",
            "Iteration: 8771 ---- loss: 0.6478350158803398 \n",
            "Iteration: 8772 ---- loss: 0.6478335185877472 \n",
            "Iteration: 8773 ---- loss: 0.6478320227220479 \n",
            "Iteration: 8774 ---- loss: 0.6478305276041072 \n",
            "Iteration: 8775 ---- loss: 0.6478290333075537 \n",
            "Iteration: 8776 ---- loss: 0.6478275396227273 \n",
            "Iteration: 8777 ---- loss: 0.6478260465176657 \n",
            "Iteration: 8778 ---- loss: 0.6478245541077547 \n",
            "Iteration: 8779 ---- loss: 0.6478230622572424 \n",
            "Iteration: 8780 ---- loss: 0.6478215710232959 \n",
            "Iteration: 8781 ---- loss: 0.6478200804605867 \n",
            "Iteration: 8782 ---- loss: 0.6478185904596323 \n",
            "Iteration: 8783 ---- loss: 0.6478171010827009 \n",
            "Iteration: 8784 ---- loss: 0.6478156123644182 \n",
            "Iteration: 8785 ---- loss: 0.6478141242095936 \n",
            "Iteration: 8786 ---- loss: 0.6478126366756423 \n",
            "Iteration: 8787 ---- loss: 0.6478111497990534 \n",
            "Iteration: 8788 ---- loss: 0.647809663730779 \n",
            "Iteration: 8789 ---- loss: 0.6478081807580254 \n",
            "Iteration: 8790 ---- loss: 0.6478066984618045 \n",
            "Iteration: 8791 ---- loss: 0.647805216739626 \n",
            "Iteration: 8792 ---- loss: 0.6478037361344815 \n",
            "Iteration: 8793 ---- loss: 0.6478022594219156 \n",
            "Iteration: 8794 ---- loss: 0.6478007833391226 \n",
            "Iteration: 8795 ---- loss: 0.6477993078523813 \n",
            "Iteration: 8796 ---- loss: 0.647797833005604 \n",
            "Iteration: 8797 ---- loss: 0.6477963588275752 \n",
            "Iteration: 8798 ---- loss: 0.6477948852411126 \n",
            "Iteration: 8799 ---- loss: 0.64779341227197 \n",
            "Iteration: 8801 ---- loss: 0.6477904682723633 \n",
            "Iteration: 8802 ---- loss: 0.6477889971891144 \n",
            "Iteration: 8803 ---- loss: 0.6477875267244888 \n",
            "Iteration: 8804 ---- loss: 0.6477860569013957 \n",
            "Iteration: 8805 ---- loss: 0.6477845877214278 \n",
            "Iteration: 8806 ---- loss: 0.6477831191365608 \n",
            "Iteration: 8807 ---- loss: 0.6477816512948928 \n",
            "Iteration: 8808 ---- loss: 0.6477801851068528 \n",
            "Iteration: 8809 ---- loss: 0.6477787195829807 \n",
            "Iteration: 8810 ---- loss: 0.647777254660212 \n",
            "Iteration: 8811 ---- loss: 0.6477757903471516 \n",
            "Iteration: 8812 ---- loss: 0.6477743266582908 \n",
            "Iteration: 8813 ---- loss: 0.6477728635983839 \n",
            "Iteration: 8814 ---- loss: 0.6477714011844075 \n",
            "Iteration: 8815 ---- loss: 0.6477699393737928 \n",
            "Iteration: 8816 ---- loss: 0.6477684781712834 \n",
            "Iteration: 8817 ---- loss: 0.6477670175889351 \n",
            "Iteration: 8818 ---- loss: 0.6477655576259488 \n",
            "Iteration: 8819 ---- loss: 0.6477640983015408 \n",
            "Iteration: 8820 ---- loss: 0.6477626395889422 \n",
            "Iteration: 8821 ---- loss: 0.6477611813913051 \n",
            "Iteration: 8822 ---- loss: 0.6477597236312841 \n",
            "Iteration: 8823 ---- loss: 0.6477582664862997 \n",
            "Iteration: 8824 ---- loss: 0.6477568099555693 \n",
            "Iteration: 8825 ---- loss: 0.6477553540439738 \n",
            "Iteration: 8826 ---- loss: 0.6477538987564393 \n",
            "Iteration: 8827 ---- loss: 0.6477524440579162 \n",
            "Iteration: 8828 ---- loss: 0.6477509898894658 \n",
            "Iteration: 8829 ---- loss: 0.6477495363291136 \n",
            "Iteration: 8830 ---- loss: 0.6477480833782184 \n",
            "Iteration: 8831 ---- loss: 0.6477466310410733 \n",
            "Iteration: 8832 ---- loss: 0.647745180212321 \n",
            "Iteration: 8833 ---- loss: 0.647743730099039 \n",
            "Iteration: 8834 ---- loss: 0.6477422808386956 \n",
            "Iteration: 8835 ---- loss: 0.6477408321734102 \n",
            "Iteration: 8836 ---- loss: 0.6477393841252391 \n",
            "Iteration: 8837 ---- loss: 0.6477379366809616 \n",
            "Iteration: 8838 ---- loss: 0.6477364898459599 \n",
            "Iteration: 8839 ---- loss: 0.6477350436199877 \n",
            "Iteration: 8840 ---- loss: 0.6477335980022643 \n",
            "Iteration: 8841 ---- loss: 0.6477321529920124 \n",
            "Iteration: 8842 ---- loss: 0.6477307085884563 \n",
            "Iteration: 8843 ---- loss: 0.6477292647908235 \n",
            "Iteration: 8844 ---- loss: 0.647727821598344 \n",
            "Iteration: 8845 ---- loss: 0.6477263790102498 \n",
            "Iteration: 8846 ---- loss: 0.6477249370257759 \n",
            "Iteration: 8847 ---- loss: 0.6477234956441594 \n",
            "Iteration: 8848 ---- loss: 0.6477220548651599 \n",
            "Iteration: 8849 ---- loss: 0.647720614695353 \n",
            "Iteration: 8851 ---- loss: 0.6477177370713697 \n",
            "Iteration: 8852 ---- loss: 0.6477163015664146 \n",
            "Iteration: 8853 ---- loss: 0.647714866905515 \n",
            "Iteration: 8854 ---- loss: 0.647713433423405 \n",
            "Iteration: 8855 ---- loss: 0.6477120009173243 \n",
            "Iteration: 8856 ---- loss: 0.6477105697797116 \n",
            "Iteration: 8857 ---- loss: 0.6477091392494645 \n",
            "Iteration: 8858 ---- loss: 0.6477077093257836 \n",
            "Iteration: 8859 ---- loss: 0.647706280007873 \n",
            "Iteration: 8860 ---- loss: 0.6477048512949396 \n",
            "Iteration: 8861 ---- loss: 0.6477034231948716 \n",
            "Iteration: 8862 ---- loss: 0.6477019957169965 \n",
            "Iteration: 8863 ---- loss: 0.6477005688172875 \n",
            "Iteration: 8864 ---- loss: 0.6476991425194 \n",
            "Iteration: 8865 ---- loss: 0.6476977168225554 \n",
            "Iteration: 8866 ---- loss: 0.6476962917259775 \n",
            "Iteration: 8867 ---- loss: 0.6476948672288931 \n",
            "Iteration: 8868 ---- loss: 0.6476934434078594 \n",
            "Iteration: 8869 ---- loss: 0.6476920201404504 \n",
            "Iteration: 8870 ---- loss: 0.6476905974708702 \n",
            "Iteration: 8871 ---- loss: 0.6476891754501749 \n",
            "Iteration: 8872 ---- loss: 0.64768775468488 \n",
            "Iteration: 8873 ---- loss: 0.6476863362434422 \n",
            "Iteration: 8874 ---- loss: 0.64768492140538 \n",
            "Iteration: 8875 ---- loss: 0.6476835071952826 \n",
            "Iteration: 8876 ---- loss: 0.6476820935886162 \n",
            "Iteration: 8877 ---- loss: 0.6476806805912305 \n",
            "Iteration: 8878 ---- loss: 0.6476792686071424 \n",
            "Iteration: 8879 ---- loss: 0.6476778576266231 \n",
            "Iteration: 8880 ---- loss: 0.6476764472734213 \n",
            "Iteration: 8881 ---- loss: 0.647675037488451 \n",
            "Iteration: 8882 ---- loss: 0.6476736283538445 \n",
            "Iteration: 8883 ---- loss: 0.6476722198891516 \n",
            "Iteration: 8884 ---- loss: 0.6476708119786679 \n",
            "Iteration: 8885 ---- loss: 0.6476694047037421 \n",
            "Iteration: 8886 ---- loss: 0.6476679980156842 \n",
            "Iteration: 8887 ---- loss: 0.6476665920506331 \n",
            "Iteration: 8888 ---- loss: 0.6476651869067093 \n",
            "Iteration: 8889 ---- loss: 0.6476637843156671 \n",
            "Iteration: 8890 ---- loss: 0.6476623826661411 \n",
            "Iteration: 8891 ---- loss: 0.6476609816694647 \n",
            "Iteration: 8892 ---- loss: 0.6476595812291938 \n",
            "Iteration: 8893 ---- loss: 0.6476581814763964 \n",
            "Iteration: 8894 ---- loss: 0.6476567823419761 \n",
            "Iteration: 8895 ---- loss: 0.647655383605777 \n",
            "Iteration: 8896 ---- loss: 0.6476539852168537 \n",
            "Iteration: 8897 ---- loss: 0.6476525874850723 \n",
            "Iteration: 8898 ---- loss: 0.6476511902578161 \n",
            "Iteration: 8899 ---- loss: 0.6476497934541734 \n",
            "Iteration: 8901 ---- loss: 0.6476470016675295 \n",
            "Iteration: 8902 ---- loss: 0.6476456071121699 \n",
            "Iteration: 8903 ---- loss: 0.6476442131832729 \n",
            "Iteration: 8904 ---- loss: 0.6476428196307639 \n",
            "Iteration: 8905 ---- loss: 0.6476414247921697 \n",
            "Iteration: 8906 ---- loss: 0.6476400305401531 \n",
            "Iteration: 8907 ---- loss: 0.6476386369008006 \n",
            "Iteration: 8908 ---- loss: 0.6476372439158866 \n",
            "Iteration: 8909 ---- loss: 0.6476358515383166 \n",
            "Iteration: 8910 ---- loss: 0.6476344597167637 \n",
            "Iteration: 8911 ---- loss: 0.6476330685400951 \n",
            "Iteration: 8912 ---- loss: 0.6476316779208323 \n",
            "Iteration: 8913 ---- loss: 0.6476302879546098 \n",
            "Iteration: 8914 ---- loss: 0.64762889845756 \n",
            "Iteration: 8915 ---- loss: 0.6476275096305633 \n",
            "Iteration: 8916 ---- loss: 0.6476261213764758 \n",
            "Iteration: 8917 ---- loss: 0.647624733922949 \n",
            "Iteration: 8918 ---- loss: 0.6476233470297517 \n",
            "Iteration: 8919 ---- loss: 0.6476219610761216 \n",
            "Iteration: 8920 ---- loss: 0.647620577961186 \n",
            "Iteration: 8921 ---- loss: 0.6476191954548797 \n",
            "Iteration: 8922 ---- loss: 0.647617813583473 \n",
            "Iteration: 8923 ---- loss: 0.647616432301195 \n",
            "Iteration: 8924 ---- loss: 0.6476150516724147 \n",
            "Iteration: 8925 ---- loss: 0.6476136737219834 \n",
            "Iteration: 8926 ---- loss: 0.6476122992397604 \n",
            "Iteration: 8927 ---- loss: 0.647610925346904 \n",
            "Iteration: 8928 ---- loss: 0.6476095521099705 \n",
            "Iteration: 8929 ---- loss: 0.6476081794927875 \n",
            "Iteration: 8930 ---- loss: 0.6476068074902083 \n",
            "Iteration: 8931 ---- loss: 0.6476054361536133 \n",
            "Iteration: 8932 ---- loss: 0.6476040653950859 \n",
            "Iteration: 8933 ---- loss: 0.6476026952914197 \n",
            "Iteration: 8934 ---- loss: 0.6476013258025867 \n",
            "Iteration: 8935 ---- loss: 0.6475999569169667 \n",
            "Iteration: 8936 ---- loss: 0.6475985887006007 \n",
            "Iteration: 8937 ---- loss: 0.6475972210559436 \n",
            "Iteration: 8938 ---- loss: 0.6475958540513248 \n",
            "Iteration: 8939 ---- loss: 0.6475944876772082 \n",
            "Iteration: 8940 ---- loss: 0.6475931218794895 \n",
            "Iteration: 8941 ---- loss: 0.6475917567590739 \n",
            "Iteration: 8942 ---- loss: 0.6475903922035394 \n",
            "Iteration: 8943 ---- loss: 0.6475890282719144 \n",
            "Iteration: 8944 ---- loss: 0.6475876649865712 \n",
            "Iteration: 8945 ---- loss: 0.6475863022736568 \n",
            "Iteration: 8946 ---- loss: 0.647584940184637 \n",
            "Iteration: 8947 ---- loss: 0.647583578728904 \n",
            "Iteration: 8948 ---- loss: 0.6475822178467147 \n",
            "Iteration: 8949 ---- loss: 0.6475808575950215 \n",
            "Iteration: 8951 ---- loss: 0.6475781391348324 \n",
            "Iteration: 8952 ---- loss: 0.6475767823173523 \n",
            "Iteration: 8953 ---- loss: 0.6475754283423087 \n",
            "Iteration: 8954 ---- loss: 0.6475740749777559 \n",
            "Iteration: 8955 ---- loss: 0.6475727222429132 \n",
            "Iteration: 8956 ---- loss: 0.6475713704325942 \n",
            "Iteration: 8957 ---- loss: 0.6475700192234283 \n",
            "Iteration: 8958 ---- loss: 0.6475686686677521 \n",
            "Iteration: 8959 ---- loss: 0.6475673187272287 \n",
            "Iteration: 8960 ---- loss: 0.6475659693406537 \n",
            "Iteration: 8961 ---- loss: 0.6475646205679247 \n",
            "Iteration: 8962 ---- loss: 0.6475632724466689 \n",
            "Iteration: 8963 ---- loss: 0.6475619249106351 \n",
            "Iteration: 8964 ---- loss: 0.6475605779645458 \n",
            "Iteration: 8965 ---- loss: 0.6475592316758677 \n",
            "Iteration: 8966 ---- loss: 0.6475578859969747 \n",
            "Iteration: 8967 ---- loss: 0.6475565409024301 \n",
            "Iteration: 8968 ---- loss: 0.647555196437178 \n",
            "Iteration: 8969 ---- loss: 0.6475538525849005 \n",
            "Iteration: 8970 ---- loss: 0.6475525089206101 \n",
            "Iteration: 8971 ---- loss: 0.6475511658721714 \n",
            "Iteration: 8972 ---- loss: 0.647549823465591 \n",
            "Iteration: 8973 ---- loss: 0.6475484816504633 \n",
            "Iteration: 8974 ---- loss: 0.6475471404252403 \n",
            "Iteration: 8975 ---- loss: 0.647545799829797 \n",
            "Iteration: 8976 ---- loss: 0.6475444598387697 \n",
            "Iteration: 8977 ---- loss: 0.6475431204519966 \n",
            "Iteration: 8978 ---- loss: 0.6475417816586648 \n",
            "Iteration: 8979 ---- loss: 0.6475404434700877 \n",
            "Iteration: 8980 ---- loss: 0.6475391059003113 \n",
            "Iteration: 8981 ---- loss: 0.6475377689232752 \n",
            "Iteration: 8982 ---- loss: 0.647536432546678 \n",
            "Iteration: 8983 ---- loss: 0.6475350967594503 \n",
            "Iteration: 8984 ---- loss: 0.6475337615869406 \n",
            "Iteration: 8985 ---- loss: 0.6475324270164974 \n",
            "Iteration: 8986 ---- loss: 0.6475310930329603 \n",
            "Iteration: 8987 ---- loss: 0.6475297596429448 \n",
            "Iteration: 8988 ---- loss: 0.6475284268435716 \n",
            "Iteration: 8989 ---- loss: 0.6475270946764149 \n",
            "Iteration: 8990 ---- loss: 0.6475257630640063 \n",
            "Iteration: 8991 ---- loss: 0.6475244330874205 \n",
            "Iteration: 8992 ---- loss: 0.6475231046717239 \n",
            "Iteration: 8993 ---- loss: 0.647521777938818 \n",
            "Iteration: 8994 ---- loss: 0.6475204523715713 \n",
            "Iteration: 8995 ---- loss: 0.647519127391937 \n",
            "Iteration: 8996 ---- loss: 0.647517803197742 \n",
            "Iteration: 8997 ---- loss: 0.6475164798772913 \n",
            "Iteration: 8998 ---- loss: 0.6475151571775816 \n",
            "Iteration: 8999 ---- loss: 0.6475138350793689 \n",
            "Iteration: 9001 ---- loss: 0.6475111941887733 \n",
            "Iteration: 9002 ---- loss: 0.6475098751897344 \n",
            "Iteration: 9003 ---- loss: 0.6475085568346016 \n",
            "Iteration: 9004 ---- loss: 0.6475072393420405 \n",
            "Iteration: 9005 ---- loss: 0.6475059232848553 \n",
            "Iteration: 9006 ---- loss: 0.6475046078506358 \n",
            "Iteration: 9007 ---- loss: 0.6475032930383435 \n",
            "Iteration: 9008 ---- loss: 0.6475019788513608 \n",
            "Iteration: 9009 ---- loss: 0.6475006652876454 \n",
            "Iteration: 9010 ---- loss: 0.6474993525396362 \n",
            "Iteration: 9011 ---- loss: 0.6474980410484731 \n",
            "Iteration: 9012 ---- loss: 0.6474967303379029 \n",
            "Iteration: 9013 ---- loss: 0.6474954217560522 \n",
            "Iteration: 9014 ---- loss: 0.6474941148399648 \n",
            "Iteration: 9015 ---- loss: 0.6474928085636176 \n",
            "Iteration: 9016 ---- loss: 0.6474915029332079 \n",
            "Iteration: 9017 ---- loss: 0.6474901979484259 \n",
            "Iteration: 9018 ---- loss: 0.6474888935881025 \n",
            "Iteration: 9019 ---- loss: 0.6474875898646518 \n",
            "Iteration: 9020 ---- loss: 0.647486286776924 \n",
            "Iteration: 9021 ---- loss: 0.6474849843237784 \n",
            "Iteration: 9022 ---- loss: 0.6474836825040844 \n",
            "Iteration: 9023 ---- loss: 0.6474823813167209 \n",
            "Iteration: 9024 ---- loss: 0.6474810807605755 \n",
            "Iteration: 9025 ---- loss: 0.6474797808365248 \n",
            "Iteration: 9026 ---- loss: 0.647478481552256 \n",
            "Iteration: 9027 ---- loss: 0.6474771828937138 \n",
            "Iteration: 9028 ---- loss: 0.6474758848564547 \n",
            "Iteration: 9029 ---- loss: 0.6474745874450255 \n",
            "Iteration: 9030 ---- loss: 0.6474732906583651 \n",
            "Iteration: 9031 ---- loss: 0.6474719944530335 \n",
            "Iteration: 9032 ---- loss: 0.647470698659987 \n",
            "Iteration: 9033 ---- loss: 0.6474694034880856 \n",
            "Iteration: 9034 ---- loss: 0.6474681089363018 \n",
            "Iteration: 9035 ---- loss: 0.6474668150036146 \n",
            "Iteration: 9036 ---- loss: 0.6474655216890104 \n",
            "Iteration: 9037 ---- loss: 0.647464229051698 \n",
            "Iteration: 9038 ---- loss: 0.6474629369883109 \n",
            "Iteration: 9039 ---- loss: 0.6474616455253861 \n",
            "Iteration: 9040 ---- loss: 0.647460354699596 \n",
            "Iteration: 9041 ---- loss: 0.6474590645242774 \n",
            "Iteration: 9042 ---- loss: 0.6474577749040702 \n",
            "Iteration: 9043 ---- loss: 0.6474564858964179 \n",
            "Iteration: 9044 ---- loss: 0.6474551975847682 \n",
            "Iteration: 9045 ---- loss: 0.6474539099149178 \n",
            "Iteration: 9046 ---- loss: 0.6474526229801466 \n",
            "Iteration: 9047 ---- loss: 0.6474513367405345 \n",
            "Iteration: 9048 ---- loss: 0.6474500510288576 \n",
            "Iteration: 9049 ---- loss: 0.6474487659225203 \n",
            "Iteration: 9051 ---- loss: 0.6474461976138535 \n",
            "Iteration: 9052 ---- loss: 0.6474449143475862 \n",
            "Iteration: 9053 ---- loss: 0.6474436317306873 \n",
            "Iteration: 9054 ---- loss: 0.6474423496430012 \n",
            "Iteration: 9055 ---- loss: 0.6474410682236058 \n",
            "Iteration: 9056 ---- loss: 0.6474397873771602 \n",
            "Iteration: 9057 ---- loss: 0.6474385071030166 \n",
            "Iteration: 9058 ---- loss: 0.6474372272845027 \n",
            "Iteration: 9059 ---- loss: 0.6474359476326228 \n",
            "Iteration: 9060 ---- loss: 0.6474346687181387 \n",
            "Iteration: 9061 ---- loss: 0.6474333905753424 \n",
            "Iteration: 9062 ---- loss: 0.6474321132441656 \n",
            "Iteration: 9063 ---- loss: 0.6474308374531451 \n",
            "Iteration: 9064 ---- loss: 0.6474295621834296 \n",
            "Iteration: 9065 ---- loss: 0.6474282876315043 \n",
            "Iteration: 9066 ---- loss: 0.6474270135597423 \n",
            "Iteration: 9067 ---- loss: 0.647425740187005 \n",
            "Iteration: 9068 ---- loss: 0.6474244673362111 \n",
            "Iteration: 9069 ---- loss: 0.647423195142007 \n",
            "Iteration: 9070 ---- loss: 0.6474219235046246 \n",
            "Iteration: 9071 ---- loss: 0.6474206524919884 \n",
            "Iteration: 9072 ---- loss: 0.6474193820578488 \n",
            "Iteration: 9073 ---- loss: 0.6474181122298223 \n",
            "Iteration: 9074 ---- loss: 0.6474168429888227 \n",
            "Iteration: 9075 ---- loss: 0.647415574348454 \n",
            "Iteration: 9076 ---- loss: 0.6474143062905549 \n",
            "Iteration: 9077 ---- loss: 0.6474130388408977 \n",
            "Iteration: 9078 ---- loss: 0.6474117719561215 \n",
            "Iteration: 9079 ---- loss: 0.6474105057002353 \n",
            "Iteration: 9080 ---- loss: 0.6474092399786651 \n",
            "Iteration: 9081 ---- loss: 0.6474079749196125 \n",
            "Iteration: 9082 ---- loss: 0.6474067104054172 \n",
            "Iteration: 9083 ---- loss: 0.6474054469893844 \n",
            "Iteration: 9084 ---- loss: 0.6474041840402314 \n",
            "Iteration: 9085 ---- loss: 0.6474029217664178 \n",
            "Iteration: 9086 ---- loss: 0.6474016600226378 \n",
            "Iteration: 9087 ---- loss: 0.6474003988930321 \n",
            "Iteration: 9088 ---- loss: 0.6473991387359418 \n",
            "Iteration: 9089 ---- loss: 0.6473978790949181 \n",
            "Iteration: 9090 ---- loss: 0.6473966201466071 \n",
            "Iteration: 9091 ---- loss: 0.6473953616762935 \n",
            "Iteration: 9092 ---- loss: 0.6473941038359767 \n",
            "Iteration: 9093 ---- loss: 0.6473928465842511 \n",
            "Iteration: 9094 ---- loss: 0.6473915898569707 \n",
            "Iteration: 9095 ---- loss: 0.6473903337992762 \n",
            "Iteration: 9096 ---- loss: 0.6473890782458106 \n",
            "Iteration: 9097 ---- loss: 0.6473878232722647 \n",
            "Iteration: 9098 ---- loss: 0.6473865689451119 \n",
            "Iteration: 9099 ---- loss: 0.6473853151113705 \n",
            "Iteration: 9101 ---- loss: 0.6473828092675658 \n",
            "Iteration: 9102 ---- loss: 0.647381557161025 \n",
            "Iteration: 9103 ---- loss: 0.6473803056606668 \n",
            "Iteration: 9104 ---- loss: 0.6473790547599114 \n",
            "Iteration: 9105 ---- loss: 0.6473778043734947 \n",
            "Iteration: 9106 ---- loss: 0.6473765545782255 \n",
            "Iteration: 9107 ---- loss: 0.6473753054010156 \n",
            "Iteration: 9108 ---- loss: 0.647374056750878 \n",
            "Iteration: 9109 ---- loss: 0.6473728089528245 \n",
            "Iteration: 9110 ---- loss: 0.6473715626885383 \n",
            "Iteration: 9111 ---- loss: 0.6473703173791095 \n",
            "Iteration: 9112 ---- loss: 0.6473690726416456 \n",
            "Iteration: 9113 ---- loss: 0.6473678285098315 \n",
            "Iteration: 9114 ---- loss: 0.6473665849758745 \n",
            "Iteration: 9115 ---- loss: 0.6473653419710316 \n",
            "Iteration: 9116 ---- loss: 0.6473640995431518 \n",
            "Iteration: 9117 ---- loss: 0.6473628577267995 \n",
            "Iteration: 9118 ---- loss: 0.6473616164804585 \n",
            "Iteration: 9119 ---- loss: 0.6473603757754313 \n",
            "Iteration: 9120 ---- loss: 0.647359135644129 \n",
            "Iteration: 9121 ---- loss: 0.6473578961050891 \n",
            "Iteration: 9122 ---- loss: 0.6473566572168341 \n",
            "Iteration: 9123 ---- loss: 0.6473554189717664 \n",
            "Iteration: 9124 ---- loss: 0.6473541812977757 \n",
            "Iteration: 9125 ---- loss: 0.6473529441940695 \n",
            "Iteration: 9126 ---- loss: 0.6473517076872198 \n",
            "Iteration: 9127 ---- loss: 0.6473504717257038 \n",
            "Iteration: 9128 ---- loss: 0.647349236749535 \n",
            "Iteration: 9129 ---- loss: 0.6473480024590157 \n",
            "Iteration: 9130 ---- loss: 0.6473467687414809 \n",
            "Iteration: 9131 ---- loss: 0.6473455356037159 \n",
            "Iteration: 9132 ---- loss: 0.6473443030657988 \n",
            "Iteration: 9133 ---- loss: 0.6473430710592016 \n",
            "Iteration: 9134 ---- loss: 0.6473418396267097 \n",
            "Iteration: 9135 ---- loss: 0.6473406090213893 \n",
            "Iteration: 9136 ---- loss: 0.6473393789854412 \n",
            "Iteration: 9137 ---- loss: 0.6473381495180702 \n",
            "Iteration: 9138 ---- loss: 0.6473369206348959 \n",
            "Iteration: 9139 ---- loss: 0.6473356923187031 \n",
            "Iteration: 9140 ---- loss: 0.6473344645492269 \n",
            "Iteration: 9141 ---- loss: 0.6473332373451899 \n",
            "Iteration: 9142 ---- loss: 0.647332011261489 \n",
            "Iteration: 9143 ---- loss: 0.6473307865388592 \n",
            "Iteration: 9144 ---- loss: 0.6473295623936629 \n",
            "Iteration: 9145 ---- loss: 0.6473283389427013 \n",
            "Iteration: 9146 ---- loss: 0.6473271160833536 \n",
            "Iteration: 9147 ---- loss: 0.6473258935863687 \n",
            "Iteration: 9148 ---- loss: 0.6473246716801418 \n",
            "Iteration: 9149 ---- loss: 0.6473234503389912 \n",
            "Iteration: 9151 ---- loss: 0.6473210093689354 \n",
            "Iteration: 9152 ---- loss: 0.6473197897445119 \n",
            "Iteration: 9153 ---- loss: 0.6473185706926896 \n",
            "Iteration: 9154 ---- loss: 0.6473173522126215 \n",
            "Iteration: 9155 ---- loss: 0.6473161343034658 \n",
            "Iteration: 9156 ---- loss: 0.6473149169643837 \n",
            "Iteration: 9157 ---- loss: 0.6473137001945408 \n",
            "Iteration: 9158 ---- loss: 0.6473124839931066 \n",
            "Iteration: 9159 ---- loss: 0.6473112683592537 \n",
            "Iteration: 9160 ---- loss: 0.6473100532921593 \n",
            "Iteration: 9161 ---- loss: 0.6473088387910038 \n",
            "Iteration: 9162 ---- loss: 0.6473076248549707 \n",
            "Iteration: 9163 ---- loss: 0.647306411618162 \n",
            "Iteration: 9164 ---- loss: 0.6473051990614801 \n",
            "Iteration: 9165 ---- loss: 0.6473039870700135 \n",
            "Iteration: 9166 ---- loss: 0.647302775642944 \n",
            "Iteration: 9167 ---- loss: 0.647301564795547 \n",
            "Iteration: 9168 ---- loss: 0.6473003545251085 \n",
            "Iteration: 9169 ---- loss: 0.6472991448268214 \n",
            "Iteration: 9170 ---- loss: 0.6472979356846809 \n",
            "Iteration: 9171 ---- loss: 0.6472967271006902 \n",
            "Iteration: 9172 ---- loss: 0.6472955190774465 \n",
            "Iteration: 9173 ---- loss: 0.647294311614375 \n",
            "Iteration: 9174 ---- loss: 0.6472931047206197 \n",
            "Iteration: 9175 ---- loss: 0.6472918983742577 \n",
            "Iteration: 9176 ---- loss: 0.6472906925854788 \n",
            "Iteration: 9177 ---- loss: 0.6472894873534993 \n",
            "Iteration: 9178 ---- loss: 0.6472882826745516 \n",
            "Iteration: 9179 ---- loss: 0.6472870784856221 \n",
            "Iteration: 9180 ---- loss: 0.647285874849049 \n",
            "Iteration: 9181 ---- loss: 0.6472846717142458 \n",
            "Iteration: 9182 ---- loss: 0.6472834688837861 \n",
            "Iteration: 9183 ---- loss: 0.6472822666056534 \n",
            "Iteration: 9184 ---- loss: 0.6472810648790828 \n",
            "Iteration: 9185 ---- loss: 0.6472798637039158 \n",
            "Iteration: 9186 ---- loss: 0.6472786630845975 \n",
            "Iteration: 9187 ---- loss: 0.6472774630075514 \n",
            "Iteration: 9188 ---- loss: 0.6472762634790326 \n",
            "Iteration: 9189 ---- loss: 0.6472750644982876 \n",
            "Iteration: 9190 ---- loss: 0.6472738660645668 \n",
            "Iteration: 9191 ---- loss: 0.6472726681771221 \n",
            "Iteration: 9192 ---- loss: 0.6472714708507904 \n",
            "Iteration: 9193 ---- loss: 0.6472702746483869 \n",
            "Iteration: 9194 ---- loss: 0.6472690789912189 \n",
            "Iteration: 9195 ---- loss: 0.6472678838815509 \n",
            "Iteration: 9196 ---- loss: 0.6472666893106991 \n",
            "Iteration: 9197 ---- loss: 0.6472654952824195 \n",
            "Iteration: 9198 ---- loss: 0.6472643017959774 \n",
            "Iteration: 9199 ---- loss: 0.6472631088992853 \n",
            "Iteration: 9201 ---- loss: 0.6472607248937601 \n",
            "Iteration: 9202 ---- loss: 0.6472595336995501 \n",
            "Iteration: 9203 ---- loss: 0.6472583430434155 \n",
            "Iteration: 9204 ---- loss: 0.6472571530175073 \n",
            "Iteration: 9205 ---- loss: 0.6472559635469806 \n",
            "Iteration: 9206 ---- loss: 0.6472547743986597 \n",
            "Iteration: 9207 ---- loss: 0.6472535857864433 \n",
            "Iteration: 9208 ---- loss: 0.6472523977096171 \n",
            "Iteration: 9209 ---- loss: 0.6472512101674692 \n",
            "Iteration: 9210 ---- loss: 0.6472500231631276 \n",
            "Iteration: 9211 ---- loss: 0.6472488367301642 \n",
            "Iteration: 9212 ---- loss: 0.6472476508316838 \n",
            "Iteration: 9213 ---- loss: 0.6472464654618241 \n",
            "Iteration: 9214 ---- loss: 0.6472452806237181 \n",
            "Iteration: 9215 ---- loss: 0.6472440963166624 \n",
            "Iteration: 9216 ---- loss: 0.6472429127263644 \n",
            "Iteration: 9217 ---- loss: 0.6472417303670062 \n",
            "Iteration: 9218 ---- loss: 0.6472405486125357 \n",
            "Iteration: 9219 ---- loss: 0.6472393673764653 \n",
            "Iteration: 9220 ---- loss: 0.6472381866527879 \n",
            "Iteration: 9221 ---- loss: 0.6472370064445414 \n",
            "Iteration: 9222 ---- loss: 0.6472358267403024 \n",
            "Iteration: 9223 ---- loss: 0.647234647511556 \n",
            "Iteration: 9224 ---- loss: 0.6472334688345973 \n",
            "Iteration: 9225 ---- loss: 0.6472322907031797 \n",
            "Iteration: 9226 ---- loss: 0.6472311130816215 \n",
            "Iteration: 9227 ---- loss: 0.6472299359818352 \n",
            "Iteration: 9228 ---- loss: 0.6472287594410412 \n",
            "Iteration: 9229 ---- loss: 0.6472275833461701 \n",
            "Iteration: 9230 ---- loss: 0.6472264077395138 \n",
            "Iteration: 9231 ---- loss: 0.6472252326644392 \n",
            "Iteration: 9232 ---- loss: 0.6472240581125616 \n",
            "Iteration: 9233 ---- loss: 0.6472228841143572 \n",
            "Iteration: 9234 ---- loss: 0.6472217105899587 \n",
            "Iteration: 9235 ---- loss: 0.6472205376031488 \n",
            "Iteration: 9236 ---- loss: 0.6472193652679229 \n",
            "Iteration: 9237 ---- loss: 0.6472181943836478 \n",
            "Iteration: 9238 ---- loss: 0.6472170244259644 \n",
            "Iteration: 9239 ---- loss: 0.6472158557461322 \n",
            "Iteration: 9240 ---- loss: 0.6472146876164578 \n",
            "Iteration: 9241 ---- loss: 0.6472135200843521 \n",
            "Iteration: 9242 ---- loss: 0.647212353427547 \n",
            "Iteration: 9243 ---- loss: 0.6472111872854299 \n",
            "Iteration: 9244 ---- loss: 0.6472100216625457 \n",
            "Iteration: 9245 ---- loss: 0.6472088565819785 \n",
            "Iteration: 9246 ---- loss: 0.647207692030259 \n",
            "Iteration: 9247 ---- loss: 0.6472065279971391 \n",
            "Iteration: 9248 ---- loss: 0.6472053644769642 \n",
            "Iteration: 9249 ---- loss: 0.6472042014703486 \n",
            "Iteration: 9251 ---- loss: 0.6472018772389866 \n",
            "Iteration: 9252 ---- loss: 0.6472007161043906 \n",
            "Iteration: 9253 ---- loss: 0.6471995554750053 \n",
            "Iteration: 9254 ---- loss: 0.6471983955092412 \n",
            "Iteration: 9255 ---- loss: 0.6471972367130463 \n",
            "Iteration: 9256 ---- loss: 0.6471960784351055 \n",
            "Iteration: 9257 ---- loss: 0.6471949206723053 \n",
            "Iteration: 9258 ---- loss: 0.6471937634499191 \n",
            "Iteration: 9259 ---- loss: 0.6471926067481805 \n",
            "Iteration: 9260 ---- loss: 0.6471914505466044 \n",
            "Iteration: 9261 ---- loss: 0.647190294881552 \n",
            "Iteration: 9262 ---- loss: 0.6471891397175804 \n",
            "Iteration: 9263 ---- loss: 0.6471879852810423 \n",
            "Iteration: 9264 ---- loss: 0.6471868314811432 \n",
            "Iteration: 9265 ---- loss: 0.6471856781808186 \n",
            "Iteration: 9266 ---- loss: 0.6471845253969586 \n",
            "Iteration: 9267 ---- loss: 0.6471833732123297 \n",
            "Iteration: 9268 ---- loss: 0.6471822215784756 \n",
            "Iteration: 9269 ---- loss: 0.6471810704497979 \n",
            "Iteration: 9270 ---- loss: 0.6471799198471215 \n",
            "Iteration: 9271 ---- loss: 0.6471787697410881 \n",
            "Iteration: 9272 ---- loss: 0.6471776201471332 \n",
            "Iteration: 9273 ---- loss: 0.6471764710722594 \n",
            "Iteration: 9274 ---- loss: 0.6471753224581976 \n",
            "Iteration: 9275 ---- loss: 0.6471741743517586 \n",
            "Iteration: 9276 ---- loss: 0.6471730267635919 \n",
            "Iteration: 9277 ---- loss: 0.6471718796810809 \n",
            "Iteration: 9278 ---- loss: 0.6471707330998886 \n",
            "Iteration: 9279 ---- loss: 0.6471695870288131 \n",
            "Iteration: 9280 ---- loss: 0.6471684432185163 \n",
            "Iteration: 9281 ---- loss: 0.647167300876705 \n",
            "Iteration: 9282 ---- loss: 0.6471661590564142 \n",
            "Iteration: 9283 ---- loss: 0.6471650180233262 \n",
            "Iteration: 9284 ---- loss: 0.6471638779597377 \n",
            "Iteration: 9285 ---- loss: 0.6471627384121038 \n",
            "Iteration: 9286 ---- loss: 0.6471615993846808 \n",
            "Iteration: 9287 ---- loss: 0.6471604608886656 \n",
            "Iteration: 9288 ---- loss: 0.6471593228973666 \n",
            "Iteration: 9289 ---- loss: 0.6471581854234313 \n",
            "Iteration: 9290 ---- loss: 0.6471570484690575 \n",
            "Iteration: 9291 ---- loss: 0.6471559120376155 \n",
            "Iteration: 9292 ---- loss: 0.6471547761109215 \n",
            "Iteration: 9293 ---- loss: 0.6471536406985872 \n",
            "Iteration: 9294 ---- loss: 0.6471525058026152 \n",
            "Iteration: 9295 ---- loss: 0.6471513714263673 \n",
            "Iteration: 9296 ---- loss: 0.6471502375523419 \n",
            "Iteration: 9297 ---- loss: 0.6471491041897229 \n",
            "Iteration: 9298 ---- loss: 0.6471479713377795 \n",
            "Iteration: 9299 ---- loss: 0.6471468390945871 \n",
            "Iteration: 9301 ---- loss: 0.6471445774728367 \n",
            "Iteration: 9302 ---- loss: 0.6471434476416469 \n",
            "Iteration: 9303 ---- loss: 0.6471423184742063 \n",
            "Iteration: 9304 ---- loss: 0.6471411898226892 \n",
            "Iteration: 9305 ---- loss: 0.6471400616817112 \n",
            "Iteration: 9306 ---- loss: 0.6471389340558849 \n",
            "Iteration: 9307 ---- loss: 0.6471378069444401 \n",
            "Iteration: 9308 ---- loss: 0.6471366803541987 \n",
            "Iteration: 9309 ---- loss: 0.6471355542717001 \n",
            "Iteration: 9310 ---- loss: 0.6471344286984918 \n",
            "Iteration: 9311 ---- loss: 0.6471333036366241 \n",
            "Iteration: 9312 ---- loss: 0.6471321790853451 \n",
            "Iteration: 9313 ---- loss: 0.64713105504915 \n",
            "Iteration: 9314 ---- loss: 0.6471299315208198 \n",
            "Iteration: 9315 ---- loss: 0.6471288084965091 \n",
            "Iteration: 9316 ---- loss: 0.647127685979816 \n",
            "Iteration: 9317 ---- loss: 0.6471265639700047 \n",
            "Iteration: 9318 ---- loss: 0.6471254424663436 \n",
            "Iteration: 9319 ---- loss: 0.6471243214755703 \n",
            "Iteration: 9320 ---- loss: 0.6471232009828572 \n",
            "Iteration: 9321 ---- loss: 0.6471220809929688 \n",
            "Iteration: 9322 ---- loss: 0.6471209615063344 \n",
            "Iteration: 9323 ---- loss: 0.6471198425222375 \n",
            "Iteration: 9324 ---- loss: 0.6471187240399635 \n",
            "Iteration: 9325 ---- loss: 0.6471176060627508 \n",
            "Iteration: 9326 ---- loss: 0.6471164885853955 \n",
            "Iteration: 9327 ---- loss: 0.6471153716040214 \n",
            "Iteration: 9328 ---- loss: 0.6471142551216424 \n",
            "Iteration: 9329 ---- loss: 0.6471131391375585 \n",
            "Iteration: 9330 ---- loss: 0.647112023651071 \n",
            "Iteration: 9331 ---- loss: 0.6471109086893811 \n",
            "Iteration: 9332 ---- loss: 0.6471097942079778 \n",
            "Iteration: 9333 ---- loss: 0.6471086802480029 \n",
            "Iteration: 9334 ---- loss: 0.6471075667602177 \n",
            "Iteration: 9335 ---- loss: 0.6471064538019153 \n",
            "Iteration: 9336 ---- loss: 0.6471053417930661 \n",
            "Iteration: 9337 ---- loss: 0.6471042301467024 \n",
            "Iteration: 9338 ---- loss: 0.6471031190136434 \n",
            "Iteration: 9339 ---- loss: 0.6471020083752337 \n",
            "Iteration: 9340 ---- loss: 0.6471008982568662 \n",
            "Iteration: 9341 ---- loss: 0.6470997886302676 \n",
            "Iteration: 9342 ---- loss: 0.6470986794510369 \n",
            "Iteration: 9343 ---- loss: 0.6470975707714974 \n",
            "Iteration: 9344 ---- loss: 0.6470964625847492 \n",
            "Iteration: 9345 ---- loss: 0.6470953548760372 \n",
            "Iteration: 9346 ---- loss: 0.6470942476693431 \n",
            "Iteration: 9347 ---- loss: 0.6470931409431739 \n",
            "Iteration: 9348 ---- loss: 0.6470920354619696 \n",
            "Iteration: 9349 ---- loss: 0.6470909312336726 \n",
            "Iteration: 9351 ---- loss: 0.6470887242541303 \n",
            "Iteration: 9352 ---- loss: 0.6470876218864252 \n",
            "Iteration: 9353 ---- loss: 0.6470865234216712 \n",
            "Iteration: 9354 ---- loss: 0.6470854255405215 \n",
            "Iteration: 9355 ---- loss: 0.6470843285944586 \n",
            "Iteration: 9356 ---- loss: 0.6470832321611124 \n",
            "Iteration: 9357 ---- loss: 0.6470821362269372 \n",
            "Iteration: 9358 ---- loss: 0.6470810408020743 \n",
            "Iteration: 9359 ---- loss: 0.6470799459010028 \n",
            "Iteration: 9360 ---- loss: 0.6470788514904772 \n",
            "Iteration: 9361 ---- loss: 0.6470777575582025 \n",
            "Iteration: 9362 ---- loss: 0.6470766641089194 \n",
            "Iteration: 9363 ---- loss: 0.6470755711664475 \n",
            "Iteration: 9364 ---- loss: 0.6470744787189142 \n",
            "Iteration: 9365 ---- loss: 0.6470733867748124 \n",
            "Iteration: 9366 ---- loss: 0.6470722953419954 \n",
            "Iteration: 9367 ---- loss: 0.6470712045057802 \n",
            "Iteration: 9368 ---- loss: 0.6470701153257296 \n",
            "Iteration: 9369 ---- loss: 0.6470690281048982 \n",
            "Iteration: 9370 ---- loss: 0.647067941411842 \n",
            "Iteration: 9371 ---- loss: 0.6470668552413925 \n",
            "Iteration: 9372 ---- loss: 0.6470657695831904 \n",
            "Iteration: 9373 ---- loss: 0.647064684448415 \n",
            "Iteration: 9374 ---- loss: 0.6470635998375276 \n",
            "Iteration: 9375 ---- loss: 0.6470625157550446 \n",
            "Iteration: 9376 ---- loss: 0.6470614321794856 \n",
            "Iteration: 9377 ---- loss: 0.6470603491183001 \n",
            "Iteration: 9378 ---- loss: 0.6470592665721868 \n",
            "Iteration: 9379 ---- loss: 0.6470581845403102 \n",
            "Iteration: 9380 ---- loss: 0.6470571030304207 \n",
            "Iteration: 9381 ---- loss: 0.6470560220954409 \n",
            "Iteration: 9382 ---- loss: 0.647054941708407 \n",
            "Iteration: 9383 ---- loss: 0.6470538617648446 \n",
            "Iteration: 9384 ---- loss: 0.6470527823332762 \n",
            "Iteration: 9385 ---- loss: 0.6470517034128824 \n",
            "Iteration: 9386 ---- loss: 0.6470506250111954 \n",
            "Iteration: 9387 ---- loss: 0.6470495471109527 \n",
            "Iteration: 9388 ---- loss: 0.6470484697186345 \n",
            "Iteration: 9389 ---- loss: 0.6470473928342578 \n",
            "Iteration: 9390 ---- loss: 0.6470463164570267 \n",
            "Iteration: 9391 ---- loss: 0.6470452405861483 \n",
            "Iteration: 9392 ---- loss: 0.6470441652208352 \n",
            "Iteration: 9393 ---- loss: 0.6470430903652272 \n",
            "Iteration: 9394 ---- loss: 0.6470420160103486 \n",
            "Iteration: 9395 ---- loss: 0.6470409421564686 \n",
            "Iteration: 9396 ---- loss: 0.6470398688050427 \n",
            "Iteration: 9397 ---- loss: 0.6470387959553038 \n",
            "Iteration: 9398 ---- loss: 0.6470377236064877 \n",
            "Iteration: 9399 ---- loss: 0.6470366517578339 \n",
            "Iteration: 9401 ---- loss: 0.6470345095579931 \n",
            "Iteration: 9402 ---- loss: 0.6470334392670667 \n",
            "Iteration: 9403 ---- loss: 0.6470323695440161 \n",
            "Iteration: 9404 ---- loss: 0.6470313000848993 \n",
            "Iteration: 9405 ---- loss: 0.6470302311179976 \n",
            "Iteration: 9406 ---- loss: 0.6470291627428946 \n",
            "Iteration: 9407 ---- loss: 0.6470280949888977 \n",
            "Iteration: 9408 ---- loss: 0.6470270277289204 \n",
            "Iteration: 9409 ---- loss: 0.6470259609621655 \n",
            "Iteration: 9410 ---- loss: 0.6470248946878405 \n",
            "Iteration: 9411 ---- loss: 0.6470238289051581 \n",
            "Iteration: 9412 ---- loss: 0.6470227636191443 \n",
            "Iteration: 9413 ---- loss: 0.6470216988331796 \n",
            "Iteration: 9414 ---- loss: 0.6470206345366282 \n",
            "Iteration: 9415 ---- loss: 0.6470195707287201 \n",
            "Iteration: 9416 ---- loss: 0.6470185074086902 \n",
            "Iteration: 9417 ---- loss: 0.6470174445757767 \n",
            "Iteration: 9418 ---- loss: 0.6470163822292234 \n",
            "Iteration: 9419 ---- loss: 0.6470153203954966 \n",
            "Iteration: 9420 ---- loss: 0.647014259078081 \n",
            "Iteration: 9421 ---- loss: 0.6470131982585374 \n",
            "Iteration: 9422 ---- loss: 0.6470121378951857 \n",
            "Iteration: 9423 ---- loss: 0.647011077956938 \n",
            "Iteration: 9424 ---- loss: 0.6470100185028534 \n",
            "Iteration: 9425 ---- loss: 0.6470089595321852 \n",
            "Iteration: 9426 ---- loss: 0.6470079010445489 \n",
            "Iteration: 9427 ---- loss: 0.6470068430530564 \n",
            "Iteration: 9428 ---- loss: 0.6470057855270435 \n",
            "Iteration: 9429 ---- loss: 0.6470047284815076 \n",
            "Iteration: 9430 ---- loss: 0.6470036719157211 \n",
            "Iteration: 9431 ---- loss: 0.6470026158289597 \n",
            "Iteration: 9432 ---- loss: 0.6470015602205026 \n",
            "Iteration: 9433 ---- loss: 0.647000505089633 \n",
            "Iteration: 9434 ---- loss: 0.6469994504615216 \n",
            "Iteration: 9435 ---- loss: 0.6469983964874186 \n",
            "Iteration: 9436 ---- loss: 0.6469973429888164 \n",
            "Iteration: 9437 ---- loss: 0.64699628996552 \n",
            "Iteration: 9438 ---- loss: 0.6469952374168287 \n",
            "Iteration: 9439 ---- loss: 0.6469941853420458 \n",
            "Iteration: 9440 ---- loss: 0.6469931336938468 \n",
            "Iteration: 9441 ---- loss: 0.6469920823506411 \n",
            "Iteration: 9442 ---- loss: 0.6469910314784512 \n",
            "Iteration: 9443 ---- loss: 0.6469899810765948 \n",
            "Iteration: 9444 ---- loss: 0.6469889311443917 \n",
            "Iteration: 9445 ---- loss: 0.6469878816845295 \n",
            "Iteration: 9446 ---- loss: 0.6469868326905677 \n",
            "Iteration: 9447 ---- loss: 0.6469857841621957 \n",
            "Iteration: 9448 ---- loss: 0.6469847361007905 \n",
            "Iteration: 9449 ---- loss: 0.6469836885056853 \n",
            "Iteration: 9451 ---- loss: 0.6469815950668859 \n",
            "Iteration: 9452 ---- loss: 0.6469805491705123 \n",
            "Iteration: 9453 ---- loss: 0.6469795038219138 \n",
            "Iteration: 9454 ---- loss: 0.6469784591084476 \n",
            "Iteration: 9455 ---- loss: 0.6469774148649112 \n",
            "Iteration: 9456 ---- loss: 0.6469763710906069 \n",
            "Iteration: 9457 ---- loss: 0.6469753277848411 \n",
            "Iteration: 9458 ---- loss: 0.6469742849469233 \n",
            "Iteration: 9459 ---- loss: 0.6469732425761655 \n",
            "Iteration: 9460 ---- loss: 0.6469722006718833 \n",
            "Iteration: 9461 ---- loss: 0.6469711592203825 \n",
            "Iteration: 9462 ---- loss: 0.6469701181987927 \n",
            "Iteration: 9463 ---- loss: 0.6469690776416042 \n",
            "Iteration: 9464 ---- loss: 0.646968037706588 \n",
            "Iteration: 9465 ---- loss: 0.6469669989153888 \n",
            "Iteration: 9466 ---- loss: 0.6469659605981941 \n",
            "Iteration: 9467 ---- loss: 0.6469649227542683 \n",
            "Iteration: 9468 ---- loss: 0.64696388538288 \n",
            "Iteration: 9469 ---- loss: 0.6469628484833012 \n",
            "Iteration: 9470 ---- loss: 0.6469618120548074 \n",
            "Iteration: 9471 ---- loss: 0.6469607760966791 \n",
            "Iteration: 9472 ---- loss: 0.6469597406081992 \n",
            "Iteration: 9473 ---- loss: 0.6469587055886541 \n",
            "Iteration: 9474 ---- loss: 0.6469576710373345 \n",
            "Iteration: 9475 ---- loss: 0.6469566369535338 \n",
            "Iteration: 9476 ---- loss: 0.6469556033365493 \n",
            "Iteration: 9477 ---- loss: 0.6469545701856811 \n",
            "Iteration: 9478 ---- loss: 0.6469535375002329 \n",
            "Iteration: 9479 ---- loss: 0.6469525052795113 \n",
            "Iteration: 9480 ---- loss: 0.646951473774121 \n",
            "Iteration: 9481 ---- loss: 0.6469504439876085 \n",
            "Iteration: 9482 ---- loss: 0.6469494146694847 \n",
            "Iteration: 9483 ---- loss: 0.6469483858190365 \n",
            "Iteration: 9484 ---- loss: 0.6469473574355536 \n",
            "Iteration: 9485 ---- loss: 0.6469463295183306 \n",
            "Iteration: 9486 ---- loss: 0.6469453020666642 \n",
            "Iteration: 9487 ---- loss: 0.6469442750798547 \n",
            "Iteration: 9488 ---- loss: 0.6469432485905512 \n",
            "Iteration: 9489 ---- loss: 0.646942222584495 \n",
            "Iteration: 9490 ---- loss: 0.6469411970415536 \n",
            "Iteration: 9491 ---- loss: 0.6469401719610266 \n",
            "Iteration: 9492 ---- loss: 0.6469391473422172 \n",
            "Iteration: 9493 ---- loss: 0.6469381231844319 \n",
            "Iteration: 9494 ---- loss: 0.646937099489531 \n",
            "Iteration: 9495 ---- loss: 0.6469360762547705 \n",
            "Iteration: 9496 ---- loss: 0.6469350534757661 \n",
            "Iteration: 9497 ---- loss: 0.6469340311550492 \n",
            "Iteration: 9498 ---- loss: 0.6469330092919424 \n",
            "Iteration: 9499 ---- loss: 0.646931988420894 \n",
            "Iteration: 9501 ---- loss: 0.6469299508269397 \n",
            "Iteration: 9502 ---- loss: 0.6469289327320288 \n",
            "Iteration: 9503 ---- loss: 0.6469279150984438 \n",
            "Iteration: 9504 ---- loss: 0.6469268980219104 \n",
            "Iteration: 9505 ---- loss: 0.6469258815270904 \n",
            "Iteration: 9506 ---- loss: 0.6469248654921625 \n",
            "Iteration: 9507 ---- loss: 0.6469238499208677 \n",
            "Iteration: 9508 ---- loss: 0.6469228348037612 \n",
            "Iteration: 9509 ---- loss: 0.6469218201442771 \n",
            "Iteration: 9510 ---- loss: 0.6469208059419825 \n",
            "Iteration: 9511 ---- loss: 0.6469197921962079 \n",
            "Iteration: 9512 ---- loss: 0.6469187789062858 \n",
            "Iteration: 9513 ---- loss: 0.6469177660715523 \n",
            "Iteration: 9514 ---- loss: 0.6469167536913453 \n",
            "Iteration: 9515 ---- loss: 0.6469157417650063 \n",
            "Iteration: 9516 ---- loss: 0.646914730291879 \n",
            "Iteration: 9517 ---- loss: 0.6469137192406912 \n",
            "Iteration: 9518 ---- loss: 0.6469127081948999 \n",
            "Iteration: 9519 ---- loss: 0.6469116975849648 \n",
            "Iteration: 9520 ---- loss: 0.6469106873466093 \n",
            "Iteration: 9521 ---- loss: 0.646909677602219 \n",
            "Iteration: 9522 ---- loss: 0.6469086683056908 \n",
            "Iteration: 9523 ---- loss: 0.6469076594591678 \n",
            "Iteration: 9524 ---- loss: 0.6469066510556145 \n",
            "Iteration: 9525 ---- loss: 0.6469056431070628 \n",
            "Iteration: 9526 ---- loss: 0.6469046356130097 \n",
            "Iteration: 9527 ---- loss: 0.6469036285566978 \n",
            "Iteration: 9528 ---- loss: 0.6469026219498755 \n",
            "Iteration: 9529 ---- loss: 0.646901615784348 \n",
            "Iteration: 9530 ---- loss: 0.6469006100612625 \n",
            "Iteration: 9531 ---- loss: 0.6468996047850787 \n",
            "Iteration: 9532 ---- loss: 0.6468985999422534 \n",
            "Iteration: 9533 ---- loss: 0.6468975955539862 \n",
            "Iteration: 9534 ---- loss: 0.6468965915881756 \n",
            "Iteration: 9535 ---- loss: 0.6468955880862358 \n",
            "Iteration: 9536 ---- loss: 0.6468945850367626 \n",
            "Iteration: 9537 ---- loss: 0.6468935826804997 \n",
            "Iteration: 9538 ---- loss: 0.646892580756212 \n",
            "Iteration: 9539 ---- loss: 0.646891579280114 \n",
            "Iteration: 9540 ---- loss: 0.646890578250385 \n",
            "Iteration: 9541 ---- loss: 0.6468895776452881 \n",
            "Iteration: 9542 ---- loss: 0.6468885776072484 \n",
            "Iteration: 9543 ---- loss: 0.6468875781179588 \n",
            "Iteration: 9544 ---- loss: 0.6468865792427198 \n",
            "Iteration: 9545 ---- loss: 0.6468855808065387 \n",
            "Iteration: 9546 ---- loss: 0.6468845830354171 \n",
            "Iteration: 9547 ---- loss: 0.6468835890119851 \n",
            "Iteration: 9548 ---- loss: 0.6468825954220911 \n",
            "Iteration: 9549 ---- loss: 0.6468816022828262 \n",
            "Iteration: 9551 ---- loss: 0.6468796173533099 \n",
            "Iteration: 9552 ---- loss: 0.6468786255605071 \n",
            "Iteration: 9553 ---- loss: 0.6468776342140999 \n",
            "Iteration: 9554 ---- loss: 0.6468766433032588 \n",
            "Iteration: 9555 ---- loss: 0.6468756528488792 \n",
            "Iteration: 9556 ---- loss: 0.6468746628333968 \n",
            "Iteration: 9557 ---- loss: 0.6468736732547691 \n",
            "Iteration: 9558 ---- loss: 0.6468726841347608 \n",
            "Iteration: 9559 ---- loss: 0.6468716954661824 \n",
            "Iteration: 9560 ---- loss: 0.6468707072334641 \n",
            "Iteration: 9561 ---- loss: 0.6468697194514007 \n",
            "Iteration: 9562 ---- loss: 0.6468687321070333 \n",
            "Iteration: 9563 ---- loss: 0.646867745195271 \n",
            "Iteration: 9564 ---- loss: 0.6468667598456098 \n",
            "Iteration: 9565 ---- loss: 0.6468657765890637 \n",
            "Iteration: 9566 ---- loss: 0.6468647937781775 \n",
            "Iteration: 9567 ---- loss: 0.6468638113787321 \n",
            "Iteration: 9568 ---- loss: 0.6468628293946345 \n",
            "Iteration: 9569 ---- loss: 0.6468618478525343 \n",
            "Iteration: 9570 ---- loss: 0.6468608667656103 \n",
            "Iteration: 9571 ---- loss: 0.6468598861950927 \n",
            "Iteration: 9572 ---- loss: 0.6468589060830403 \n",
            "Iteration: 9573 ---- loss: 0.6468579264116575 \n",
            "Iteration: 9574 ---- loss: 0.6468569471909059 \n",
            "Iteration: 9575 ---- loss: 0.6468559684249588 \n",
            "Iteration: 9576 ---- loss: 0.6468549901113643 \n",
            "Iteration: 9577 ---- loss: 0.6468540122379918 \n",
            "Iteration: 9578 ---- loss: 0.6468530348128461 \n",
            "Iteration: 9579 ---- loss: 0.6468520578369504 \n",
            "Iteration: 9580 ---- loss: 0.6468510813151183 \n",
            "Iteration: 9581 ---- loss: 0.6468501052290503 \n",
            "Iteration: 9582 ---- loss: 0.6468491295883422 \n",
            "Iteration: 9583 ---- loss: 0.6468481543923461 \n",
            "Iteration: 9584 ---- loss: 0.6468471796464509 \n",
            "Iteration: 9585 ---- loss: 0.6468462053423681 \n",
            "Iteration: 9586 ---- loss: 0.64684523147614 \n",
            "Iteration: 9587 ---- loss: 0.6468442580520586 \n",
            "Iteration: 9588 ---- loss: 0.6468432850694883 \n",
            "Iteration: 9589 ---- loss: 0.6468423125315477 \n",
            "Iteration: 9590 ---- loss: 0.6468413404355395 \n",
            "Iteration: 9591 ---- loss: 0.6468403687732066 \n",
            "Iteration: 9592 ---- loss: 0.646839397549866 \n",
            "Iteration: 9593 ---- loss: 0.646838426764894 \n",
            "Iteration: 9594 ---- loss: 0.6468374564176683 \n",
            "Iteration: 9595 ---- loss: 0.6468364865109039 \n",
            "Iteration: 9596 ---- loss: 0.6468355169864796 \n",
            "Iteration: 9597 ---- loss: 0.6468345478262973 \n",
            "Iteration: 9598 ---- loss: 0.646833579101319 \n",
            "Iteration: 9599 ---- loss: 0.6468326108109332 \n",
            "Iteration: 9601 ---- loss: 0.646830675531501 \n",
            "Iteration: 9602 ---- loss: 0.6468297085766068 \n",
            "Iteration: 9603 ---- loss: 0.6468287428737393 \n",
            "Iteration: 9604 ---- loss: 0.6468277775979503 \n",
            "Iteration: 9605 ---- loss: 0.6468268127547864 \n",
            "Iteration: 9606 ---- loss: 0.646825848343643 \n",
            "Iteration: 9607 ---- loss: 0.6468248843639183 \n",
            "Iteration: 9608 ---- loss: 0.6468239208150116 \n",
            "Iteration: 9609 ---- loss: 0.6468229576963245 \n",
            "Iteration: 9610 ---- loss: 0.6468219950072603 \n",
            "Iteration: 9611 ---- loss: 0.6468210327472241 \n",
            "Iteration: 9612 ---- loss: 0.6468200709156225 \n",
            "Iteration: 9613 ---- loss: 0.6468191095128835 \n",
            "Iteration: 9614 ---- loss: 0.6468181485385168 \n",
            "Iteration: 9615 ---- loss: 0.6468171879881787 \n",
            "Iteration: 9616 ---- loss: 0.6468162278639205 \n",
            "Iteration: 9617 ---- loss: 0.646815268165157 \n",
            "Iteration: 9618 ---- loss: 0.6468143088913053 \n",
            "Iteration: 9619 ---- loss: 0.6468133500417842 \n",
            "Iteration: 9620 ---- loss: 0.6468123916160134 \n",
            "Iteration: 9621 ---- loss: 0.6468114336134148 \n",
            "Iteration: 9622 ---- loss: 0.6468104760334115 \n",
            "Iteration: 9623 ---- loss: 0.6468095188754284 \n",
            "Iteration: 9624 ---- loss: 0.6468085622121509 \n",
            "Iteration: 9625 ---- loss: 0.6468076066694048 \n",
            "Iteration: 9626 ---- loss: 0.6468066516177702 \n",
            "Iteration: 9627 ---- loss: 0.6468056969892352 \n",
            "Iteration: 9628 ---- loss: 0.646804742783221 \n",
            "Iteration: 9629 ---- loss: 0.6468037889991507 \n",
            "Iteration: 9630 ---- loss: 0.6468028356364482 \n",
            "Iteration: 9631 ---- loss: 0.6468018826945395 \n",
            "Iteration: 9632 ---- loss: 0.6468009301728523 \n",
            "Iteration: 9633 ---- loss: 0.6467999780708155 \n",
            "Iteration: 9634 ---- loss: 0.6467990263878596 \n",
            "Iteration: 9635 ---- loss: 0.6467980751234165 \n",
            "Iteration: 9636 ---- loss: 0.6467971242769198 \n",
            "Iteration: 9637 ---- loss: 0.6467961738478046 \n",
            "Iteration: 9638 ---- loss: 0.6467952238355067 \n",
            "Iteration: 9639 ---- loss: 0.6467942742394646 \n",
            "Iteration: 9640 ---- loss: 0.6467933250591171 \n",
            "Iteration: 9641 ---- loss: 0.646792376293905 \n",
            "Iteration: 9642 ---- loss: 0.6467914279432703 \n",
            "Iteration: 9643 ---- loss: 0.6467904800066562 \n",
            "Iteration: 9644 ---- loss: 0.6467895324835071 \n",
            "Iteration: 9645 ---- loss: 0.6467885853732697 \n",
            "Iteration: 9646 ---- loss: 0.646787638675391 \n",
            "Iteration: 9647 ---- loss: 0.64678669238932 \n",
            "Iteration: 9648 ---- loss: 0.646785746514506 \n",
            "Iteration: 9649 ---- loss: 0.6467848010504008 \n",
            "Iteration: 9651 ---- loss: 0.6467829113521278 \n",
            "Iteration: 9652 ---- loss: 0.6467819671168688 \n",
            "Iteration: 9653 ---- loss: 0.6467810232901356 \n",
            "Iteration: 9654 ---- loss: 0.6467800798713865 \n",
            "Iteration: 9655 ---- loss: 0.6467791368600797 \n",
            "Iteration: 9656 ---- loss: 0.6467781942556752 \n",
            "Iteration: 9657 ---- loss: 0.6467772520576337 \n",
            "Iteration: 9658 ---- loss: 0.646776310265418 \n",
            "Iteration: 9659 ---- loss: 0.6467753688784913 \n",
            "Iteration: 9660 ---- loss: 0.6467744278963181 \n",
            "Iteration: 9661 ---- loss: 0.6467734873183643 \n",
            "Iteration: 9662 ---- loss: 0.6467725471440965 \n",
            "Iteration: 9663 ---- loss: 0.6467716073729829 \n",
            "Iteration: 9664 ---- loss: 0.6467706680044925 \n",
            "Iteration: 9665 ---- loss: 0.6467697290380955 \n",
            "Iteration: 9666 ---- loss: 0.6467687904732635 \n",
            "Iteration: 9667 ---- loss: 0.6467678523094686 \n",
            "Iteration: 9668 ---- loss: 0.6467669146128324 \n",
            "Iteration: 9669 ---- loss: 0.6467659773810513 \n",
            "Iteration: 9670 ---- loss: 0.6467650406056281 \n",
            "Iteration: 9671 ---- loss: 0.646764104519273 \n",
            "Iteration: 9672 ---- loss: 0.6467631689519153 \n",
            "Iteration: 9673 ---- loss: 0.6467622338923956 \n",
            "Iteration: 9674 ---- loss: 0.6467612992318573 \n",
            "Iteration: 9675 ---- loss: 0.6467603649697786 \n",
            "Iteration: 9676 ---- loss: 0.6467594311056396 \n",
            "Iteration: 9677 ---- loss: 0.6467584976389207 \n",
            "Iteration: 9678 ---- loss: 0.6467575645691034 \n",
            "Iteration: 9679 ---- loss: 0.6467566318956707 \n",
            "Iteration: 9680 ---- loss: 0.6467556996181059 \n",
            "Iteration: 9681 ---- loss: 0.6467547677358938 \n",
            "Iteration: 9682 ---- loss: 0.6467538363236566 \n",
            "Iteration: 9683 ---- loss: 0.6467529054060913 \n",
            "Iteration: 9684 ---- loss: 0.6467519748836197 \n",
            "Iteration: 9685 ---- loss: 0.6467510447557289 \n",
            "Iteration: 9686 ---- loss: 0.6467501150123558 \n",
            "Iteration: 9687 ---- loss: 0.6467491855831552 \n",
            "Iteration: 9688 ---- loss: 0.6467482565469422 \n",
            "Iteration: 9689 ---- loss: 0.6467473279032081 \n",
            "Iteration: 9690 ---- loss: 0.6467463996514452 \n",
            "Iteration: 9691 ---- loss: 0.6467454717911469 \n",
            "Iteration: 9692 ---- loss: 0.6467445443218068 \n",
            "Iteration: 9693 ---- loss: 0.6467436172429204 \n",
            "Iteration: 9694 ---- loss: 0.6467426905539831 \n",
            "Iteration: 9695 ---- loss: 0.6467417643311174 \n",
            "Iteration: 9696 ---- loss: 0.6467408386110252 \n",
            "Iteration: 9697 ---- loss: 0.6467399132814501 \n",
            "Iteration: 9698 ---- loss: 0.6467389883418876 \n",
            "Iteration: 9699 ---- loss: 0.6467380637918343 \n",
            "Iteration: 9701 ---- loss: 0.6467362158582444 \n",
            "Iteration: 9702 ---- loss: 0.6467352924737049 \n",
            "Iteration: 9703 ---- loss: 0.6467343694766687 \n",
            "Iteration: 9704 ---- loss: 0.6467334468369318 \n",
            "Iteration: 9705 ---- loss: 0.6467325245183472 \n",
            "Iteration: 9706 ---- loss: 0.6467316025857055 \n",
            "Iteration: 9707 ---- loss: 0.6467306810385106 \n",
            "Iteration: 9708 ---- loss: 0.6467297598762664 \n",
            "Iteration: 9709 ---- loss: 0.6467288390984773 \n",
            "Iteration: 9710 ---- loss: 0.6467279190237192 \n",
            "Iteration: 9711 ---- loss: 0.6467270002523665 \n",
            "Iteration: 9712 ---- loss: 0.6467260818679268 \n",
            "Iteration: 9713 ---- loss: 0.6467251651563611 \n",
            "Iteration: 9714 ---- loss: 0.6467242502382916 \n",
            "Iteration: 9715 ---- loss: 0.6467233357198574 \n",
            "Iteration: 9716 ---- loss: 0.6467224216004803 \n",
            "Iteration: 9717 ---- loss: 0.6467215078795846 \n",
            "Iteration: 9718 ---- loss: 0.6467205945565969 \n",
            "Iteration: 9719 ---- loss: 0.646719681630946 \n",
            "Iteration: 9720 ---- loss: 0.6467187691020625 \n",
            "Iteration: 9721 ---- loss: 0.6467178569693798 \n",
            "Iteration: 9722 ---- loss: 0.6467169452323325 \n",
            "Iteration: 9723 ---- loss: 0.6467160338903577 \n",
            "Iteration: 9724 ---- loss: 0.646715122942895 \n",
            "Iteration: 9725 ---- loss: 0.6467142123893852 \n",
            "Iteration: 9726 ---- loss: 0.6467133022292713 \n",
            "Iteration: 9727 ---- loss: 0.6467123924619984 \n",
            "Iteration: 9728 ---- loss: 0.6467114830876484 \n",
            "Iteration: 9729 ---- loss: 0.6467105741151298 \n",
            "Iteration: 9730 ---- loss: 0.6467096655341636 \n",
            "Iteration: 9731 ---- loss: 0.646708757344202 \n",
            "Iteration: 9732 ---- loss: 0.6467078495446986 \n",
            "Iteration: 9733 ---- loss: 0.6467069421351089 \n",
            "Iteration: 9734 ---- loss: 0.6467060351148903 \n",
            "Iteration: 9735 ---- loss: 0.6467051284835015 \n",
            "Iteration: 9736 ---- loss: 0.6467042222404031 \n",
            "Iteration: 9737 ---- loss: 0.6467033163850576 \n",
            "Iteration: 9738 ---- loss: 0.6467024113753306 \n",
            "Iteration: 9739 ---- loss: 0.6467015072097988 \n",
            "Iteration: 9740 ---- loss: 0.6467006035313821 \n",
            "Iteration: 9741 ---- loss: 0.6466997002393006 \n",
            "Iteration: 9742 ---- loss: 0.6466987973329847 \n",
            "Iteration: 9743 ---- loss: 0.6466978948118678 \n",
            "Iteration: 9744 ---- loss: 0.6466969926753862 \n",
            "Iteration: 9745 ---- loss: 0.6466960909229781 \n",
            "Iteration: 9746 ---- loss: 0.6466951895540843 \n",
            "Iteration: 9747 ---- loss: 0.6466942885681484 \n",
            "Iteration: 9748 ---- loss: 0.646693387971465 \n",
            "Iteration: 9749 ---- loss: 0.6466924877790169 \n",
            "Iteration: 9751 ---- loss: 0.6466906885299945 \n",
            "Iteration: 9752 ---- loss: 0.6466897894760969 \n",
            "Iteration: 9753 ---- loss: 0.6466888908018834 \n",
            "Iteration: 9754 ---- loss: 0.6466879925068125 \n",
            "Iteration: 9755 ---- loss: 0.6466870945903449 \n",
            "Iteration: 9756 ---- loss: 0.646686197051944 \n",
            "Iteration: 9757 ---- loss: 0.6466852998910743 \n",
            "Iteration: 9758 ---- loss: 0.6466844032119762 \n",
            "Iteration: 9759 ---- loss: 0.6466835070699233 \n",
            "Iteration: 9760 ---- loss: 0.6466826113039384 \n",
            "Iteration: 9761 ---- loss: 0.6466817159134947 \n",
            "Iteration: 9762 ---- loss: 0.6466808211069156 \n",
            "Iteration: 9763 ---- loss: 0.6466799273089561 \n",
            "Iteration: 9764 ---- loss: 0.6466790338877099 \n",
            "Iteration: 9765 ---- loss: 0.646678140842643 \n",
            "Iteration: 9766 ---- loss: 0.6466772481732239 \n",
            "Iteration: 9767 ---- loss: 0.6466763558789221 \n",
            "Iteration: 9768 ---- loss: 0.6466754643334728 \n",
            "Iteration: 9769 ---- loss: 0.6466745737525748 \n",
            "Iteration: 9770 ---- loss: 0.6466736835495464 \n",
            "Iteration: 9771 ---- loss: 0.6466727937238376 \n",
            "Iteration: 9772 ---- loss: 0.6466719042749015 \n",
            "Iteration: 9773 ---- loss: 0.6466710152021933 \n",
            "Iteration: 9774 ---- loss: 0.6466701265051701 \n",
            "Iteration: 9775 ---- loss: 0.6466692381832911 \n",
            "Iteration: 9776 ---- loss: 0.6466683502360184 \n",
            "Iteration: 9777 ---- loss: 0.6466674626628153 \n",
            "Iteration: 9778 ---- loss: 0.6466665754631482 \n",
            "Iteration: 9779 ---- loss: 0.6466656886364847 \n",
            "Iteration: 9780 ---- loss: 0.6466648021822948 \n",
            "Iteration: 9781 ---- loss: 0.6466639161945955 \n",
            "Iteration: 9782 ---- loss: 0.6466630326542426 \n",
            "Iteration: 9783 ---- loss: 0.6466621494962006 \n",
            "Iteration: 9784 ---- loss: 0.6466612667198841 \n",
            "Iteration: 9785 ---- loss: 0.6466603843247108 \n",
            "Iteration: 9786 ---- loss: 0.6466595023101018 \n",
            "Iteration: 9787 ---- loss: 0.6466586206754809 \n",
            "Iteration: 9788 ---- loss: 0.6466577394202748 \n",
            "Iteration: 9789 ---- loss: 0.6466568585439127 \n",
            "Iteration: 9790 ---- loss: 0.6466559780458275 \n",
            "Iteration: 9791 ---- loss: 0.6466550979254536 \n",
            "Iteration: 9792 ---- loss: 0.6466542181822288 \n",
            "Iteration: 9793 ---- loss: 0.6466533388155933 \n",
            "Iteration: 9794 ---- loss: 0.6466524598249901 \n",
            "Iteration: 9795 ---- loss: 0.6466515812098645 \n",
            "Iteration: 9796 ---- loss: 0.6466507029696643 \n",
            "Iteration: 9797 ---- loss: 0.64664982510384 \n",
            "Iteration: 9798 ---- loss: 0.646648947611844 \n",
            "Iteration: 9799 ---- loss: 0.6466480704931314 \n",
            "Iteration: 9801 ---- loss: 0.6466463182880058 \n",
            "Iteration: 9802 ---- loss: 0.6466454429375507 \n",
            "Iteration: 9803 ---- loss: 0.646644567961517 \n",
            "Iteration: 9804 ---- loss: 0.6466436933593513 \n",
            "Iteration: 9805 ---- loss: 0.6466428191305025 \n",
            "Iteration: 9806 ---- loss: 0.6466419454452873 \n",
            "Iteration: 9807 ---- loss: 0.6466410728041392 \n",
            "Iteration: 9808 ---- loss: 0.6466402013245538 \n",
            "Iteration: 9809 ---- loss: 0.6466393300486278 \n",
            "Iteration: 9810 ---- loss: 0.6466384591463895 \n",
            "Iteration: 9811 ---- loss: 0.6466375886252139 \n",
            "Iteration: 9812 ---- loss: 0.6466367186816637 \n",
            "Iteration: 9813 ---- loss: 0.6466358489085894 \n",
            "Iteration: 9814 ---- loss: 0.6466349795062405 \n",
            "Iteration: 9815 ---- loss: 0.6466341104763411 \n",
            "Iteration: 9816 ---- loss: 0.646633241953934 \n",
            "Iteration: 9817 ---- loss: 0.6466323737293254 \n",
            "Iteration: 9818 ---- loss: 0.6466315058100544 \n",
            "Iteration: 9819 ---- loss: 0.6466306382604955 \n",
            "Iteration: 9820 ---- loss: 0.6466297711305821 \n",
            "Iteration: 9821 ---- loss: 0.6466289044618135 \n",
            "Iteration: 9822 ---- loss: 0.6466280380149615 \n",
            "Iteration: 9823 ---- loss: 0.6466271719351749 \n",
            "Iteration: 9824 ---- loss: 0.6466263062217856 \n",
            "Iteration: 9825 ---- loss: 0.6466254410139382 \n",
            "Iteration: 9826 ---- loss: 0.6466245760805726 \n",
            "Iteration: 9827 ---- loss: 0.6466237114602916 \n",
            "Iteration: 9828 ---- loss: 0.6466228482109435 \n",
            "Iteration: 9829 ---- loss: 0.646621985318828 \n",
            "Iteration: 9830 ---- loss: 0.646621123837856 \n",
            "Iteration: 9831 ---- loss: 0.6466202615739568 \n",
            "Iteration: 9832 ---- loss: 0.6466194000549873 \n",
            "Iteration: 9833 ---- loss: 0.6466185393154961 \n",
            "Iteration: 9834 ---- loss: 0.6466176782714542 \n",
            "Iteration: 9835 ---- loss: 0.6466168181854257 \n",
            "Iteration: 9836 ---- loss: 0.6466159584214811 \n",
            "Iteration: 9837 ---- loss: 0.6466150984707876 \n",
            "Iteration: 9838 ---- loss: 0.6466142394968655 \n",
            "Iteration: 9839 ---- loss: 0.6466133808413811 \n",
            "Iteration: 9840 ---- loss: 0.646612522145298 \n",
            "Iteration: 9841 ---- loss: 0.646611664323696 \n",
            "Iteration: 9842 ---- loss: 0.6466108067106603 \n",
            "Iteration: 9843 ---- loss: 0.6466099490464117 \n",
            "Iteration: 9844 ---- loss: 0.646609092345449 \n",
            "Iteration: 9845 ---- loss: 0.6466082360248491 \n",
            "Iteration: 9846 ---- loss: 0.6466073794132042 \n",
            "Iteration: 9847 ---- loss: 0.6466065235886835 \n",
            "Iteration: 9848 ---- loss: 0.6466056683077411 \n",
            "Iteration: 9849 ---- loss: 0.6466048129715606 \n",
            "Iteration: 9851 ---- loss: 0.646603105129413 \n",
            "Iteration: 9852 ---- loss: 0.6466022512162093 \n",
            "Iteration: 9853 ---- loss: 0.6466013976749245 \n",
            "Iteration: 9854 ---- loss: 0.6466005453768695 \n",
            "Iteration: 9855 ---- loss: 0.6465996927222675 \n",
            "Iteration: 9856 ---- loss: 0.6465988403863306 \n",
            "Iteration: 9857 ---- loss: 0.6465979888401726 \n",
            "Iteration: 9858 ---- loss: 0.6465971375632447 \n",
            "Iteration: 9859 ---- loss: 0.6465962862615111 \n",
            "Iteration: 9860 ---- loss: 0.6465954353966816 \n",
            "Iteration: 9861 ---- loss: 0.6465945857539933 \n",
            "Iteration: 9862 ---- loss: 0.646593735594881 \n",
            "Iteration: 9863 ---- loss: 0.6465928858570966 \n",
            "Iteration: 9864 ---- loss: 0.6465920369151323 \n",
            "Iteration: 9865 ---- loss: 0.6465911883255456 \n",
            "Iteration: 9866 ---- loss: 0.6465903396807744 \n",
            "Iteration: 9867 ---- loss: 0.6465894915483491 \n",
            "Iteration: 9868 ---- loss: 0.6465886443294465 \n",
            "Iteration: 9869 ---- loss: 0.646587796935564 \n",
            "Iteration: 9870 ---- loss: 0.6465869497500484 \n",
            "Iteration: 9871 ---- loss: 0.6465861029568469 \n",
            "Iteration: 9872 ---- loss: 0.6465852573849258 \n",
            "Iteration: 9873 ---- loss: 0.6465844114868137 \n",
            "Iteration: 9874 ---- loss: 0.6465835658595281 \n",
            "Iteration: 9875 ---- loss: 0.6465827206139381 \n",
            "Iteration: 9876 ---- loss: 0.6465818764291951 \n",
            "Iteration: 9877 ---- loss: 0.6465810320223999 \n",
            "Iteration: 9878 ---- loss: 0.6465801878232991 \n",
            "Iteration: 9879 ---- loss: 0.6465793440673536 \n",
            "Iteration: 9880 ---- loss: 0.6465785007621251 \n",
            "Iteration: 9881 ---- loss: 0.6465776583420859 \n",
            "Iteration: 9882 ---- loss: 0.6465768159701136 \n",
            "Iteration: 9883 ---- loss: 0.6465759739671065 \n",
            "Iteration: 9884 ---- loss: 0.6465751323266933 \n",
            "Iteration: 9885 ---- loss: 0.6465742917778601 \n",
            "Iteration: 9886 ---- loss: 0.6465734511947715 \n",
            "Iteration: 9887 ---- loss: 0.6465726107422225 \n",
            "Iteration: 9888 ---- loss: 0.6465717706508697 \n",
            "Iteration: 9889 ---- loss: 0.6465709311544704 \n",
            "Iteration: 9890 ---- loss: 0.6465700922151737 \n",
            "Iteration: 9891 ---- loss: 0.6465692533124264 \n",
            "Iteration: 9892 ---- loss: 0.6465684160091165 \n",
            "Iteration: 9893 ---- loss: 0.646567579074397 \n",
            "Iteration: 9894 ---- loss: 0.6465667428392898 \n",
            "Iteration: 9895 ---- loss: 0.6465659067855422 \n",
            "Iteration: 9896 ---- loss: 0.6465650708533924 \n",
            "Iteration: 9897 ---- loss: 0.6465642352822111 \n",
            "Iteration: 9898 ---- loss: 0.6465634000714053 \n",
            "Iteration: 9899 ---- loss: 0.6465625656986419 \n",
            "Iteration: 9901 ---- loss: 0.646560897299301 \n",
            "Iteration: 9902 ---- loss: 0.6465600635082162 \n",
            "Iteration: 9903 ---- loss: 0.6465592300763293 \n",
            "Iteration: 9904 ---- loss: 0.646558397222924 \n",
            "Iteration: 9905 ---- loss: 0.6465575648339865 \n",
            "Iteration: 9906 ---- loss: 0.6465567326210058 \n",
            "Iteration: 9907 ---- loss: 0.6465559007464231 \n",
            "Iteration: 9908 ---- loss: 0.6465550691657049 \n",
            "Iteration: 9909 ---- loss: 0.6465542379340675 \n",
            "Iteration: 9910 ---- loss: 0.6465534070509864 \n",
            "Iteration: 9911 ---- loss: 0.6465525765159394 \n",
            "Iteration: 9912 ---- loss: 0.6465517463284076 \n",
            "Iteration: 9913 ---- loss: 0.6465509165221283 \n",
            "Iteration: 9914 ---- loss: 0.6465500871116991 \n",
            "Iteration: 9915 ---- loss: 0.6465492579626781 \n",
            "Iteration: 9916 ---- loss: 0.6465484291591641 \n",
            "Iteration: 9917 ---- loss: 0.6465476007006485 \n",
            "Iteration: 9918 ---- loss: 0.6465467725866259 \n",
            "Iteration: 9919 ---- loss: 0.6465459448165929 \n",
            "Iteration: 9920 ---- loss: 0.6465451173900484 \n",
            "Iteration: 9921 ---- loss: 0.6465442904043353 \n",
            "Iteration: 9922 ---- loss: 0.6465434636803785 \n",
            "Iteration: 9923 ---- loss: 0.6465426372805286 \n",
            "Iteration: 9924 ---- loss: 0.646541811222225 \n",
            "Iteration: 9925 ---- loss: 0.6465409855049764 \n",
            "Iteration: 9926 ---- loss: 0.6465401601282937 \n",
            "Iteration: 9927 ---- loss: 0.646539335136682 \n",
            "Iteration: 9928 ---- loss: 0.6465385106600879 \n",
            "Iteration: 9929 ---- loss: 0.6465376865957856 \n",
            "Iteration: 9930 ---- loss: 0.6465368627845121 \n",
            "Iteration: 9931 ---- loss: 0.6465360393125629 \n",
            "Iteration: 9932 ---- loss: 0.6465352161794546 \n",
            "Iteration: 9933 ---- loss: 0.646534393929392 \n",
            "Iteration: 9934 ---- loss: 0.646533572521844 \n",
            "Iteration: 9935 ---- loss: 0.6465327514569625 \n",
            "Iteration: 9936 ---- loss: 0.6465319307874996 \n",
            "Iteration: 9937 ---- loss: 0.6465311104635743 \n",
            "Iteration: 9938 ---- loss: 0.6465302905683808 \n",
            "Iteration: 9939 ---- loss: 0.6465294712659225 \n",
            "Iteration: 9940 ---- loss: 0.6465286523056872 \n",
            "Iteration: 9941 ---- loss: 0.6465278338398363 \n",
            "Iteration: 9942 ---- loss: 0.6465270165010444 \n",
            "Iteration: 9943 ---- loss: 0.646526199507518 \n",
            "Iteration: 9944 ---- loss: 0.6465253829160342 \n",
            "Iteration: 9945 ---- loss: 0.6465245666608975 \n",
            "Iteration: 9946 ---- loss: 0.6465237506988809 \n",
            "Iteration: 9947 ---- loss: 0.6465229350800797 \n",
            "Iteration: 9948 ---- loss: 0.6465221198039757 \n",
            "Iteration: 9949 ---- loss: 0.6465213048700523 \n",
            "Iteration: 9951 ---- loss: 0.6465196760266981 \n",
            "Iteration: 9952 ---- loss: 0.6465188621476669 \n",
            "Iteration: 9953 ---- loss: 0.6465180486490664 \n",
            "Iteration: 9954 ---- loss: 0.6465172354174488 \n",
            "Iteration: 9955 ---- loss: 0.6465164225708357 \n",
            "Iteration: 9956 ---- loss: 0.6465156100632524 \n",
            "Iteration: 9957 ---- loss: 0.6465147970052353 \n",
            "Iteration: 9958 ---- loss: 0.6465139842827607 \n",
            "Iteration: 9959 ---- loss: 0.6465131718953512 \n",
            "Iteration: 9960 ---- loss: 0.6465123598425321 \n",
            "Iteration: 9961 ---- loss: 0.6465115482035324 \n",
            "Iteration: 9962 ---- loss: 0.6465107368390413 \n",
            "Iteration: 9963 ---- loss: 0.6465099257865443 \n",
            "Iteration: 9964 ---- loss: 0.6465091150667907 \n",
            "Iteration: 9965 ---- loss: 0.6465083046793136 \n",
            "Iteration: 9966 ---- loss: 0.6465074946236484 \n",
            "Iteration: 9967 ---- loss: 0.6465066848993314 \n",
            "Iteration: 9968 ---- loss: 0.6465058758237925 \n",
            "Iteration: 9969 ---- loss: 0.646505067370366 \n",
            "Iteration: 9970 ---- loss: 0.6465042592243863 \n",
            "Iteration: 9971 ---- loss: 0.6465034513180665 \n",
            "Iteration: 9972 ---- loss: 0.646502643742378 \n",
            "Iteration: 9973 ---- loss: 0.6465018364968564 \n",
            "Iteration: 9974 ---- loss: 0.6465010295810393 \n",
            "Iteration: 9975 ---- loss: 0.6465002229944656 \n",
            "Iteration: 9976 ---- loss: 0.6464994167366765 \n",
            "Iteration: 9977 ---- loss: 0.6464986108072145 \n",
            "Iteration: 9978 ---- loss: 0.6464978052239155 \n",
            "Iteration: 9979 ---- loss: 0.6464970000279773 \n",
            "Iteration: 9980 ---- loss: 0.6464961951003603 \n",
            "Iteration: 9981 ---- loss: 0.646495390528212 \n",
            "Iteration: 9982 ---- loss: 0.6464945862821959 \n",
            "Iteration: 9983 ---- loss: 0.6464937823618638 \n",
            "Iteration: 9984 ---- loss: 0.6464929787667683 \n",
            "Iteration: 9985 ---- loss: 0.6464921754964643 \n",
            "Iteration: 9986 ---- loss: 0.6464913725505079 \n",
            "Iteration: 9987 ---- loss: 0.6464905699420757 \n",
            "Iteration: 9988 ---- loss: 0.6464897677250587 \n",
            "Iteration: 9989 ---- loss: 0.6464889657491388 \n",
            "Iteration: 9990 ---- loss: 0.6464881640958339 \n",
            "Iteration: 9991 ---- loss: 0.6464873627633934 \n",
            "Iteration: 9992 ---- loss: 0.6464865617458391 \n",
            "Iteration: 9993 ---- loss: 0.646485761049274 \n",
            "Iteration: 9994 ---- loss: 0.646484960673266 \n",
            "Iteration: 9995 ---- loss: 0.6464841606173845 \n",
            "Iteration: 9996 ---- loss: 0.6464833608811993 \n",
            "Iteration: 9997 ---- loss: 0.6464825615800147 \n",
            "Iteration: 9998 ---- loss: 0.6464817629202297 \n",
            "Iteration: 9999 ---- loss: 0.646480964580098 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_history)\n",
        "plt.plot(test_loss_history, 'r')\n",
        "plt.legend(['Training','Test'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jYI5YE1oSKj7",
        "outputId": "c31c3dd0-f52c-4bf6-fb11-a43253d4c661"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predicting"
      ],
      "metadata": {
        "id": "5WXpzbpnSafX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred, _ = foward_propagation(np.transpose(x_test), parameters, architecture)"
      ],
      "metadata": {
        "id": "Tz3hHGNXSYwd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.transpose(x_test)[1],ymax*y_test,'.')\n",
        "plt.plot(np.transpose(x_test)[1],ymax*Y_pred.reshape([-1,1]),'.r')\n",
        "plt.legend(['Real','Predicted'])\n",
        "plt.ylabel('rented_bikes')\n",
        "plt.xlabel('temperature')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "GErS4aRbSmU1",
        "outputId": "85f37927-0457-42da-bee9-41261b9c305b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(3*np.transpose(x_test)[0],ymax*y_test,'.')\n",
        "plt.plot(3*np.transpose(x_test)[0],ymax*Y_pred.reshape([-1,1]),'.r')\n",
        "plt.legend(['Real','Predicted'])\n",
        "plt.ylabel('rented_bikes')\n",
        "plt.xlabel('weather')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "indice=[1,2,3]\n",
        "plt.xticks(indice, fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "P1I7SRpNSvQ8",
        "outputId": "19777735-c529-4a34-a855-268cc0a82d82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}